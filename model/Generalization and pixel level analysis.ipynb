{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10705470,"sourceType":"datasetVersion","datasetId":6634755},{"sourceId":12004501,"sourceType":"datasetVersion","datasetId":7551751},{"sourceId":12418679,"sourceType":"datasetVersion","datasetId":7832504},{"sourceId":12453420,"sourceType":"datasetVersion","datasetId":7855696},{"sourceId":433290,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":341724,"modelId":363043}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install rasterio geopandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T03:04:38.420005Z","iopub.execute_input":"2025-07-13T03:04:38.420641Z","iopub.status.idle":"2025-07-13T03:04:43.429237Z","shell.execute_reply.started":"2025-07-13T03:04:38.420607Z","shell.execute_reply":"2025-07-13T03:04:43.428340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nimport os\nimport joblib\nimport math\nfrom tqdm import tqdm\nimport datetime\nimport rasterio # Import the rasterio library\n\n# --- Configuration (can be common or passed as args) ---\nMODEL_NAME = \"GAT_Transformer_Prediction\"\nLOOKBACK = 24\nBATCH_SIZE = 128\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Re-define necessary classes and functions from your training notebook ---\n# These are crucial for the model and data loading/processing to work.\n\n# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# Graph Attention Layer\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n        super(GraphAttentionLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.dropout_val = dropout\n        self.alpha = alpha\n        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n        self.dropout_layer = nn.Dropout(self.dropout_val)\n\n    def forward(self, h, adj): # h is (B, N, F_in), adj is (N,N)\n        Wh = torch.matmul(h, self.W)\n        a_input = self._prepare_attention_input(Wh)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention_masked = torch.where(adj.unsqueeze(0) > 0, e, zero_vec)\n        attention_softmax = F.softmax(attention_masked, dim=-1)\n        attention_dropout = self.dropout_layer(attention_softmax)\n        h_prime = torch.matmul(attention_dropout, Wh)\n        return F.elu(h_prime)\n\n    def _prepare_attention_input(self, Wh):\n        B, N, _ = Wh.size()\n        Wh_i = Wh.unsqueeze(2).expand(B, N, N, -1)\n        Wh_j = Wh.unsqueeze(1).expand(B, N, N, -1)\n        return torch.cat([Wh_i, Wh_j], dim=-1)\n\n# MultiHeadGraphAttention\nclass MultiHeadGraphAttention(nn.Module):\n    def __init__(self, in_features, out_features_per_head, n_heads, dropout=0.2, alpha=0.2, concat=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.n_heads = n_heads\n        self.concat = concat\n        self.attentions = nn.ModuleList([GraphAttentionLayer(in_features, out_features_per_head, dropout, alpha) for _ in range(n_heads)])\n        self.out_dim = out_features_per_head * n_heads if concat else out_features_per_head\n\n    def forward(self, x, adj): # x is (B,N,F_in), adj is (N,N)\n        head_outputs = [att(x, adj) for att in self.attentions]\n        if self.concat:\n            return torch.cat(head_outputs, dim=-1)\n        else:\n            return torch.mean(torch.stack(head_outputs, dim=-1), dim=-1)\n\n# GAT+Transformer Model\nclass SpiPredictorGATTransformer(nn.Module):\n    def __init__(self, input_size, d_model_gat=64, gat_heads=4,\n                 d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n                 dim_feedforward_transformer=256, dropout_rate=0.2):\n        super(SpiPredictorGATTransformer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.gat_input_proj = nn.Linear(input_size, d_model_gat)\n        self.gat_attention = MultiHeadGraphAttention(\n            in_features=d_model_gat,\n            out_features_per_head=d_model_gat // gat_heads,\n            n_heads=gat_heads, dropout=dropout_rate, concat=True\n        )\n        gat_output_dim = self.gat_attention.out_dim\n        self.pos_encoder = PositionalEncoding(gat_output_dim, max_len=LOOKBACK + 10)\n        transformer_encoder_layer = nn.TransformerEncoderLayer(\n            d_model=gat_output_dim, nhead=transformer_heads, dim_feedforward=dim_feedforward_transformer,\n            dropout=dropout_rate, batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_transformer_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(gat_output_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x, adj): # x: (B,L,F_in), adj: (L,L)\n        x_proj = F.elu(self.gat_input_proj(x))\n        x_gat_drop = F.dropout(x_proj, self.dropout_rate, training=self.training)\n        x_gat_attended = self.gat_attention(x_gat_drop, adj)\n        x_pos_encoded = self.pos_encoder(x_gat_attended)\n        x_transformed = self.transformer_encoder(x_pos_encoded)\n        x_last_step = x_transformed[:, -1, :]\n        output = self.fc(x_last_step)\n        return output\n\n# TimeSeriesDataset for GAT (Optimized Adjacency)\nclass TimeSeriesDatasetGAT(Dataset):\n    def __init__(self, features, targets, single_adj_matrix, scaler_X=None, scaler_y=None, is_train=True):\n        original_shape = features.shape\n        if is_train and scaler_X is None:\n            self.scaler_X = StandardScaler()\n            flattened = features.reshape(-1, original_shape[-1])\n            scaled = self.scaler_X.fit_transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        elif not is_train and scaler_X is not None:\n            self.scaler_X = scaler_X\n            flattened = features.reshape(-1, original_shape[-1])\n            scaled = self.scaler_X.transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        else:\n            self.features = features\n            self.scaler_X = scaler_X\n        if is_train and scaler_y is None:\n            self.scaler_y = StandardScaler()\n            self.targets = self.scaler_y.fit_transform(targets)\n        elif not is_train and scaler_y is not None:\n            self.scaler_y = scaler_y\n            self.targets = self.scaler_y.transform(targets)\n        else:\n            self.targets = targets\n            self.scaler_y = scaler_y\n        self.features = torch.tensor(self.features, dtype=torch.float32)\n        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n        if not isinstance(single_adj_matrix, torch.Tensor):\n            self.single_adj_matrix = torch.tensor(single_adj_matrix, dtype=torch.float32)\n        else:\n            self.single_adj_matrix = single_adj_matrix.to(dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.single_adj_matrix, self.targets[idx]\n\n    def get_scalers(self):\n        return self.scaler_X, self.scaler_y\n\n# Evaluation Function for GAT-based models\ndef evaluate_model_gat_based(model, test_loader, scaler_y, device, model_name=\"GAT-based Model\"):\n    model.eval()\n    all_preds_s, all_targets_s = [], []\n    with torch.no_grad():\n        for batch_X, batch_adj_single, batch_y_s in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n            batch_X, batch_adj_single = batch_X.to(device), batch_adj_single.to(device)\n            outputs_s = model(batch_X, batch_adj_single[0]) # Pass one adj\n            all_preds_s.append(outputs_s.cpu().numpy())\n            all_targets_s.append(batch_y_s.numpy())\n    preds_s_np = np.vstack(all_preds_s)\n    tgts_s_np = np.vstack(all_targets_s)\n    if scaler_y:\n        preds_o = scaler_y.inverse_transform(preds_s_np)\n        tgts_o = scaler_y.inverse_transform(tgts_s_np)\n    else:\n        preds_o = preds_s_np\n        tgts_o = tgts_s_np\n    mse = mean_squared_error(tgts_o, preds_o)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(tgts_o, preds_o)\n    mae = mean_absolute_error(tgts_o, preds_o)\n    print(f\"\\nOverall Evaluation metrics ({model_name}):\\n MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n    return preds_o, tgts_o, mse, rmse, r2, mae\n\n# Plotting and Reconstruction Functions\ndef plot_all_test_samples(targets_original, predictions_original, model_name, save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'all_test_samples_{model_name}.png')\n    plt.figure(figsize=(12, 8)); plt.scatter(targets_original, predictions_original, alpha=0.3, color='blue', label='Samples')\n    min_val = min(targets_original.min(), predictions_original.min()); max_val = max(targets_original.max(), predictions_original.max())\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y=x')\n    corr = np.nan\n    if len(targets_original.ravel()) > 1 and len(predictions_original.ravel()) > 1:\n        z = np.polyfit(targets_original.ravel(), predictions_original.ravel(), 1); p = np.poly1d(z)\n        sorted_targets = np.sort(targets_original.ravel())\n        plt.plot(sorted_targets, p(sorted_targets), 'g-', label=f'Regression: y={z[0]:.3f}x+{z[1]:.3f}')\n        corr = np.corrcoef(targets_original.ravel(), predictions_original.ravel())[0, 1]\n    plt.xlabel('True Values'); plt.ylabel('Predictions'); plt.title(f'Predictions vs True Values ({model_name}, r = {corr:.3f})')\n    plt.legend(); plt.grid(True)\n    mse = mean_squared_error(targets_original, predictions_original); rmse = np.sqrt(mse); mae = mean_absolute_error(targets_original, predictions_original); r2 = r2_score(targets_original, predictions_original)\n    stats_text = (f\"MSE: {mse:.4f}\\nRMSE: {rmse:.4f}\\nMAE: {mae:.4f}\\nR²: {r2:.4f}\\nCorr: {corr:.4f}\")\n    plt.figtext(0.15, 0.75, stats_text, bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n    plt.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(save_path); plt.show()\n\ndef reconstruct_rasters(predictions_flat, test_sample_indices, original_mask, spi_full_shape):\n    unique_test_time_indices_original = sorted(list(set([t for t, _ in test_sample_indices])))\n    reconstructed_spatial_rasters = np.full((len(unique_test_time_indices_original), spi_full_shape[1], spi_full_shape[2]), np.nan)\n    valid_pixel_row_coords, valid_pixel_col_coords = np.where(original_mask)\n    for i, (original_t, pixel_idx_in_mask) in enumerate(test_sample_indices):\n        relative_t_idx_in_reconstruction = unique_test_time_indices_original.index(original_t)\n        h_coord = valid_pixel_row_coords[pixel_idx_in_mask]\n        w_coord = valid_pixel_col_coords[pixel_idx_in_mask]\n        reconstructed_spatial_rasters[relative_t_idx_in_reconstruction, h_coord, w_coord] = predictions_flat[i][0]\n    return reconstructed_spatial_rasters, unique_test_time_indices_original\n\ndef visualize_raster_samples(real_rasters_for_test_timesteps, pred_rasters_for_test_timesteps,\n                             mask_for_visualization, unique_test_time_indices, model_name,\n                             dates_list, num_samples_to_plot=5, output_dir='visualizations'):\n    if not os.path.exists(output_dir): os.makedirs(output_dir)\n    prefix = f'raster_sample_{model_name.lower()}'\n    colors = ['#730000', '#E60000', '#FFAA00', '#FCD37F', '#FFFF00', '#FFFFFF', '#DCF8FF', '#96D2FF', '#46A5FF', '#0000FF', '#000080']\n    cmap_usdm_spi = plt.cm.colors.LinearSegmentedColormap.from_list('usdm_spi', colors, N=256)\n    vmin_spi, vmax_spi = -3, 3\n    num_available_rasters = len(real_rasters_for_test_timesteps)\n    if num_available_rasters == 0: print(\"No rasters to visualize.\"); return\n    plot_indices = np.random.choice(num_available_rasters, min(num_samples_to_plot, num_available_rasters), replace=False)\n    for i, raster_idx_in_test_set in enumerate(plot_indices):\n        original_time_index = unique_test_time_indices[raster_idx_in_test_set]\n        date_str = dates_list[original_time_index].strftime('%Y-%m')\n\n        true_raster_slice = real_rasters_for_test_timesteps[raster_idx_in_test_set]\n        pred_raster_slice = pred_rasters_for_test_timesteps[raster_idx_in_test_set]\n        diff_raster_slice = np.full_like(true_raster_slice, np.nan)\n        valid_pixels_for_diff = ~np.isnan(true_raster_slice) & ~np.isnan(pred_raster_slice)\n        diff_raster_slice[valid_pixels_for_diff] = true_raster_slice[valid_pixels_for_diff] - pred_raster_slice[valid_pixels_for_diff]\n        diff_abs_max = np.nanmax(np.abs(diff_raster_slice)) if not np.all(np.isnan(diff_raster_slice)) else 1.0\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6)); fig.suptitle(f'SPI Comparison ({model_name}) - Time: {date_str} (Original Index: {original_time_index})', fontsize=16)\n        ax1 = axes[0]; masked_true_spi = np.ma.array(true_raster_slice, mask=~mask_for_visualization); im1 = ax1.imshow(masked_true_spi, cmap=cmap_usdm_spi, vmin=vmin_spi, vmax=vmax_spi)\n        fig.colorbar(im1, ax=ax1, label='SPI Value', fraction=0.046, pad=0.04); ax1.set_title('True SPI'); ax1.axis('off')\n        ax2 = axes[1]; masked_pred_spi = np.ma.array(pred_raster_slice, mask=~mask_for_visualization); im2 = ax2.imshow(masked_pred_spi, cmap=cmap_usdm_spi, vmin=vmin_spi, vmax=vmax_spi)\n        fig.colorbar(im2, ax=ax2, label='SPI Value', fraction=0.046, pad=0.04); ax2.set_title('Predicted SPI'); ax2.axis('off')\n        ax3 = axes[2]; masked_diff_spi = np.ma.array(diff_raster_slice, mask=~mask_for_visualization); im3 = ax3.imshow(masked_diff_spi, cmap='RdBu_r', vmin=-diff_abs_max, vmax=diff_abs_max)\n        fig.colorbar(im3, ax=ax3, label='Difference (True - Predicted)', fraction=0.046, pad=0.04); ax3.set_title('Difference'); ax3.axis('off')\n        plt.tight_layout(rect=[0, 0, 1, 0.95]); save_filename = os.path.join(output_dir, f'{prefix}_time_{date_str.replace(\"-\", \"_\")}.png')\n        plt.savefig(save_filename, dpi=200); plt.show()\n\n# --- NEW: Function to save raster as GeoTIFF ---\ndef save_raster_as_geotiff(raster_data, ref_geotiff_path, out_path):\n    \"\"\"\n    Saves a numpy array as a GeoTIFF, using another GeoTIFF as a reference for metadata.\n    Handles NaN values by setting a nodata value in the output profile.\n    \"\"\"\n    try:\n        with rasterio.open(ref_geotiff_path) as src:\n            profile = src.profile\n            # Use a nodata value that is unlikely to be a real prediction\n            nodata_val = -9999.0 \n            \n            # Replace NaNs in the data with the chosen nodata value\n            raster_data_with_nodata = np.nan_to_num(raster_data, nan=nodata_val)\n            \n            profile.update(\n                dtype=rasterio.float32,\n                count=1,\n                compress='lzw', # Optional but good practice\n                nodata=nodata_val # Set the nodata value in the file's metadata\n            )\n\n        with rasterio.open(out_path, 'w', **profile) as dst:\n            dst.write(raster_data_with_nodata.astype(rasterio.float32), 1)\n        print(f\"Successfully saved GeoTIFF: {out_path}\")\n\n    except Exception as e:\n        print(f\"ERROR: Could not save GeoTIFF {out_path}. Reason: {e}\")\n\n\n# --- load_data_for_prediction (Modified for selecting time steps) ---\ndef load_data_for_prediction(imputed_file_path, lookback_period, selected_time_steps=None):\n    print(f\"Loading data for prediction from: {imputed_file_path}\")\n    data_imputed = np.load(imputed_file_path)\n    ndvi_data = data_imputed['NDVI_imputed']\n    soil_moisture_data = data_imputed['SoilMoisture_imputed']\n    lst_data = data_imputed['LST_imputed']\n    spi_data = data_imputed['SPI_original']\n    mask = data_imputed['mask']\n\n    print(f\"Loaded imputed feature shapes: NDVI: {ndvi_data.shape}, SM: {soil_moisture_data.shape}, LST: {lst_data.shape}\")\n    print(f\"Loaded SPI_original shape: {spi_data.shape}, Mask shape: {mask.shape}, Valid pixels in mask: {np.sum(mask)}\")\n\n    time_steps_total, H, W = ndvi_data.shape\n    X_list, y_list, sample_indices_output = [], [], []\n\n    num_valid_pixels_in_mask = np.sum(mask)\n    ndvi_masked_ts = [ndvi_data[t][mask] for t in range(time_steps_total)]\n    lst_masked_ts = [lst_data[t][mask] for t in range(time_steps_total)]\n    sm_masked_ts = [soil_moisture_data[t][mask] for t in range(time_steps_total)]\n    spi_masked_ts = [spi_data[t][mask] for t in range(time_steps_total)]\n\n    time_steps_to_process = range(lookback_period, time_steps_total)\n    if selected_time_steps is not None:\n        time_steps_to_process = [t for t in selected_time_steps if t >= lookback_period and t < time_steps_total]\n        print(f\"Predicting for selected time steps: {time_steps_to_process}\")\n    else:\n        print(\"Predicting for all valid time steps.\")\n\n    for t in tqdm(time_steps_to_process, desc=\"Preparing prediction samples\"):\n        targets_for_current_t_step = spi_masked_ts[t]\n        for pixel_idx_in_mask in range(num_valid_pixels_in_mask):\n            current_target_value = targets_for_current_t_step[pixel_idx_in_mask]\n            if np.isnan(current_target_value):\n                continue\n            single_pixel_feature_sequence = []\n            for i in range(lookback_period):\n                time_index_in_original_data = t - lookback_period + i\n                val_ndvi = ndvi_masked_ts[time_index_in_original_data][pixel_idx_in_mask]\n                val_lst = lst_masked_ts[time_index_in_original_data][pixel_idx_in_mask]\n                val_sm = sm_masked_ts[time_index_in_original_data][pixel_idx_in_mask]\n                single_pixel_feature_sequence.append(np.array([val_ndvi, val_lst, val_sm]))\n            X_list.append(np.array(single_pixel_feature_sequence))\n            y_list.append(current_target_value)\n            sample_indices_output.append((t, pixel_idx_in_mask))\n\n    X = np.array(X_list)\n    y = np.array(y_list).reshape(-1, 1)\n\n    print(f\"Final X shape for prediction: {X.shape}, y shape: {y.shape}, Samples: {len(sample_indices_output)}\")\n    original_data_info = {'SPI_full_shape': spi_data.shape, 'SPI_original_full': spi_data, 'time_steps_total': time_steps_total}\n    return X, y, mask, sample_indices_output, original_data_info\n\n# --- Date Generation Helper ---\ndef generate_dates(start_year, start_month, num_timesteps):\n    dates = []\n    current_date = datetime.datetime(start_year, start_month, 1)\n    for _ in range(num_timesteps):\n        dates.append(current_date)\n        if current_date.month == 12:\n            current_date = datetime.datetime(current_date.year + 1, 1, 1)\n        else:\n            current_date = datetime.datetime(current_date.year, current_date.month + 1, 1)\n    return dates\n\n# --- MODIFIED: General Prediction and Evaluation Function ---\ndef predict_and_evaluate_country(model_path, scaler_X_path, scaler_y_path,\n                                 imputed_data_path_country,\n                                 ref_geotiff_path, # ADDED: Path to the reference GeoTIFF\n                                 output_base_dir,\n                                 country_name,\n                                 data_start_year, data_start_month,\n                                 selected_time_steps=None):\n    print(f\"\\n--- Running Prediction and Evaluation for {country_name} with {MODEL_NAME} ---\")\n    print(f\"Using device: {DEVICE}\")\n\n    safe_country_name = country_name.lower().replace(' ', '_').replace(',', '')\n    output_dir = os.path.join(output_base_dir, f\"results_{safe_country_name}_{MODEL_NAME.lower().replace('+', '_')}\")\n    viz_dir = os.path.join(output_dir, \"visualizations\")\n    geotiff_dir = os.path.join(output_dir, \"geotiffs\") # NEW: Directory for GeoTIFFs\n    os.makedirs(viz_dir, exist_ok=True)\n    os.makedirs(geotiff_dir, exist_ok=True) # NEW: Create GeoTIFF directory\n    print(f\"Created output directories in: {output_dir}\")\n\n    # 1. Load the trained model and scalers\n    try:\n        scaler_X = joblib.load(scaler_X_path)\n        scaler_y = joblib.load(scaler_y_path)\n    except Exception as e:\n        print(f\"Error loading scalers: {e}. Aborting.\")\n        return\n    \n    model_input_size = 3\n    model = SpiPredictorGATTransformer(\n        input_size=model_input_size, d_model_gat=64, gat_heads=4,\n        d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n        dim_feedforward_transformer=256, dropout_rate=0.2\n    ).to(DEVICE)\n    try:\n        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n    except Exception as e:\n        print(f\"Error loading model: {e}. Aborting.\")\n        return\n    model.eval()\n\n    # 2. Load and prepare data\n    X_pred_all, y_true_all, data_mask, pred_sample_indices, orig_data_info = \\\n        load_data_for_prediction(imputed_data_path_country, LOOKBACK, selected_time_steps)\n\n    if X_pred_all.shape[0] == 0:\n        print(f\"No valid samples for prediction in {country_name}. Exiting.\")\n        return\n\n    total_timesteps_in_data = orig_data_info['time_steps_total']\n    dates_for_visualization = generate_dates(data_start_year, data_start_month, total_timesteps_in_data)\n\n    # 3. Create DataLoader\n    adj_matrix_template = torch.zeros(LOOKBACK, LOOKBACK, dtype=torch.float32)\n    for i in range(LOOKBACK):\n        if i > 0: adj_matrix_template[i, i-1] = 1\n        if i < LOOKBACK - 1: adj_matrix_template[i, i+1] = 1\n        adj_matrix_template[i, i] = 1\n\n    pred_dataset = TimeSeriesDatasetGAT(X_pred_all, y_true_all, adj_matrix_template.cpu(), scaler_X, scaler_y, is_train=False)\n    pred_loader = DataLoader(pred_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n    # 4. Make predictions and evaluate (overall)\n    preds_o, tgts_o, _, _, _, _ = evaluate_model_gat_based(model, pred_loader, scaler_y, DEVICE, f\"{MODEL_NAME} on {country_name}\")\n\n    # 5. Reconstruction and Saving\n    plot_all_test_samples(tgts_o, preds_o, f\"{MODEL_NAME}_{country_name}\", save_dir=viz_dir)\n\n    spi_shape_o = orig_data_info['SPI_full_shape']\n    recon_preds, unique_pred_t = reconstruct_rasters(preds_o, pred_sample_indices, data_mask, spi_shape_o)\n    \n    np.savez_compressed(os.path.join(output_dir, f'spi_predictions_{safe_country_name}.npz'),\n                        predictions=recon_preds, mask=data_mask, time_steps=unique_pred_t)\n\n    full_spi_original_country = orig_data_info['SPI_original_full']\n    true_pred_rs = np.array([full_spi_original_country[t] for t in unique_pred_t])\n\n    # --- ADDED: Calculate and print metrics per time step ---\n    print(f\"\\n--- Metrics per Predicted Time Step for {country_name} ---\")\n    for i, time_index in enumerate(unique_pred_t):\n        date_str = dates_for_visualization[time_index].strftime('%Y-%m')\n        \n        # Get the true and predicted rasters for the current time step\n        true_raster = true_pred_rs[i]\n        pred_raster = recon_preds[i]\n        \n        # Flatten and filter out NaNs for accurate metric calculation\n        valid_mask = ~np.isnan(true_raster) & ~np.isnan(pred_raster)\n        \n        true_values_flat = true_raster[valid_mask]\n        pred_values_flat = pred_raster[valid_mask]\n        \n        # Calculate metrics if there are any valid pixels\n        if true_values_flat.size > 0:\n            mse_step = mean_squared_error(true_values_flat, pred_values_flat)\n            rmse_step = np.sqrt(mse_step)\n            mae_step = mean_absolute_error(true_values_flat, pred_values_flat)\n            r2_step = r2_score(true_values_flat, pred_values_flat)\n            \n            print(f\"Time Step: {date_str} (Index: {time_index})\")\n            print(f\"  MSE: {mse_step:.4f}, RMSE: {rmse_step:.4f}, MAE: {mae_step:.4f}, R²: {r2_step:.4f}\")\n        else:\n            print(f\"Time Step: {date_str} (Index: {time_index}) - No valid overlapping pixels for metrics.\")\n    print(\"---------------------------------------------------\\n\")\n    \n    # 6. Save each predicted raster as a GeoTIFF\n    if ref_geotiff_path and os.path.exists(ref_geotiff_path):\n        print(f\"Saving predicted rasters as GeoTIFFs using reference: {ref_geotiff_path}\")\n        for i, time_index in enumerate(unique_pred_t):\n            date_str = dates_for_visualization[time_index].strftime('%Y_%m')\n            output_filename = f\"predicted_spi_{safe_country_name}_{date_str}.tif\"\n            output_filepath = os.path.join(geotiff_dir, output_filename)\n            raster_to_save = recon_preds[i]\n            save_raster_as_geotiff(raster_to_save, ref_geotiff_path, output_filepath)\n    elif not ref_geotiff_path:\n         print(\"WARNING: No reference GeoTIFF path provided. Skipping GeoTIFF save.\")\n    else:\n         print(f\"WARNING: Reference GeoTIFF not found at '{ref_geotiff_path}'. Skipping GeoTIFF save.\")\n\n\n    # 7. Visualize Raster Samples\n    visualize_raster_samples(true_pred_rs, recon_preds, data_mask, unique_pred_t,\n                             f\"{MODEL_NAME}_{country_name}\", dates_for_visualization, output_dir=viz_dir)\n\n    print(f\"Prediction and evaluation for {country_name} complete. Results in {output_dir}\")\n\n# --- Main execution block for prediction ---\nif __name__ == \"__main__\":\n    # Paths to your trained model and scalers\n    MODEL_DIR = \"/kaggle/input/gat-transformer-guinea-bissau/pytorch/trained-on-guinea-bissau/2/results_gat_transformer\"\n    MODEL_WEIGHTS_PATH = os.path.join(MODEL_DIR, \"best_spi_gat_transformer_model.pt\")\n    SCALER_X_PATH = os.path.join(MODEL_DIR, \"scaler_X_gat_transformer.pkl\")\n    SCALER_Y_PATH = os.path.join(MODEL_DIR, \"scaler_y_gat_transformer.pkl\")\n\n    OUTPUT_BASE_DIR = \"/kaggle/working\"\n\n    # --- NEW: Dictionary mapping country names to their reference GeoTIFFs ---\n    REFERENCE_GEOTIFFS = {\n        \"Gambia\": \"/kaggle/input/ndvi-month/NDVI_Images/Gambia,_The/NDVI_Gambia,_The_2000_02.tif\",\n        \"Guinea\": \"/kaggle/input/ndvi-month/NDVI_Images/Guinea/NDVI_Guinea_2000_02.tif\",\n        \"Guinea-Bissau\": \"/kaggle/input/ndvi-month/NDVI_Images/Guinea-Bissau/NDVI_Guinea-Bissau_2000_02.tif\",\n        \"Nigeria\": \"/kaggle/input/ndvi-month/NDVI_Images/Nigeria/NDVI_Nigeria_2000_02.tif\",\n        \"Senegal\": \"/kaggle/input/ndvi-month/NDVI_Images/Senegal/NDVI_Senegal_2000_02.tif\"\n    }\n    \n    # --- Configuration for countries ---\n    COUNTRIES_CONFIG = {\n        \"Senegal\": {\n            \"imputed_data_path\": \"/kaggle/input/imputed-data/Senegal_combined_imputed.npz\",\n            \"start_year\": 2000,\n            \"start_month\": 2,\n            \"selected_timesteps\": [270, 275, 280, 282, 285]\n        },\n        \"Gambia\": {\n            \"imputed_data_path\": \"/kaggle/input/imputed-data/Gambia,_The_combined_imputed.npz\",\n            \"start_year\": 2000,\n            \"start_month\": 2,\n            \"selected_timesteps\": [270, 275, 280, 282, 285]\n        },\n        \"Guinea-Bissau\": {\n            \"imputed_data_path\": \"/kaggle/input/imputed-data/Guinea-Bissau_combined_imputed.npz\",\n            \"start_year\": 2000,\n            \"start_month\": 2,\n            \"selected_timesteps\": [270, 275, 280, 282, 285]\n        },\n        \"Guinea\": {\n            \"imputed_data_path\": \"/kaggle/input/imputed-data/Guinea_combined_imputed.npz\",\n            \"start_year\": 2000,\n            \"start_month\": 2,\n            \"selected_timesteps\": [270, 275, 280, 282, 285]\n        }\n    }\n\n    # --- Loop through countries and run predictions ---\n    for country_name, config in COUNTRIES_CONFIG.items():\n        predict_and_evaluate_country(\n            model_path=MODEL_WEIGHTS_PATH,\n            scaler_X_path=SCALER_X_PATH,\n            scaler_y_path=SCALER_Y_PATH,\n            imputed_data_path_country=config[\"imputed_data_path\"],\n            ref_geotiff_path=REFERENCE_GEOTIFFS.get(country_name), # Pass the reference GeoTIFF path\n            output_base_dir=OUTPUT_BASE_DIR,\n            country_name=country_name,\n            data_start_year=config[\"start_year\"],\n            data_start_month=config[\"start_month\"],\n            selected_time_steps=config[\"selected_timesteps\"]\n        )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-08T17:18:48.208788Z","iopub.execute_input":"2025-07-08T17:18:48.209020Z","iopub.status.idle":"2025-07-08T17:23:42.361332Z","shell.execute_reply.started":"2025-07-08T17:18:48.208999Z","shell.execute_reply":"2025-07-08T17:23:42.360635Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nimport os\nimport joblib\nimport math\nfrom tqdm import tqdm\nimport datetime\nimport rasterio\nfrom rasterio.plot import show\nimport geopandas as gpd\n\n# --- Configuration (remains the same) ---\nMODEL_NAME = \"GAT_Transformer_Prediction\"\nLOOKBACK = 24\nBATCH_SIZE = 128\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- All class and function definitions up to the visualization part remain unchanged ---\n\n# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n\n# Graph Attention Layer\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n        super(GraphAttentionLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.dropout_val = dropout\n        self.alpha = alpha\n        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n        self.dropout_layer = nn.Dropout(self.dropout_val)\n\n    def forward(self, h, adj): # h is (B, N, F_in), adj is (N,N)\n        Wh = torch.matmul(h, self.W)\n        a_input = self._prepare_attention_input(Wh)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention_masked = torch.where(adj.unsqueeze(0) > 0, e, zero_vec)\n        attention_softmax = F.softmax(attention_masked, dim=-1)\n        attention_dropout = self.dropout_layer(attention_softmax)\n        h_prime = torch.matmul(attention_dropout, Wh)\n        return F.elu(h_prime)\n\n    def _prepare_attention_input(self, Wh):\n        B, N, _ = Wh.size()\n        Wh_i = Wh.unsqueeze(2).expand(B, N, N, -1)\n        Wh_j = Wh.unsqueeze(1).expand(B, N, N, -1)\n        return torch.cat([Wh_i, Wh_j], dim=-1)\n\n# MultiHeadGraphAttention\nclass MultiHeadGraphAttention(nn.Module):\n    def __init__(self, in_features, out_features_per_head, n_heads, dropout=0.2, alpha=0.2, concat=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.n_heads = n_heads\n        self.concat = concat\n        self.attentions = nn.ModuleList([GraphAttentionLayer(in_features, out_features_per_head, dropout, alpha) for _ in range(n_heads)])\n        self.out_dim = out_features_per_head * n_heads if concat else out_features_per_head\n\n    def forward(self, x, adj): # x is (B,N,F_in), adj is (N,N)\n        head_outputs = [att(x, adj) for att in self.attentions]\n        if self.concat:\n            return torch.cat(head_outputs, dim=-1)\n        else:\n            return torch.mean(torch.stack(head_outputs, dim=-1), dim=-1)\n\n# GAT+Transformer Model\nclass SpiPredictorGATTransformer(nn.Module):\n    def __init__(self, input_size, d_model_gat=64, gat_heads=4,\n                 d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n                 dim_feedforward_transformer=256, dropout_rate=0.2):\n        super(SpiPredictorGATTransformer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.gat_input_proj = nn.Linear(input_size, d_model_gat)\n        self.gat_attention = MultiHeadGraphAttention(\n            in_features=d_model_gat,\n            out_features_per_head=d_model_gat // gat_heads,\n            n_heads=gat_heads, dropout=dropout_rate, concat=True\n        )\n        gat_output_dim = self.gat_attention.out_dim\n        self.pos_encoder = PositionalEncoding(gat_output_dim, max_len=LOOKBACK + 10)\n        transformer_encoder_layer = nn.TransformerEncoderLayer(\n            d_model=gat_output_dim, nhead=transformer_heads, dim_feedforward=dim_feedforward_transformer,\n            dropout=dropout_rate, batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_transformer_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(gat_output_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x, adj): # x: (B,L,F_in), adj: (L,L)\n        x_proj = F.elu(self.gat_input_proj(x))\n        x_gat_drop = F.dropout(x_proj, self.dropout_rate, training=self.training)\n        x_gat_attended = self.gat_attention(x_gat_drop, adj)\n        x_pos_encoded = self.pos_encoder(x_gat_attended)\n        x_transformed = self.transformer_encoder(x_pos_encoded)\n        x_last_step = x_transformed[:, -1, :]\n        output = self.fc(x_last_step)\n        return output\n\n# TimeSeriesDataset for GAT (Optimized Adjacency)\nclass TimeSeriesDatasetGAT(Dataset):\n    def __init__(self, features, targets, single_adj_matrix, scaler_X=None, scaler_y=None, is_train=True):\n        original_shape = features.shape\n        if is_train and scaler_X is None:\n            self.scaler_X = StandardScaler()\n            flattened = features.reshape(-1, original_shape[-1])\n            scaled = self.scaler_X.fit_transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        elif not is_train and scaler_X is not None:\n            self.scaler_X = scaler_X\n            flattened = features.reshape(-1, original_shape[-1])\n            scaled = self.scaler_X.transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        else:\n            self.features = features\n            self.scaler_X = scaler_X\n        if is_train and scaler_y is None:\n            self.scaler_y = StandardScaler()\n            self.targets = self.scaler_y.fit_transform(targets)\n        elif not is_train and scaler_y is not None:\n            self.scaler_y = scaler_y\n            self.targets = self.scaler_y.transform(targets)\n        else:\n            self.targets = targets\n            self.scaler_y = scaler_y\n        self.features = torch.tensor(self.features, dtype=torch.float32)\n        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n        if not isinstance(single_adj_matrix, torch.Tensor):\n            self.single_adj_matrix = torch.tensor(single_adj_matrix, dtype=torch.float32)\n        else:\n            self.single_adj_matrix = single_adj_matrix.to(dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.single_adj_matrix, self.targets[idx]\n\n    def get_scalers(self):\n        return self.scaler_X, self.scaler_y\n\n# Evaluation Function for GAT-based models\ndef evaluate_model_gat_based(model, test_loader, scaler_y, device, model_name=\"GAT-based Model\"):\n    model.eval()\n    all_preds_s, all_targets_s = [], []\n    with torch.no_grad():\n        for batch_X, batch_adj_single, batch_y_s in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n            batch_X, batch_adj_single = batch_X.to(device), batch_adj_single.to(device)\n            outputs_s = model(batch_X, batch_adj_single[0]) # Pass one adj\n            all_preds_s.append(outputs_s.cpu().numpy())\n            all_targets_s.append(batch_y_s.numpy())\n    preds_s_np = np.vstack(all_preds_s)\n    tgts_s_np = np.vstack(all_targets_s)\n    if scaler_y:\n        preds_o = scaler_y.inverse_transform(preds_s_np)\n        tgts_o = scaler_y.inverse_transform(tgts_s_np)\n    else:\n        preds_o = preds_s_np\n        tgts_o = tgts_s_np\n    mse = mean_squared_error(tgts_o, preds_o)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(tgts_o, preds_o)\n    mae = mean_absolute_error(tgts_o, preds_o)\n    print(f\"\\nOverall Evaluation metrics ({model_name}):\\n MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n    return preds_o, tgts_o, mse, rmse, r2, mae\n\n# Plotting and Reconstruction Functions\ndef plot_all_test_samples(targets_original, predictions_original, model_name, save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'all_test_samples_{model_name}.png')\n    plt.figure(figsize=(12, 8)); plt.scatter(targets_original, predictions_original, alpha=0.3, color='blue', label='Samples')\n    min_val = min(targets_original.min(), predictions_original.min()); max_val = max(targets_original.max(), predictions_original.max())\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='y=x')\n    corr = np.nan\n    if len(targets_original.ravel()) > 1 and len(predictions_original.ravel()) > 1:\n        z = np.polyfit(targets_original.ravel(), predictions_original.ravel(), 1); p = np.poly1d(z)\n        sorted_targets = np.sort(targets_original.ravel())\n        plt.plot(sorted_targets, p(sorted_targets), 'g-', label=f'Regression: y={z[0]:.3f}x+{z[1]:.3f}')\n        corr = np.corrcoef(targets_original.ravel(), predictions_original.ravel())[0, 1]\n    plt.xlabel('True Values'); plt.ylabel('Predictions'); plt.title(f'Predictions vs True Values ({model_name}, r = {corr:.3f})')\n    plt.legend(); plt.grid(True)\n    mse = mean_squared_error(targets_original, predictions_original); rmse = np.sqrt(mse); mae = mean_absolute_error(targets_original, predictions_original); r2 = r2_score(targets_original, predictions_original)\n    stats_text = (f\"MSE: {mse:.4f}\\nRMSE: {rmse:.4f}\\nMAE: {mae:.4f}\\nR²: {r2:.4f}\\nCorr: {corr:.4f}\")\n    plt.figtext(0.15, 0.75, stats_text, bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n    plt.tight_layout(rect=[0, 0, 1, 0.96]); plt.savefig(save_path); plt.show()\n\ndef reconstruct_rasters(predictions_flat, test_sample_indices, original_mask, spi_full_shape):\n    unique_test_time_indices_original = sorted(list(set([t for t, _ in test_sample_indices])))\n    reconstructed_spatial_rasters = np.full((len(unique_test_time_indices_original), spi_full_shape[1], spi_full_shape[2]), np.nan)\n    valid_pixel_row_coords, valid_pixel_col_coords = np.where(original_mask)\n    for i, (original_t, pixel_idx_in_mask) in enumerate(test_sample_indices):\n        relative_t_idx_in_reconstruction = unique_test_time_indices_original.index(original_t)\n        h_coord = valid_pixel_row_coords[pixel_idx_in_mask]\n        w_coord = valid_pixel_col_coords[pixel_idx_in_mask]\n        reconstructed_spatial_rasters[relative_t_idx_in_reconstruction, h_coord, w_coord] = predictions_flat[i][0]\n    return reconstructed_spatial_rasters, unique_test_time_indices_original\n\ndef visualize_raster_samples(real_rasters_for_test_timesteps, pred_rasters_for_test_timesteps,\n                             mask_for_visualization, unique_test_time_indices, model_name,\n                             dates_list, ref_geotiff_path, shapefile_path, country_name,\n                             num_samples_to_plot=5, output_dir='visualizations'):\n    \"\"\"\n    This function for single-country plots is unchanged.\n    \"\"\"\n    if not os.path.exists(output_dir): os.makedirs(output_dir)\n    if not ref_geotiff_path or not os.path.exists(ref_geotiff_path) or not shapefile_path or not os.path.exists(shapefile_path):\n        print(\"ERROR: Reference GeoTIFF or Shapefile path is invalid or file not found. Skipping visualization.\")\n        return\n    with rasterio.open(ref_geotiff_path) as src:\n        plot_extent = (src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top)\n        raster_crs = src.crs\n    try:\n        country_border = gpd.read_file(shapefile_path)\n        if country_border.crs != raster_crs:\n            country_border = country_border.to_crs(raster_crs)\n        country_centroid = country_border.geometry.centroid.iloc[0]\n    except Exception as e:\n        print(f\"Could not read/process shapefile: {e}. Borders will not be plotted.\")\n        country_border = None\n    prefix = f'raster_sample_{model_name.lower()}'\n    colors = ['#730000', '#E60000', '#FFAA00', '#FCD37F', '#FFFF00', '#FFFFFF', '#DCF8FF', '#96D2FF', '#46A5FF', '#0000FF', '#000080']\n    cmap_usdm_spi = plt.cm.colors.LinearSegmentedColormap.from_list('usdm_spi', colors, N=256)\n    vmin_spi, vmax_spi = -3, 3\n    num_available_rasters = len(real_rasters_for_test_timesteps)\n    if num_available_rasters == 0:\n        print(\"No rasters to visualize.\")\n        return\n    plot_indices = np.random.choice(num_available_rasters, min(num_samples_to_plot, num_available_rasters), replace=False)\n    for i, raster_idx_in_test_set in enumerate(plot_indices):\n        original_time_index = unique_test_time_indices[raster_idx_in_test_set]\n        date_str = dates_list[original_time_index].strftime('%Y-%m')\n        true_raster_slice = real_rasters_for_test_timesteps[raster_idx_in_test_set]\n        pred_raster_slice = pred_rasters_for_test_timesteps[raster_idx_in_test_set]\n        diff_raster_slice = true_raster_slice - pred_raster_slice\n        diff_abs_max = np.nanmax(np.abs(diff_raster_slice)) if not np.all(np.isnan(diff_raster_slice)) else 1.0\n        fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n        fig.suptitle(f'SPI Comparison for {country_name} - Time: {date_str} (Index: {original_time_index})', fontsize=18)\n        plot_data = [\n            {'ax': axes[0], 'data': true_raster_slice, 'title': 'True SPI', 'cmap': cmap_usdm_spi, 'vmin': vmin_spi, 'vmax': vmax_spi},\n            {'ax': axes[1], 'data': pred_raster_slice, 'title': 'Predicted SPI', 'cmap': cmap_usdm_spi, 'vmin': vmin_spi, 'vmax': vmax_spi},\n            {'ax': axes[2], 'data': diff_raster_slice, 'title': 'Difference', 'cmap': 'RdBu_r', 'vmin': -diff_abs_max, 'vmax': diff_abs_max}\n        ]\n        for p in plot_data:\n            ax = p['ax']\n            masked_data = np.ma.array(p['data'], mask=~mask_for_visualization)\n            im = ax.imshow(masked_data, cmap=p['cmap'], vmin=p['vmin'], vmax=p['vmax'], extent=plot_extent)\n            label = 'SPI Value' if 'SPI' in p['title'] else 'Difference (True - Predicted)'\n            fig.colorbar(im, ax=ax, label=label, fraction=0.046, pad=0.04)\n            ax.set_title(p['title'], fontsize=14)\n            if country_border is not None:\n                country_border.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=1.5)\n                ax.text(country_centroid.x, country_centroid.y, country_name, ha='center', va='center', fontsize=12, color='black', bbox=dict(facecolor='none', edgecolor='none'))\n            ax.set_xlabel(\"Longitude\"); ax.set_ylabel(\"Latitude\"); ax.tick_params(axis='x', rotation=45)\n        plt.tight_layout(rect=[0, 0, 1, 0.95])\n        save_filename = os.path.join(output_dir, f'{prefix}_time_{date_str.replace(\"-\", \"_\")}.png')\n        plt.savefig(save_filename, dpi=200)\n        plt.show()\n\ndef save_raster_as_geotiff(raster_data, ref_geotiff_path, out_path):\n    try:\n        with rasterio.open(ref_geotiff_path) as src:\n            profile = src.profile\n            nodata_val = -9999.0\n            raster_data_with_nodata = np.nan_to_num(raster_data, nan=nodata_val)\n            profile.update(dtype=rasterio.float32, count=1, compress='lzw', nodata=nodata_val)\n        with rasterio.open(out_path, 'w', **profile) as dst:\n            dst.write(raster_data_with_nodata.astype(rasterio.float32), 1)\n    except Exception as e:\n        print(f\"ERROR: Could not save GeoTIFF {out_path}. Reason: {e}\")\n\ndef load_data_for_prediction(imputed_file_path, lookback_period, selected_time_steps=None):\n    print(f\"Loading data for prediction from: {imputed_file_path}\")\n    data_imputed = np.load(imputed_file_path)\n    ndvi_data, soil_moisture_data, lst_data = data_imputed['NDVI_imputed'], data_imputed['SoilMoisture_imputed'], data_imputed['LST_imputed']\n    spi_data, mask = data_imputed['SPI_original'], data_imputed['mask']\n    print(f\"Loaded SPI_original shape: {spi_data.shape}, Mask shape: {mask.shape}, Valid pixels in mask: {np.sum(mask)}\")\n    time_steps_total, H, W = ndvi_data.shape\n    X_list, y_list, sample_indices_output = [], [], []\n    num_valid_pixels_in_mask = np.sum(mask)\n    ndvi_masked_ts = [ndvi_data[t][mask] for t in range(time_steps_total)]\n    lst_masked_ts = [lst_data[t][mask] for t in range(time_steps_total)]\n    sm_masked_ts = [soil_moisture_data[t][mask] for t in range(time_steps_total)]\n    spi_masked_ts = [spi_data[t][mask] for t in range(time_steps_total)]\n    time_steps_to_process = range(lookback_period, time_steps_total)\n    if selected_time_steps is not None:\n        time_steps_to_process = [t for t in selected_time_steps if t >= lookback_period and t < time_steps_total]\n    for t in tqdm(time_steps_to_process, desc=\"Preparing prediction samples\"):\n        targets_for_current_t_step = spi_masked_ts[t]\n        for pixel_idx_in_mask in range(num_valid_pixels_in_mask):\n            current_target_value = targets_for_current_t_step[pixel_idx_in_mask]\n            if np.isnan(current_target_value): continue\n            single_pixel_feature_sequence = [np.array([\n                ndvi_masked_ts[t - lookback_period + i][pixel_idx_in_mask],\n                lst_masked_ts[t - lookback_period + i][pixel_idx_in_mask],\n                sm_masked_ts[t - lookback_period + i][pixel_idx_in_mask]\n            ]) for i in range(lookback_period)]\n            X_list.append(np.array(single_pixel_feature_sequence))\n            y_list.append(current_target_value)\n            sample_indices_output.append((t, pixel_idx_in_mask))\n    X, y = np.array(X_list), np.array(y_list).reshape(-1, 1)\n    print(f\"Final X shape for prediction: {X.shape}, y shape: {y.shape}, Samples: {len(sample_indices_output)}\")\n    original_data_info = {'SPI_full_shape': spi_data.shape, 'SPI_original_full': spi_data, 'time_steps_total': time_steps_total}\n    return X, y, mask, sample_indices_output, original_data_info\n\ndef generate_dates(start_year, start_month, num_timesteps):\n    dates = []\n    current_date = datetime.datetime(start_year, start_month, 1)\n    for _ in range(num_timesteps):\n        dates.append(current_date)\n        current_date = (current_date.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)\n    return dates\n\ndef predict_and_evaluate_country(model_path, scaler_X_path, scaler_y_path,\n                                     imputed_data_path_country,\n                                     ref_geotiff_path, shapefile_path,\n                                     output_base_dir, country_name,\n                                     data_start_year, data_start_month,\n                                     selected_time_steps=None):\n    print(f\"\\n--- Running Prediction and Evaluation for {country_name} ---\")\n    safe_country_name = country_name.lower().replace(' ', '_').replace(',', '')\n    output_dir = os.path.join(output_base_dir, f\"results_{safe_country_name}_{MODEL_NAME.lower().replace('+', '_')}\")\n    viz_dir = os.path.join(output_dir, \"visualizations\")\n    geotiff_dir_pred = os.path.join(output_dir, \"geotiffs_predicted\")\n    geotiff_dir_true = os.path.join(output_dir, \"geotiffs_true\")\n    os.makedirs(viz_dir, exist_ok=True)\n    os.makedirs(geotiff_dir_pred, exist_ok=True)\n    os.makedirs(geotiff_dir_true, exist_ok=True)\n    scaler_X, scaler_y = joblib.load(scaler_X_path), joblib.load(scaler_y_path)\n    model = SpiPredictorGATTransformer(input_size=3).to(DEVICE)\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE)); model.eval()\n    X_pred_all, y_true_all, data_mask, pred_sample_indices, orig_data_info = \\\n        load_data_for_prediction(imputed_data_path_country, LOOKBACK, selected_time_steps)\n    if X_pred_all.shape[0] == 0:\n        print(f\"No valid samples for prediction in {country_name}. Exiting.\"); return\n    dates_for_visualization = generate_dates(data_start_year, data_start_month, orig_data_info['time_steps_total'])\n    adj_matrix_template = torch.eye(LOOKBACK, dtype=torch.float32)\n    for i in range(LOOKBACK):\n        if i > 0: adj_matrix_template[i, i-1] = 1\n        if i < LOOKBACK - 1: adj_matrix_template[i, i+1] = 1\n    pred_dataset = TimeSeriesDatasetGAT(X_pred_all, y_true_all, adj_matrix_template.cpu(), scaler_X, scaler_y, is_train=False)\n    pred_loader = DataLoader(pred_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    preds_o, tgts_o, _, _, _, _ = evaluate_model_gat_based(model, pred_loader, scaler_y, DEVICE, f\"{MODEL_NAME} on {country_name}\")\n    recon_preds, unique_pred_t = reconstruct_rasters(preds_o, pred_sample_indices, data_mask, orig_data_info['SPI_full_shape'])\n    true_pred_rs = np.array([orig_data_info['SPI_original_full'][t] for t in unique_pred_t])\n    if ref_geotiff_path and os.path.exists(ref_geotiff_path):\n        print(f\"Saving predicted and true rasters as GeoTIFFs...\")\n        for i, time_index in enumerate(unique_pred_t):\n            date_str = dates_for_visualization[time_index].strftime('%Y_%m')\n            pred_filename = f\"predicted_spi_{safe_country_name}_{date_str}.tif\"\n            pred_filepath = os.path.join(geotiff_dir_pred, pred_filename)\n            save_raster_as_geotiff(recon_preds[i], ref_geotiff_path, pred_filepath)\n            true_filename = f\"true_spi_{safe_country_name}_{date_str}.tif\"\n            true_filepath = os.path.join(geotiff_dir_true, true_filename)\n            save_raster_as_geotiff(true_pred_rs[i], ref_geotiff_path, true_filepath)\n    visualize_raster_samples(\n        real_rasters_for_test_timesteps=true_pred_rs, pred_rasters_for_test_timesteps=recon_preds,\n        mask_for_visualization=data_mask, unique_test_time_indices=unique_pred_t,\n        model_name=f\"{MODEL_NAME}_{country_name}\", dates_list=dates_for_visualization,\n        ref_geotiff_path=ref_geotiff_path, shapefile_path=shapefile_path,\n        country_name=country_name, output_dir=viz_dir\n    )\n    print(f\"Prediction and evaluation for {country_name} complete.\")\n\n\n# --- MODIFIED: This is the only function that has been changed. ---\ndef create_combined_visualization_detailed(countries_config, country_shapefiles, ref_geotiffs, output_base_dir, model_name, date_str):\n    print(f\"\\n--- Creating Detailed Combined Visualization for Time Step: {date_str} ---\")\n    # Note the change to sharex=False, sharey=False to allow for individual colorbars to fit properly\n    fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n\n    colors_spi = ['#730000', '#E60000', '#FFAA00', '#FCD37F', '#FFFF00', '#FFFFFF', '#DCF8FF', '#96D2FF', '#46A5FF', '#0000FF', '#000080']\n    cmap_spi = plt.cm.colors.LinearSegmentedColormap.from_list('usdm_spi', colors_spi, N=256)\n    norm_spi = plt.Normalize(vmin=-3, vmax=3)\n    cmap_diff = 'RdBu_r'\n    \n    max_abs_diff = 0.0\n    country_keys = list(countries_config.keys())\n    \n    # Store image handles for colorbars\n    im_handles = {'true': [], 'pred': [], 'diff': []}\n\n    print(\"Pass 1: Calculating difference range...\")\n    for country_name in country_keys:\n        safe_country_name = country_name.lower().replace(' ', '_').replace(',', '')\n        results_dir = os.path.join(output_base_dir, f\"results_{safe_country_name}_{model_name.lower().replace('+', '_')}\")\n        true_path = os.path.join(results_dir, \"geotiffs_true\", f\"true_spi_{safe_country_name}_{date_str}.tif\")\n        pred_path = os.path.join(results_dir, \"geotiffs_predicted\", f\"predicted_spi_{safe_country_name}_{date_str}.tif\")\n        if os.path.exists(true_path) and os.path.exists(pred_path):\n            with rasterio.open(true_path) as t_src, rasterio.open(pred_path) as p_src:\n                diff = t_src.read(1, masked=True) - p_src.read(1, masked=True)\n                if not np.all(diff.mask):\n                    current_max = np.ma.max(np.abs(diff))\n                    if current_max > max_abs_diff:\n                        max_abs_diff = current_max\n    \n    max_abs_diff = max(max_abs_diff, 1.0)\n    norm_diff = plt.Normalize(vmin=-max_abs_diff, vmax=max_abs_diff)\n    \n    print(f\"Pass 2: Plotting data with diff range [{-max_abs_diff:.2f}, {max_abs_diff:.2f}]...\")\n    all_borders_list = []\n    for country_name in country_keys:\n        safe_country_name = country_name.lower().replace(' ', '_').replace(',', '')\n        results_dir = os.path.join(output_base_dir, f\"results_{safe_country_name}_{model_name.lower().replace('+', '_')}\")\n        true_path = os.path.join(results_dir, \"geotiffs_true\", f\"true_spi_{safe_country_name}_{date_str}.tif\")\n        pred_path = os.path.join(results_dir, \"geotiffs_predicted\", f\"predicted_spi_{safe_country_name}_{date_str}.tif\")\n\n        if os.path.exists(true_path) and os.path.exists(pred_path):\n            with rasterio.open(true_path) as src:\n                show(src, ax=axes[0], cmap=cmap_spi, norm=norm_spi)\n            with rasterio.open(pred_path) as src:\n                show(src, ax=axes[1], cmap=cmap_spi, norm=norm_spi)\n            # Difference plot requires reading the data again\n            with rasterio.open(true_path) as t_src, rasterio.open(pred_path) as p_src:\n                diff_data = t_src.read(1, masked=True) - p_src.read(1, masked=True)\n                show(diff_data, ax=axes[2], transform=t_src.transform, cmap=cmap_diff, norm=norm_diff)\n\n        shapefile_path = country_shapefiles.get(country_name)\n        if shapefile_path and os.path.exists(shapefile_path):\n            gdf = gpd.read_file(shapefile_path)\n            with rasterio.open(ref_geotiffs[country_name]) as ref_src:\n                gdf = gdf.to_crs(ref_src.crs)\n                all_borders_list.append(gdf)\n                centroid = gdf.geometry.centroid.iloc[0]\n                for ax in axes:\n                    ax.text(centroid.x, centroid.y, country_name, ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n    \n    if all_borders_list:\n        combined_borders = gpd.pd.concat(all_borders_list, ignore_index=True)\n        for ax in axes:\n            combined_borders.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=1.5)\n\n    plot_titles = ['True SPI', 'Predicted SPI', 'Difference (True - Predicted)']\n    for i, ax in enumerate(axes):\n        ax.set_title(plot_titles[i], fontsize=16)\n        ax.set_xlabel('Longitude'); ax.set_ylabel('Latitude')\n        ax.tick_params(axis='x', rotation=45); ax.grid(True, linestyle='--', alpha=0.6)\n    axes[1].set_ylabel(''); axes[2].set_ylabel('')\n\n    # --- MODIFICATION START ---\n    # Create mappable objects that represent the color scales\n    spi_mappable = plt.cm.ScalarMappable(norm=norm_spi, cmap=cmap_spi)\n    diff_mappable = plt.cm.ScalarMappable(norm=norm_diff, cmap=cmap_diff)\n    \n    # Create a separate colorbar for each subplot, matching the single-country style\n    fig.colorbar(spi_mappable, ax=axes[0], label='SPI Value', fraction=0.046, pad=0.04)\n    fig.colorbar(spi_mappable, ax=axes[1], label='SPI Value', fraction=0.046, pad=0.04)\n    fig.colorbar(diff_mappable, ax=axes[2], label='Difference (True - Predicted)', fraction=0.046, pad=0.04)\n    # --- MODIFICATION END ---\n    \n    fig.suptitle(f'Combined Regional SPI Comparison - {date_str.replace(\"_\", \"-\")}', fontsize=20)\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    output_path = os.path.join(output_base_dir, f'combined_comparison_{date_str}.png')\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"Combined visualization saved to: {output_path}\")\n\n\n# --- Main execution block for prediction (unchanged) ---\nif __name__ == \"__main__\":\n    MODEL_DIR = \"/kaggle/input/gat-transformer-guinea-bissau/pytorch/trained-on-guinea-bissau/2/results_gat_transformer\"\n    MODEL_WEIGHTS_PATH = os.path.join(MODEL_DIR, \"best_spi_gat_transformer_model.pt\")\n    SCALER_X_PATH = os.path.join(MODEL_DIR, \"scaler_X_gat_transformer.pkl\")\n    SCALER_Y_PATH = os.path.join(MODEL_DIR, \"scaler_y_gat_transformer.pkl\")\n    OUTPUT_BASE_DIR = \"/kaggle/working\"\n\n    REFERENCE_GEOTIFFS = {\n        \"Gambia\": \"/kaggle/input/ndvi-month/NDVI_Images/Gambia,_The/NDVI_Gambia,_The_2000_02.tif\",\n        \"Guinea\": \"/kaggle/input/ndvi-month/NDVI_Images/Guinea/NDVI_Guinea_2000_02.tif\",\n        \"Guinea-Bissau\": \"/kaggle/input/ndvi-month/NDVI_Images/Guinea-Bissau/NDVI_Guinea-Bissau_2000_02.tif\",\n        \"Senegal\": \"/kaggle/input/ndvi-month/NDVI_Images/Senegal/NDVI_Senegal_2000_02.tif\"\n    }\n    COUNTRY_SHAPEFILES = {\n        \"Guinea\": \"/kaggle/input/shape-files-study-area/gadm41_GIN_0.shp\",\n        \"Gambia\": \"/kaggle/input/shape-files-study-area/gadm41_GMB_0.shp\",\n        \"Guinea-Bissau\": \"/kaggle/input/shape-files-study-area/gadm41_GNB_0.shp\",\n        \"Senegal\": \"/kaggle/input/shape-files-study-area/gadm41_SEN_0.shp\"\n    }\n\n    timesteps_to_predict = [270, 275, 280, 282, 285]\n    COUNTRIES_CONFIG = {\n        \"Senegal\": {\"imputed_data_path\": \"/kaggle/input/imputed-data/Senegal_combined_imputed.npz\", \"start_year\": 2000, \"start_month\": 2, \"selected_timesteps\": timesteps_to_predict},\n        \"Gambia\": {\"imputed_data_path\": \"/kaggle/input/imputed-data/Gambia,_The_combined_imputed.npz\", \"start_year\": 2000, \"start_month\": 2, \"selected_timesteps\": timesteps_to_predict},\n        \"Guinea-Bissau\": {\"imputed_data_path\": \"/kaggle/input/imputed-data/Guinea-Bissau_combined_imputed.npz\", \"start_year\": 2000, \"start_month\": 2, \"selected_timesteps\": timesteps_to_predict},\n        \"Guinea\": {\"imputed_data_path\": \"/kaggle/input/imputed-data/Guinea_combined_imputed.npz\", \"start_year\": 2000, \"start_month\": 2, \"selected_timesteps\": timesteps_to_predict}\n    }\n\n    for country_name, config in COUNTRIES_CONFIG.items():\n        predict_and_evaluate_country(\n            model_path=MODEL_WEIGHTS_PATH,\n            scaler_X_path=SCALER_X_PATH,\n            scaler_y_path=SCALER_Y_PATH,\n            imputed_data_path_country=config[\"imputed_data_path\"],\n            ref_geotiff_path=REFERENCE_GEOTIFFS.get(country_name),\n            shapefile_path=COUNTRY_SHAPEFILES.get(country_name),\n            output_base_dir=OUTPUT_BASE_DIR,\n            country_name=country_name,\n            data_start_year=config[\"start_year\"],\n            data_start_month=config[\"start_month\"],\n            selected_time_steps=config[\"selected_timesteps\"]\n        )\n    \n    if timesteps_to_predict:\n        dates = generate_dates(2000, 2, max(timesteps_to_predict) + 5) \n        for time_idx in timesteps_to_predict:\n            date_to_plot_str = dates[time_idx].strftime('%Y_%m')\n            create_combined_visualization_detailed(\n                countries_config=COUNTRIES_CONFIG,\n                country_shapefiles=COUNTRY_SHAPEFILES,\n                ref_geotiffs=REFERENCE_GEOTIFFS,\n                output_base_dir=OUTPUT_BASE_DIR,\n                model_name=MODEL_NAME,\n                date_str=date_to_plot_str\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T04:19:20.843545Z","iopub.execute_input":"2025-07-13T04:19:20.843866Z","iopub.status.idle":"2025-07-13T04:23:59.778565Z","shell.execute_reply.started":"2025-07-13T04:19:20.843844Z","shell.execute_reply":"2025-07-13T04:23:59.777783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport rasterio\nfrom scipy import stats\nfrom sklearn.metrics import r2_score\nimport datetime\n\n# --- Helper function to generate dates ---\ndef generate_dates(start_year, start_month, num_timesteps):\n    dates = []\n    current_date = datetime.datetime(start_year, start_month, 1)\n    for _ in range(num_timesteps):\n        dates.append(current_date)\n        if current_date.month == 12:\n            current_date = datetime.datetime(current_date.year + 1, 1, 1)\n        else:\n            current_date = datetime.datetime(current_date.year, current_date.month + 1, 1)\n    return dates\n\n# --- Main function for pixel analysis ---\ndef plot_pixel_analysis(ax_scatter, ax_timeseries, country_name, coords, predictions_path, ref_geotiff_path, imputed_data_path, all_dates):\n    \"\"\"\n    Creates a scatter and time series plot for a single pixel using specific data paths.\n    \"\"\"\n    print(f\"--- Analyzing pixel for {country_name} at {coords} ---\")\n    lat, lon = coords\n    \n    # --- 1. Check if all required files exist ---\n    if not all(os.path.exists(p) for p in [predictions_path, ref_geotiff_path, imputed_data_path]):\n        print(f\"ERROR: Missing data files for {country_name}. Skipping analysis.\")\n        ax_scatter.text(0.5, 0.5, 'Data not found', ha='center', color='red')\n        ax_timeseries.text(0.5, 0.5, 'Data not found', ha='center', color='red')\n        ax_scatter.set_title(f\"{country_name}: Error\")\n        return\n\n    # --- 2. Load data and find the pixel index ---\n    predictions_data = np.load(predictions_path)\n    predicted_rasters = predictions_data['predictions']\n    predicted_time_indices = predictions_data['time_steps']\n    \n    original_data = np.load(imputed_data_path)\n    true_rasters = original_data['SPI_original']\n\n    with rasterio.open(ref_geotiff_path) as src:\n        row, col = src.index(lon, lat)\n\n    # --- 3. Extract Time Series for the pixel ---\n    forecasted_values = predicted_rasters[:, row, col]\n    actual_values = true_rasters[predicted_time_indices, row, col]\n    dates_slice = [all_dates[i] for i in predicted_time_indices]\n    \n    valid_mask = ~np.isnan(actual_values) & ~np.isnan(forecasted_values)\n    actual_values = actual_values[valid_mask]\n    forecasted_values = forecasted_values[valid_mask]\n    dates_slice = np.array(dates_slice)[valid_mask]\n\n    if len(actual_values) < 2:\n        print(f\"Not enough valid data points for pixel in {country_name}.\")\n        ax_scatter.text(0.5, 0.5, 'Not enough data', ha='center')\n        return\n\n    # --- 4. Create Scatter Plot ---\n    r2 = r2_score(actual_values, forecasted_values)\n    slope, intercept, _, _, _ = stats.linregress(actual_values, forecasted_values)\n    \n    ax_scatter.plot(actual_values, forecasted_values, 'o', color='royalblue', alpha=0.6)\n    line_1_1 = np.linspace(min(actual_values.min(), forecasted_values.min()), max(actual_values.max(), forecasted_values.max()), 100)\n    ax_scatter.plot(line_1_1, line_1_1, 'k--', label='1:1 line')\n    ax_scatter.plot(line_1_1, slope * line_1_1 + intercept, 'g-', label='Fit line')\n    \n    ax_scatter.set_xlabel(\"Actual\")\n    ax_scatter.set_ylabel(\"Forecasted\")\n    \n    title_text = f\"{country_name} (Lat: {lat:.2f}, Lon: {lon:.2f})\\n$R^2 = {r2:.4f}$\"\n    ax_scatter.set_title(title_text)\n    \n    ax_scatter.legend()\n    ax_scatter.grid(True)\n    ax_scatter.set_aspect('equal', 'box')\n\n    # --- 5. Create Time Series Plot ---\n    ax_timeseries.plot(dates_slice, actual_values, label='Actual', color='royalblue')\n    ax_timeseries.plot(dates_slice, forecasted_values, label='Forecasted', color='red')\n    ax_timeseries.set_xlabel(\"Date\")\n    ax_timeseries.set_ylabel(\"Drought (SPI)\")\n    \n    # CHANGED: Explicitly set the legend location\n    ax_timeseries.legend(loc='upper right')\n    \n    ax_timeseries.grid(True)\n    plt.setp(ax_timeseries.get_xticklabels(), rotation=30, ha='right')\n\n\n# =============================================================================\n# == SCRIPT EXECUTION =========================================================\n# =============================================================================\n\nif __name__ == \"__main__\":\n    \n    # --- 1. Define Paths and Locations ---\n    IMPUTED_DATA_PATHS = {\n        \"Gambia\": \"/kaggle/input/imputed-data/Gambia,_The_combined_imputed.npz\",\n        \"Guinea-Bissau\": \"/kaggle/input/imputed-data/Guinea-Bissau_combined_imputed.npz\"\n    }\n\n    REFERENCE_GEOTIFFS = {\n        \"Gambia\": \"/kaggle/input/ndvi-month/NDVI_Images/Gambia,_The/NDVI_Gambia,_The_2000_02.tif\",\n        \"Guinea-Bissau\": \"/kaggle/input/ndvi-month/NDVI_Images/Guinea-Bissau/NDVI_Guinea-Bissau_2000_02.tif\",\n    }\n    \n    PREDICTION_NPZ_PATHS = {\n        \"Gambia\": \"/kaggle/input/gambia-predictions/results_gat_transformer/spi_gat_transformer_predictions.npz\",\n        \"Guinea-Bissau\": \"/kaggle/input/gat-transformer-guinea-bissau/pytorch/trained-on-guinea-bissau/2/results_gat_transformer/spi_gat_transformer_predictions.npz\"\n    }\n\n    PIXEL_LOCATIONS = {\n        \"Gambia\": (13.4, -15.5),\n        \"Guinea-Bissau\": (12.0, -15.0)\n    }\n\n    # --- 2. Setup Plotting ---\n    fig, axes = plt.subplots(2, 2, figsize=(12, 9))\n    fig.suptitle(\"Pixel-Level Analysis: Actual vs. Forecasted SPI\", fontsize=16)\n\n    # From Feb 2000 to Dec 2023 is 287 months.\n    all_dataset_dates = generate_dates(2000, 2, 287) \n\n    # --- 3. Loop and Plot ---\n    for i, country_name in enumerate(PIXEL_LOCATIONS.keys()):\n        ax_scatter = axes[i, 0]\n        ax_timeseries = axes[i, 1]\n        \n        plot_pixel_analysis(\n            ax_scatter, \n            ax_timeseries, \n            country_name, \n            PIXEL_LOCATIONS[country_name],\n            PREDICTION_NPZ_PATHS[country_name],\n            REFERENCE_GEOTIFFS[country_name],\n            IMPUTED_DATA_PATHS[country_name],\n            all_dataset_dates\n        )\n\n    plt.tight_layout(rect=[0, 0, 1, 0.95])\n    \n    # --- 4. Save Final Figure ---\n    output_dir = \"/kaggle/working/\"\n    final_plot_path = os.path.join(output_dir, \"pixel_analysis_summary_Gambia_GB.png\")\n    plt.savefig(final_plot_path, dpi=300)\n    plt.show()\n    \n    print(f\"\\nPixel analysis summary saved to: {final_plot_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T04:18:38.898632Z","iopub.execute_input":"2025-07-13T04:18:38.899393Z","iopub.status.idle":"2025-07-13T04:18:41.393028Z","shell.execute_reply.started":"2025-07-13T04:18:38.899361Z","shell.execute_reply":"2025-07-13T04:18:41.392266Z"}},"outputs":[],"execution_count":null}]}