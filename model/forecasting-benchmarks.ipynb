{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":260,"sourceType":"datasetVersion","datasetId":122},{"sourceId":6968385,"sourceType":"datasetVersion","datasetId":4003664},{"sourceId":9573068,"sourceType":"datasetVersion","datasetId":5835460}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport math\nimport joblib\nimport itertools\n\n# --- Configuration ---\n# These will be overridden by the grid search, but serve as defaults if not in grid\nMODEL_NAME = \"GAT_Transformer_Power\"\nLOOKBACK = 24 * 7  # Using a fixed 1-week lookback for all grid search runs\nNUM_EPOCHS = 100\nPATIENCE = 15\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# --- Generic function to load tabular Multivariate Time Series data ---\ndef load_mts_data(data_df, feature_cols, target_cols, lookback_period):\n    print(f\"--- Creating sequences with lookback={lookback_period} ---\")\n    features = data_df[feature_cols].values\n    targets = data_df[target_cols].values\n    X_list, y_list = [], []\n    for i in range(lookback_period, len(data_df)):\n        X_list.append(features[i - lookback_period : i])\n        y_list.append(targets[i])\n    X, y = np.array(X_list), np.array(y_list)\n    print(f\"Loaded MTS data: X shape: {X.shape}, y shape: {y.shape}\")\n    return X, y\n\n# --- Chronological Train-Test Split ---\ndef chronological_train_test_split(X, y, test_ratio=0.2):\n    num_samples = X.shape[0]\n    num_test_samples = int(num_samples * test_ratio)\n    split_index = num_samples - num_test_samples\n    X_train, X_test = X[:split_index], X[split_index:]\n    y_train, y_test = y[:split_index], y[split_index:]\n    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n    return X_train, X_test, y_train, y_test\n\n# --- TimeSeriesDataset for GAT (Optimized Adjacency) ---\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, features, targets, single_adj_matrix, scaler_X=None, scaler_y=None, is_train=True):\n        original_shape = features.shape\n        if is_train and scaler_X is None:\n            self.scaler_X = StandardScaler()\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.fit_transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        elif not is_train and scaler_X is not None:\n            self.scaler_X = scaler_X\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        else: self.features, self.scaler_X = features, scaler_X\n\n        if is_train and scaler_y is None:\n            self.scaler_y = StandardScaler(); self.targets = self.scaler_y.fit_transform(targets)\n        elif not is_train and scaler_y is not None:\n            self.scaler_y = scaler_y; self.targets = self.scaler_y.transform(targets)\n        else: self.targets, self.scaler_y = targets, scaler_y\n\n        self.features = torch.tensor(self.features, dtype=torch.float32)\n        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n\n        if not isinstance(single_adj_matrix, torch.Tensor):\n            self.single_adj_matrix = torch.tensor(single_adj_matrix, dtype=torch.float32)\n        else:\n            self.single_adj_matrix = single_adj_matrix.to(dtype=torch.float32)\n\n    def __len__(self): return len(self.features)\n    def __getitem__(self, idx): return self.features[idx], self.single_adj_matrix, self.targets[idx]\n    def get_scalers(self): return self.scaler_X, self.scaler_y\n\n# --- Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term); pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0); self.register_buffer('pe', pe)\n    def forward(self, x):\n        pe_sliced = self.pe[:, :x.size(1), :]\n        try:\n            result = x + pe_sliced\n        except RuntimeError as e:\n            print(f\"ERROR in PositionalEncoding: x.shape={x.shape}, pe_sliced.shape={pe_sliced.shape}\")\n            raise e\n        return result\n\n# --- Graph Attention Layer & MultiHeadGraphAttention ---\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n        super(GraphAttentionLayer, self).__init__()\n        self.dropout_val, self.alpha = dropout, alpha\n        self.W = nn.Parameter(torch.empty(size=(in_features, out_features))); nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1))); nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu = nn.LeakyReLU(self.alpha); self.dropout_layer = nn.Dropout(self.dropout_val)\n    def forward(self, h, adj):\n        Wh = torch.matmul(h, self.W); a_input = self._prepare_attention_input(Wh)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n        zero_vec = -9e15*torch.ones_like(e); attention_masked = torch.where(adj.unsqueeze(0) > 0, e, zero_vec)\n        attention_softmax = F.softmax(attention_masked, dim=-1); attention_dropout = self.dropout_layer(attention_softmax)\n        h_prime = torch.matmul(attention_dropout, Wh); return F.elu(h_prime)\n    def _prepare_attention_input(self, Wh):\n        B, N, _ = Wh.size(); Wh_i = Wh.unsqueeze(2).expand(B, N, N, -1); Wh_j = Wh.unsqueeze(1).expand(B, N, N, -1)\n        return torch.cat([Wh_i, Wh_j], dim=-1)\n\nclass MultiHeadGraphAttention(nn.Module):\n    def __init__(self, in_features, out_features_per_head, n_heads, dropout=0.2, alpha=0.2, concat=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.concat = concat\n        self.attentions = nn.ModuleList([GraphAttentionLayer(in_features, out_features_per_head, dropout=dropout, alpha=alpha) for _ in range(n_heads)])\n        self.out_dim = out_features_per_head * n_heads if concat else out_features_per_head\n    def forward(self, x, adj):\n        head_outputs = [att(x, adj) for att in self.attentions]\n        return torch.cat(head_outputs, dim=-1) if self.concat else torch.mean(torch.stack(head_outputs, dim=-1), dim=-1)\n\n# --- GAT+Transformer Model ---\nclass MTSPredictorGATTransformer(nn.Module):\n    def __init__(self, input_size, output_size, d_model_gat=64, gat_heads=4,\n                 d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n                 dim_feedforward_transformer=256, dropout_rate=0.2):\n        super(MTSPredictorGATTransformer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.gat_input_proj = nn.Linear(input_size, d_model_gat)\n        self.gat_attention = MultiHeadGraphAttention(in_features=d_model_gat, out_features_per_head=d_model_gat // gat_heads,\n                                                     n_heads=gat_heads, dropout=self.dropout_rate, concat=True)\n        gat_output_dim = self.gat_attention.out_dim\n        self.pos_encoder = PositionalEncoding(gat_output_dim, max_len=LOOKBACK + 100)\n        transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=gat_output_dim, nhead=transformer_heads,\n                                                               dim_feedforward=dim_feedforward_transformer,\n                                                               dropout=self.dropout_rate, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_transformer_layers)\n        self.fc = nn.Sequential(nn.Linear(gat_output_dim, 64), nn.ReLU(), nn.Dropout(self.dropout_rate),\n                                nn.Linear(64, 32), nn.ReLU(), nn.Dropout(self.dropout_rate),\n                                nn.Linear(32, output_size))\n    def forward(self, x, adj):\n        x_proj = F.elu(self.gat_input_proj(x))\n        x_gat_attended = self.gat_attention(x_proj, adj)\n        if x_gat_attended.dim() == 4 and x_gat_attended.shape[0] == 1:\n             x_gat_attended = x_gat_attended.squeeze(0)\n        x_pos_encoded = self.pos_encoder(x_gat_attended)\n        x_transformed = self.transformer_encoder(x_pos_encoded)\n        x_last_step = x_transformed[:, -1, :]\n        return self.fc(x_last_step)\n\n# --- Training Function ---\ndef train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device, model_save_name):\n    best_val_loss = float('inf'); epochs_no_improve = 0\n    train_losses_history, val_losses_history = [], []\n    for epoch in range(num_epochs):\n        model.train(); epoch_train_loss = 0\n        for batch_X, batch_adj_single, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n            batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n            outputs = model(batch_X, batch_adj_single[0])\n            loss = criterion(outputs, batch_y)\n            optimizer.zero_grad(); loss.backward(); optimizer.step(); epoch_train_loss += loss.item()\n        epoch_train_loss /= len(train_loader); train_losses_history.append(epoch_train_loss)\n\n        model.eval(); epoch_val_loss = 0\n        with torch.no_grad():\n            for batch_X, batch_adj_single, batch_y in val_loader:\n                batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n                outputs = model(batch_X, batch_adj_single[0])\n                loss = criterion(outputs, batch_y); epoch_val_loss += loss.item()\n        epoch_val_loss /= len(val_loader); val_losses_history.append(epoch_val_loss)\n\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch {epoch+1}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f}, LR={current_lr:.6f}\")\n        if scheduler: scheduler.step(epoch_val_loss)\n\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss; epochs_no_improve = 0\n            torch.save(model.state_dict(), model_save_name)\n        else:\n            epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(f\"Early stopping @ epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n            break\n    if os.path.exists(model_save_name):\n        model.load_state_dict(torch.load(model_save_name))\n    return model, train_losses_history, val_losses_history, best_val_loss\n\n# --- Evaluation Function ---\ndef evaluate_model_gat_based(model, test_loader, scaler_y, device, model_name=\"GAT-based Model\"):\n    model.eval(); all_preds_s, all_targets_s = [], []\n    with torch.no_grad():\n        for batch_X, batch_adj_single, batch_y_s in test_loader:\n            batch_X, batch_adj_single = batch_X.to(device), batch_adj_single.to(device)\n            outputs_s = model(batch_X, batch_adj_single[0])\n            all_preds_s.append(outputs_s.cpu().numpy()); all_targets_s.append(batch_y_s.cpu().numpy())\n    preds_s_np = np.vstack(all_preds_s); tgts_s_np = np.vstack(all_targets_s)\n\n    if scaler_y:\n        preds_o = scaler_y.inverse_transform(preds_s_np); tgts_o = scaler_y.inverse_transform(tgts_s_np)\n    else: preds_o, tgts_o = preds_s_np, tgts_s_np\n\n    mse = mean_squared_error(tgts_o, preds_o); rmse = np.sqrt(mse); r2 = r2_score(tgts_o, preds_o); mae = mean_absolute_error(tgts_o, preds_o)\n    print(f\"\\nEvaluation metrics ({model_name}):\\n MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n    return preds_o, tgts_o, mse, rmse, r2, mae\n\n# --- Plotting Functions ---\ndef plot_training_losses(train_losses, val_losses, model_name, save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'training_losses_{model_name}.png')\n    plt.figure(figsize=(10, 6)); plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n    plt.xlabel('Epochs'); plt.ylabel('Loss (MSE)'); plt.title(f'Training & Validation Losses ({model_name})')\n    plt.legend(); plt.grid(True); plt.savefig(save_path); plt.show()\n\ndef plot_predictions_vs_true(targets_original, predictions_original, model_name, target_name=\"Target\", save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'predictions_vs_true_{model_name}.png')\n    plt.figure(figsize=(12, 8)); plt.scatter(targets_original, predictions_original, alpha=0.3, label='Sample Predictions')\n    min_val = min(targets_original.ravel().min(), predictions_original.ravel().min())\n    max_val = max(targets_original.ravel().max(), predictions_original.ravel().max())\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='y=x (Perfect Prediction)')\n    plt.xlabel(f'True Values ({target_name})'); plt.ylabel(f'Predicted Values ({target_name})')\n    plt.title(f'Predictions vs True Values for {target_name} ({model_name})')\n    plt.legend(); plt.grid(True); plt.tight_layout(); plt.savefig(save_path); plt.show()\n\n# --- New Function to run a single training session for grid search ---\ndef run_training_session(params, train_dataset, val_dataset, adj_matrix_template, model_input_size, model_output_size, output_dir):\n    \"\"\"\n    Encapsulates a full training run for a given set of hyperparameters.\n    \"\"\"\n    print(f\"\\n--- Running with params: {params} ---\")\n\n    # Unpack hyperparameters\n    lr = params['LEARNING_RATE']\n    dropout_rate = params['DROPOUT_RATE']\n    weight_decay = params['WEIGHT_DECAY']\n    batch_size = params['BATCH_SIZE']\n\n    # Create DataLoaders with current batch size\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\n    # Initialize model with current dropout rate\n    model = MTSPredictorGATTransformer(\n        input_size=model_input_size,\n        output_size=model_output_size,\n        dropout_rate=dropout_rate\n    ).to(DEVICE)\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7, verbose=False)\n\n    # Define a unique model save path for this run\n    param_str = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n    model_save_name = os.path.join(output_dir, f\"best_model_{param_str}.pt\")\n\n    # Run training\n    _, _, _, best_val_loss = train_model_gat_based(\n        model, train_loader, val_loader, criterion, optimizer, scheduler,\n        NUM_EPOCHS, PATIENCE, DEVICE, model_save_name\n    )\n\n    return best_val_loss\n\n# --- Main Function ---\ndef main_mts_forecaster():\n    print(f\"--- Running {MODEL_NAME} Grid Search ---\")\n    print(f\"Using device: {DEVICE}\")\n    output_dir = f\"results_{MODEL_NAME.lower()}_grid_search\"\n    viz_dir = os.path.join(output_dir, \"visualizations\")\n    if not os.path.exists(viz_dir): os.makedirs(viz_dir)\n\n    # --- 1. DEFINE HYPERPARAMETER GRID ---\n    param_grid = {\n        'LEARNING_RATE': [0.001, 0.0005],\n        'DROPOUT_RATE': [0.3, 0.5],\n        'WEIGHT_DECAY': [1e-5, 1e-4],\n        'BATCH_SIZE': [64] # Keeping batch size fixed to reduce run time, can add more e.g., [32, 64]\n    }\n    print(\"--- Hyperparameter Grid ---\")\n    print(param_grid)\n\n    # --- 2. LOAD AND PREPROCESS DATA ONCE ---\n    data_path = '/kaggle/input/electric-power-consumption-data-set/household_power_consumption.txt'\n    if not os.path.exists(data_path):\n        print(f\"ERROR: Dataset not found at '{data_path}'\"); return\n\n    print(\"Loading and preprocessing data... This may take a moment.\")\n    df = pd.read_csv(\n        data_path, sep=';', low_memory=False, na_values=['?'],\n        parse_dates={'datetime': ['Date', 'Time']},\n        infer_datetime_format=True, index_col='datetime'\n    )\n    for col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    df.ffill(inplace=True)\n\n    agg_rules = {\n        'Global_active_power': 'sum', 'Global_reactive_power': 'sum',\n        'Voltage': 'mean', 'Global_intensity': 'mean',\n        'Sub_metering_1': 'sum', 'Sub_metering_2': 'sum', 'Sub_metering_3': 'sum'\n    }\n    df_resampled = df.resample('h').agg(agg_rules)\n    if df_resampled.isnull().sum().any():\n        df_resampled.ffill(inplace=True); df_resampled.bfill(inplace=True)\n\n    print(f\"Data resampled to hourly. New shape: {df_resampled.shape}\")\n    \n    feature_cols = [\n        'Global_active_power', 'Global_reactive_power', 'Voltage', \n        'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3'\n    ]\n    target_cols = ['Global_active_power']\n    target_name_for_plot = \"Global Active Power\"\n    \n    X_all, y_all = load_mts_data(df_resampled, feature_cols, target_cols, LOOKBACK)\n    if X_all.shape[0] == 0: print(\"No valid samples generated. Exiting.\"); return\n    \n    X_tv_and_val, X_test, y_tv_and_val, y_test = chronological_train_test_split(X_all, y_all, test_ratio=0.2)\n    if X_tv_and_val.shape[0] == 0: print(f\"No training/validation samples. Exiting.\"); return\n\n    # --- 3. CREATE DATASETS ONCE ---\n    adj_matrix_template = torch.zeros(LOOKBACK, LOOKBACK, dtype=torch.float32)\n    for i in range(LOOKBACK):\n        if i > 0: adj_matrix_template[i, i-1] = 1\n        if i < LOOKBACK - 1: adj_matrix_template[i, i+1] = 1\n        adj_matrix_template[i, i] = 1\n    adj_matrix_template = adj_matrix_template.to(DEVICE)\n\n    full_train_val_dataset = TimeSeriesDataset(X_tv_and_val, y_tv_and_val, adj_matrix_template.cpu(), is_train=True)\n    scaler_X, scaler_y = full_train_val_dataset.get_scalers()\n\n    train_s = int(0.8 * len(full_train_val_dataset))\n    val_s = len(full_train_val_dataset) - train_s\n    gen = torch.Generator().manual_seed(42)\n    train_dataset, val_dataset = torch.utils.data.random_split(full_train_val_dataset, [train_s, val_s], generator=gen)\n\n    # --- 4. RUN GRID SEARCH LOOP ---\n    best_params = {}\n    best_val_loss = float('inf')\n    \n    param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n\n    for params in param_combinations:\n        val_loss = run_training_session(\n            params=params,\n            train_dataset=train_dataset,\n            val_dataset=val_dataset,\n            adj_matrix_template=adj_matrix_template,\n            model_input_size=X_tv_and_val.shape[-1],\n            model_output_size=y_tv_and_val.shape[-1],\n            output_dir=output_dir\n        )\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_params = params\n            print(f\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n            print(f\"!!! New best validation loss: {best_val_loss:.4f} with params: {best_params}\")\n            print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n\n    print(f\"\\n--- Grid Search Complete ---\")\n    print(f\"Best validation loss found: {best_val_loss:.4f}\")\n    print(f\"Best hyperparameters: {best_params}\")\n\n    # --- 5. FINAL EVALUATION WITH BEST HYPERPARAMETERS ---\n    print(\"\\n--- Training final model with best hyperparameters on all training data ---\")\n    \n    final_model = MTSPredictorGATTransformer(\n        input_size=X_tv_and_val.shape[-1],\n        output_size=y_tv_and_val.shape[-1],\n        dropout_rate=best_params['DROPOUT_RATE']\n    ).to(DEVICE)\n    \n    # --- FIX WAS HERE ---\n    final_optimizer = optim.Adam(\n        final_model.parameters(), \n        lr=best_params['LEARNING_RATE'], \n        weight_decay=best_params['WEIGHT_DECAY']\n    )\n    # --- END OF FIX ---\n\n    final_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        final_optimizer, mode='min', factor=0.1, patience=7, verbose=True\n    )\n    final_criterion = nn.MSELoss()\n\n    final_train_loader = DataLoader(full_train_val_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=True)\n    final_val_loader = DataLoader(val_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n    \n    final_model_save_path = os.path.join(output_dir, \"final_best_model.pt\")\n    \n    print(\"Starting final training run...\")\n    trained_model, train_L, val_L, _ = train_model_gat_based(\n        final_model, final_train_loader, final_val_loader, final_criterion, final_optimizer, final_scheduler,\n        NUM_EPOCHS, PATIENCE, DEVICE, final_model_save_path\n    )\n    \n    plot_training_losses(train_L, val_L, f\"{MODEL_NAME}_final\", save_dir=viz_dir)\n   \n    if X_test.shape[0] > 0:\n        print(\"\\n--- Evaluating final model on test set ---\")\n        test_dataset = TimeSeriesDataset(X_test, y_test, adj_matrix_template.cpu(), scaler_X, scaler_y, is_train=False)\n        test_loader = DataLoader(test_dataset, batch_size=best_params['BATCH_SIZE'], shuffle=False)\n        \n        preds_o, tgts_o, _, _, _, _ = evaluate_model_gat_based(trained_model, test_loader, scaler_y, DEVICE, f\"{MODEL_NAME}_final\")\n        plot_predictions_vs_true(tgts_o, preds_o, f\"{MODEL_NAME}_final\", target_name=target_name_for_plot, save_dir=viz_dir)\n    else:\n        print(\"Test set was empty. Skipping final evaluation.\")\n\n    print(f\"Grid search and final evaluation complete. Results in {output_dir}\")\nif __name__ == \"__main__\":\n    main_mts_forecaster()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T14:53:48.956886Z","iopub.execute_input":"2025-06-13T14:53:48.957195Z","iopub.status.idle":"2025-06-13T14:55:30.239538Z","shell.execute_reply.started":"2025-06-13T14:53:48.957171Z","shell.execute_reply":"2025-06-13T14:55:30.237977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport math\nimport joblib\n\n# --- Configuration ---\nMODEL_NAME = \"GAT_Transformer_Power_Improved\" # Updated Model Name\nLOOKBACK = 24 * 7  # One week lookback (168 hours)\nBATCH_SIZE = 64   # Adjusted Batch Size\nLEARNING_RATE = 0.001 # Adjusted Learning Rate\nNUM_EPOCHS = 200   # Can be increased\nPATIENCE = 15      # Increased patience for early stopping, works well with LR scheduler\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nWEIGHT_DECAY =  0.0001 # Adjusted weight decay for regularization\nDROPOUT_RATE = 0.3 # Adjusted dropout rate\n\n# --- Generic function to load tabular Multivariate Time Series data ---\ndef load_mts_data(data_df, feature_cols, target_cols, lookback_period):\n    print(f\"--- {MODEL_NAME}: Creating sequences from data ---\")\n    features = data_df[feature_cols].values\n    targets = data_df[target_cols].values\n\n    X_list = []\n    y_list = []\n\n    for i in range(lookback_period, len(data_df)):\n        X_list.append(features[i - lookback_period : i])\n        y_list.append(targets[i])\n\n    X = np.array(X_list)\n    y = np.array(y_list)\n\n    print(f\"Loaded MTS data: X shape: {X.shape}, y shape: {y.shape}\")\n    return X, y\n\n# --- Chronological Train-Test Split ---\ndef chronological_train_test_split(X, y, test_ratio=0.2):\n    num_samples = X.shape[0]\n    num_test_samples = int(num_samples * test_ratio)\n    split_index = num_samples - num_test_samples\n\n    X_train = X[:split_index]\n    y_train = y[:split_index]\n    X_test = X[split_index:]\n    y_test = y[split_index:]\n\n    print(f\"X_train shape ({MODEL_NAME}): {X_train.shape}\")\n    print(f\"X_test shape ({MODEL_NAME}): {X_test.shape}\")\n    \n    return X_train, X_test, y_train, y_test\n\n# --- TimeSeriesDataset for GAT (Optimized Adjacency) ---\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, features, targets, single_adj_matrix, scaler_X=None, scaler_y=None, is_train=True):\n        original_shape = features.shape\n        if is_train and scaler_X is None:\n            self.scaler_X = StandardScaler()\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.fit_transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        elif not is_train and scaler_X is not None:\n            self.scaler_X = scaler_X\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        else: self.features = features; self.scaler_X = scaler_X\n        \n        if is_train and scaler_y is None:\n            self.scaler_y = StandardScaler(); self.targets = self.scaler_y.fit_transform(targets)\n        elif not is_train and scaler_y is not None:\n            self.scaler_y = scaler_y; self.targets = self.scaler_y.transform(targets)\n        else: self.targets = targets; self.scaler_y = scaler_y\n        \n        self.features = torch.tensor(self.features, dtype=torch.float32)\n        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n        \n        if not isinstance(single_adj_matrix, torch.Tensor):\n            self.single_adj_matrix = torch.tensor(single_adj_matrix, dtype=torch.float32)\n        else:\n            self.single_adj_matrix = single_adj_matrix.to(dtype=torch.float32)\n\n    def __len__(self): return len(self.features)\n    def __getitem__(self, idx): return self.features[idx], self.single_adj_matrix, self.targets[idx]\n    def get_scalers(self): return self.scaler_X, self.scaler_y\n\n# --- Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000): \n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term); pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0); self.register_buffer('pe', pe)\n    def forward(self, x):\n        # x shape: (batch, seq_len, d_model)\n        # self.pe shape: (1, max_len, d_model)\n        pe_sliced = self.pe[:, :x.size(1), :] # Correct slicing\n        try:\n            result = x + pe_sliced\n        except RuntimeError as e:\n            print(f\"ERROR during PositionalEncoding addition: x.shape={x.shape}, pe_sliced.shape={pe_sliced.shape}, self.pe.shape={self.pe.shape}\")\n            raise e\n        return result\n\n# --- Graph Attention Layer & MultiHeadGraphAttention ---\n# Using dropout_rate from MTSPredictorGATTransformer (passed as 'dropout')\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=DROPOUT_RATE, alpha=0.2): \n        super(GraphAttentionLayer, self).__init__()\n        self.in_features=in_features; self.out_features=out_features; self.dropout_val=dropout; self.alpha=alpha\n        self.W=nn.Parameter(torch.empty(size=(in_features, out_features))); nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a=nn.Parameter(torch.empty(size=(2*out_features, 1))); nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu=nn.LeakyReLU(self.alpha); self.dropout_layer=nn.Dropout(self.dropout_val)\n    def forward(self, h, adj):\n        Wh=torch.matmul(h, self.W); a_input=self._prepare_attention_input(Wh)\n        e=self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n        zero_vec = -9e15*torch.ones_like(e); attention_masked = torch.where(adj.unsqueeze(0) > 0, e, zero_vec)\n        attention_softmax=F.softmax(attention_masked, dim=-1); attention_dropout=self.dropout_layer(attention_softmax)\n        h_prime=torch.matmul(attention_dropout, Wh); return F.elu(h_prime)\n    def _prepare_attention_input(self, Wh):\n        B,N,_=Wh.size(); Wh_i=Wh.unsqueeze(2).expand(B,N,N,-1); Wh_j=Wh.unsqueeze(1).expand(B,N,N,-1)\n        return torch.cat([Wh_i, Wh_j], dim=-1)\n\nclass MultiHeadGraphAttention(nn.Module):\n    def __init__(self, in_features, out_features_per_head, n_heads, dropout=DROPOUT_RATE, alpha=0.2, concat=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.n_heads=n_heads; self.concat=concat\n        self.attentions=nn.ModuleList([GraphAttentionLayer(in_features, out_features_per_head, dropout=dropout, alpha=alpha) for _ in range(n_heads)])\n        self.out_dim = out_features_per_head * n_heads if concat else out_features_per_head\n    def forward(self, x, adj):\n        head_outputs=[att(x, adj) for att in self.attentions]\n        if self.concat: return torch.cat(head_outputs, dim=-1)\n        else: return torch.mean(torch.stack(head_outputs, dim=-1), dim=-1)\n\n# --- GAT+Transformer Model ---\nclass MTSPredictorGATTransformer(nn.Module):\n    def __init__(self, input_size, output_size, d_model_gat=64, gat_heads=4,\n                 d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n                 dim_feedforward_transformer=256, dropout_rate=DROPOUT_RATE): # Use global DROPOUT_RATE\n        super(MTSPredictorGATTransformer, self).__init__()\n        self.dropout_rate = dropout_rate # Use the passed dropout_rate\n        self.gat_input_proj = nn.Linear(input_size, d_model_gat)\n        self.gat_attention = MultiHeadGraphAttention(\n            in_features=d_model_gat,\n            out_features_per_head=d_model_gat // gat_heads,\n            n_heads=gat_heads, dropout=self.dropout_rate, concat=True # Pass self.dropout_rate\n        )\n        gat_output_dim = self.gat_attention.out_dim\n        # Ensure max_len is appropriate for LOOKBACK\n        self.pos_encoder = PositionalEncoding(gat_output_dim, max_len=LOOKBACK + 100) \n        transformer_encoder_layer = nn.TransformerEncoderLayer(\n            d_model=gat_output_dim, nhead=transformer_heads, dim_feedforward=dim_feedforward_transformer,\n            dropout=self.dropout_rate, batch_first=True # Use self.dropout_rate\n        )\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_transformer_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(gat_output_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(self.dropout_rate), # Use self.dropout_rate\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(self.dropout_rate), # Use self.dropout_rate\n            nn.Linear(32, output_size)\n        )\n    def forward(self, x, adj):\n        x_proj = F.elu(self.gat_input_proj(x))\n        # The dropout before GAT was removed in previous versions as GAT layers have internal dropout.\n        # If needed, it can be added back:\n        # x_gat_drop = F.dropout(x_proj, self.dropout_rate, training=self.training)\n        # x_gat_attended = self.gat_attention(x_gat_drop, adj)\n        x_gat_attended = self.gat_attention(x_proj, adj)\n\n\n        if x_gat_attended.dim() == 4 and x_gat_attended.shape[0] == 1:\n             x_gat_attended = x_gat_attended.squeeze(0)\n\n        x_pos_encoded = self.pos_encoder(x_gat_attended)\n        x_transformed = self.transformer_encoder(x_pos_encoded)\n        x_last_step = x_transformed[:, -1, :]\n        output = self.fc(x_last_step)\n        return output\n\n# --- Training Function (Modified to accept and use scheduler) ---\ndef train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device, model_save_name):\n    best_val_loss = float('inf'); epochs_no_improve = 0\n    train_losses_history, val_losses_history = [], []\n    for epoch in range(num_epochs):\n        model.train(); epoch_train_loss = 0\n        for batch_X, batch_adj_single, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n            batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n            outputs = model(batch_X, batch_adj_single[0]);\n            loss = criterion(outputs, batch_y)\n            optimizer.zero_grad(); loss.backward(); optimizer.step(); epoch_train_loss += loss.item()\n        epoch_train_loss /= len(train_loader); train_losses_history.append(epoch_train_loss)\n        \n        model.eval(); epoch_val_loss = 0\n        with torch.no_grad():\n            for batch_X, batch_adj_single, batch_y in val_loader:\n                batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n                outputs = model(batch_X, batch_adj_single[0])\n                loss = criterion(outputs, batch_y); epoch_val_loss += loss.item()\n        epoch_val_loss /= len(val_loader); val_losses_history.append(epoch_val_loss)\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch {epoch+1}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f}, LR={current_lr:.6f}\")\n\n        scheduler.step(epoch_val_loss) # Step the scheduler\n\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss; epochs_no_improve = 0; torch.save(model.state_dict(), model_save_name)\n            print(f\"Validation loss improved to {best_val_loss:.4f}. Saving model.\")\n        else: \n            epochs_no_improve += 1\n            print(f\"Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n        if epochs_no_improve >= patience: \n            print(f\"Early stopping @ epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n            break\n    \n    if os.path.exists(model_save_name): \n        print(f\"Loading best model from {model_save_name}\")\n        model.load_state_dict(torch.load(model_save_name))\n    return model, train_losses_history, val_losses_history\n\n# --- Evaluation Function ---\ndef evaluate_model_gat_based(model, test_loader, scaler_y, device, model_name=\"GAT-based Model\"):\n    model.eval(); all_preds_s, all_targets_s = [], []\n    with torch.no_grad():\n        for batch_X, batch_adj_single, batch_y_s in test_loader:\n            batch_X, batch_adj_single = batch_X.to(device), batch_adj_single.to(device)\n            outputs_s = model(batch_X, batch_adj_single[0])\n            all_preds_s.append(outputs_s.cpu().numpy()); all_targets_s.append(batch_y_s.cpu().numpy())\n    preds_s_np = np.vstack(all_preds_s); tgts_s_np = np.vstack(all_targets_s)\n    \n    if scaler_y: \n        preds_o = scaler_y.inverse_transform(preds_s_np)\n        tgts_o = scaler_y.inverse_transform(tgts_s_np)\n    else: \n        preds_o = preds_s_np; tgts_o = tgts_s_np\n    \n    mse=mean_squared_error(tgts_o,preds_o); rmse=np.sqrt(mse); r2=r2_score(tgts_o,preds_o); mae=mean_absolute_error(tgts_o,preds_o)\n    print(f\"\\nEvaluation metrics ({model_name}):\\n MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n    return preds_o, tgts_o, mse, rmse, r2, mae\n\n# --- Plotting Functions ---\ndef plot_training_losses(train_losses, val_losses, model_name, save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'training_losses_{model_name}.png')\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n    plt.xlabel('Epochs'); plt.ylabel('Loss (MSE)'); plt.title(f'Training & Validation Losses ({model_name})')\n    plt.legend(); plt.grid(True); plt.savefig(save_path); plt.show()\n\ndef plot_predictions_vs_true(targets_original, predictions_original, model_name, target_name=\"Target\", save_dir='visualizations'): # Added target_name\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'predictions_vs_true_{model_name}.png')\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(targets_original, predictions_original, alpha=0.3, label='Sample Predictions')\n    min_val = min(targets_original.ravel().min(), predictions_original.ravel().min())\n    max_val = max(targets_original.ravel().max(), predictions_original.ravel().max())\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='y=x (Perfect Prediction)')\n\n    plt.xlabel(f'True Values ({target_name})'); plt.ylabel(f'Predicted Values ({target_name})') # Use target_name\n    plt.title(f'Predictions vs True Values for {target_name} ({model_name})') # Use target_name\n    plt.legend(); plt.grid(True)\n    plt.tight_layout(); plt.savefig(save_path); plt.show()\n\n# --- Main Function ---\ndef main_mts_forecaster():\n    print(f\"--- Running {MODEL_NAME} Model ---\")\n    print(f\"Using device: {DEVICE}\")\n    # Update output_dir to reflect parameter changes\n    output_dir = f\"results_{MODEL_NAME.lower()}_lb{LOOKBACK}_bs{BATCH_SIZE}_lr{LEARNING_RATE}_dr{DROPOUT_RATE}_wd{WEIGHT_DECAY}\"\n    viz_dir = os.path.join(output_dir, \"visualizations\")\n    if not os.path.exists(viz_dir): os.makedirs(viz_dir)\n\n    data_path = '/kaggle/input/electric-power-consumption-data-set/household_power_consumption.txt'\n    if not os.path.exists(data_path):\n        print(f\"ERROR: Dataset not found at '{data_path}'\")\n        return\n\n    print(\"Loading and preprocessing data... This may take a moment.\")\n    df = pd.read_csv(\n        data_path, sep=';', low_memory=False, na_values=['?'],\n        parse_dates={'datetime': ['Date', 'Time']},\n        infer_datetime_format=True, index_col='datetime'\n    )\n    \n    # Convert all columns to numeric after loading, before ffill\n    for col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    df.ffill(inplace=True)\n    # df.bfill(inplace=True) # Optional: fill any remaining at the beginning\n\n    # Check for any NaNs remaining after ffill (should be none if dataset starts with valid values)\n    if df.isnull().sum().any():\n        print(\"Warning: NaNs still present after ffill. Filling remaining with 0 (consider implications).\")\n        df.fillna(0, inplace=True) # Fill any NaNs that might be left (e.g. if all values in a column were NaN initially)\n\n    agg_rules = {\n        'Global_active_power': 'sum', 'Global_reactive_power': 'sum',\n        'Voltage': 'mean', 'Global_intensity': 'mean',\n        'Sub_metering_1': 'sum', 'Sub_metering_2': 'sum', 'Sub_metering_3': 'sum'\n    }\n    df_resampled = df.resample('h').agg(agg_rules)\n    \n    # Additional check for NaNs after resampling (e.g., if an entire hour had no data)\n    if df_resampled.isnull().sum().any():\n        print(\"Warning: NaNs found after resampling. Applying ffill then bfill.\")\n        df_resampled.ffill(inplace=True)\n        df_resampled.bfill(inplace=True) # Fill NaNs at the beginning if any\n        if df_resampled.isnull().sum().any(): # If still NaNs, means entire series part is missing\n            print(\"Critical: NaNs still present after resampling and fill. Filling with 0.\")\n            df_resampled.fillna(0, inplace=True)\n\n\n    print(f\"Data resampled to hourly. New shape: {df_resampled.shape}\")\n    print(df_resampled.head())\n\n    feature_cols = [\n        'Global_active_power', 'Global_reactive_power', 'Voltage', \n        'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3'\n    ]\n    target_cols = ['Global_active_power']\n    target_name_for_plot = \"Global Active Power\" # For plot labels\n    \n    data_df = df_resampled\n    \n    X_all, y_all = load_mts_data(data_df, feature_cols, target_cols, LOOKBACK)\n    \n    if X_all.shape[0] == 0: print(\"No valid samples generated. Exiting.\"); return\n    \n    X_tv, X_test, y_tv, y_test = chronological_train_test_split(X_all, y_all, test_ratio=0.2)\n    \n    if X_tv.shape[0] == 0: print(f\"No training/validation samples for {MODEL_NAME}. Exiting.\"); return\n\n    adj_matrix_template = torch.zeros(LOOKBACK, LOOKBACK, dtype=torch.float32)\n    for i in range(LOOKBACK):\n        if i > 0: adj_matrix_template[i, i-1] = 1\n        if i < LOOKBACK - 1: adj_matrix_template[i, i+1] = 1\n        adj_matrix_template[i, i] = 1\n    adj_matrix_template = adj_matrix_template.to(DEVICE)\n    \n    tv_dataset = TimeSeriesDataset(X_tv, y_tv, adj_matrix_template.cpu(), is_train=True)\n    scaler_X, scaler_y = tv_dataset.get_scalers()\n    \n    test_loader = None\n    if X_test.shape[0] > 0:\n        test_dataset = TimeSeriesDataset(X_test, y_test, adj_matrix_template.cpu(), scaler_X, scaler_y, is_train=False)\n        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n    train_s = int(0.8 * len(tv_dataset)); val_s = len(tv_dataset) - train_s\n    if val_s == 0 and train_s > 0:\n        print(\"Warning: Validation set size is 0. Using training set for validation.\")\n        train_sub, val_sub = tv_dataset, tv_dataset\n    elif train_s == 0 or val_s == 0: \n        print(\"Not enough data for train/val split. Exiting.\"); return\n    else: \n        gen=torch.Generator().manual_seed(42)\n        train_sub, val_sub = torch.utils.data.random_split(tv_dataset, [train_s, val_s], generator=gen)\n\n    train_loader = DataLoader(train_sub, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_sub, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n   \n    model_input_size = X_tv.shape[-1]\n    model_output_size = y_tv.shape[-1]\n    \n    model = MTSPredictorGATTransformer(\n        input_size=model_input_size, \n        output_size=model_output_size,\n        d_model_gat=64, gat_heads=4,\n        d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n        dim_feedforward_transformer=256, dropout_rate=DROPOUT_RATE # Use global DROPOUT_RATE\n    ).to(DEVICE)\n    \n    print(f\"\\n{MODEL_NAME} Model Architecture:\\n{model}\")\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) # Use global LEARNING_RATE and WEIGHT_DECAY\n    \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7, verbose=True) # Scheduler patience adjusted\n   \n    model_save_path = os.path.join(output_dir, f\"best_mts_{MODEL_NAME.lower()}_model.pt\")\n    # Pass scheduler to the training function\n    trained_model, train_L, val_L = train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS, PATIENCE, DEVICE, model_save_path)\n    plot_training_losses(train_L, val_L, MODEL_NAME, save_dir=viz_dir)\n   \n    if test_loader and X_test.shape[0] > 0:\n        preds_o, tgts_o, mse, rmse, r2, mae = evaluate_model_gat_based(trained_model, test_loader, scaler_y, DEVICE, MODEL_NAME)\n        plot_predictions_vs_true(tgts_o, preds_o, MODEL_NAME, target_name=target_name_for_plot, save_dir=viz_dir) # Pass target_name\n        np.savez_compressed(os.path.join(output_dir, f'{MODEL_NAME.lower()}_test_results.npz'), predictions=preds_o, targets=tgts_o)\n    else: print(f\"Test set for {MODEL_NAME} was empty. Skipping final evaluation.\")\n\n    if scaler_X: joblib.dump(scaler_X, os.path.join(output_dir, f\"scaler_X_{MODEL_NAME.lower()}.pkl\"))\n    if scaler_y: joblib.dump(scaler_y, os.path.join(output_dir, f\"scaler_y_{MODEL_NAME.lower()}.pkl\"))\n    print(f\"{MODEL_NAME} model training complete. Results in {output_dir}\")\n\nif __name__ == \"__main__\":\n    main_mts_forecaster()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport math\nimport joblib\n\n# --- Configuration ---\nMODEL_NAME = \"GAT_Transformer_DC_Weather_TAVG\"\nLOOKBACK = 365  # Using 30 days of weather data to predict the next day\nBATCH_SIZE = 128\nLEARNING_RATE = 0.001\nNUM_EPOCHS = 100 # Can be increased for better performance\nPATIENCE = 10\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Generic function to load tabular Multivariate Time Series data ---\ndef load_mts_data(data_df, feature_cols, target_cols, lookback_period):\n    print(f\"--- {MODEL_NAME}: Creating sequences from data ---\")\n    features = data_df[feature_cols].values\n    targets = data_df[target_cols].values\n\n    X_list = []\n    y_list = []\n\n    # Create sequences\n    for i in range(lookback_period, len(data_df)):\n        X_list.append(features[i - lookback_period : i])\n        y_list.append(targets[i])\n\n    X = np.array(X_list)\n    y = np.array(y_list)\n\n    print(f\"Loaded MTS data: X shape: {X.shape}, y shape: {y.shape}\")\n    return X, y\n\n# --- Chronological Train-Test Split ---\ndef chronological_train_test_split(X, y, test_ratio=0.2):\n    num_samples = X.shape[0]\n    num_test_samples = int(num_samples * test_ratio)\n    split_index = num_samples - num_test_samples\n\n    X_train = X[:split_index]\n    y_train = y[:split_index]\n    X_test = X[split_index:]\n    y_test = y[split_index:]\n\n    print(f\"X_train shape ({MODEL_NAME}): {X_train.shape}\")\n    print(f\"X_test shape ({MODEL_NAME}): {X_test.shape}\")\n    \n    return X_train, X_test, y_train, y_test\n\n# --- TimeSeriesDataset for GAT (Optimized Adjacency) ---\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, features, targets, single_adj_matrix, scaler_X=None, scaler_y=None, is_train=True):\n        original_shape = features.shape\n        if is_train and scaler_X is None:\n            self.scaler_X = StandardScaler()\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.fit_transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        elif not is_train and scaler_X is not None:\n            self.scaler_X = scaler_X\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        else: self.features = features; self.scaler_X = scaler_X\n        \n        if is_train and scaler_y is None:\n            self.scaler_y = StandardScaler(); self.targets = self.scaler_y.fit_transform(targets)\n        elif not is_train and scaler_y is not None:\n            self.scaler_y = scaler_y; self.targets = self.scaler_y.transform(targets)\n        else: self.targets = targets; self.scaler_y = scaler_y\n        \n        self.features = torch.tensor(self.features, dtype=torch.float32)\n        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n        \n        if not isinstance(single_adj_matrix, torch.Tensor):\n            self.single_adj_matrix = torch.tensor(single_adj_matrix, dtype=torch.float32)\n        else:\n            self.single_adj_matrix = single_adj_matrix.to(dtype=torch.float32)\n\n    def __len__(self): return len(self.features)\n    def __getitem__(self, idx): return self.features[idx], self.single_adj_matrix, self.targets[idx]\n    def get_scalers(self): return self.scaler_X, self.scaler_y\n\n# --- Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term); pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0); self.register_buffer('pe', pe)\n    def forward(self, x):\n        pe_sliced = self.pe[:, :x.size(1)]\n        try:\n            result = x + pe_sliced\n        except RuntimeError as e:\n            print(f\"ERROR during PositionalEncoding addition: x.shape={x.shape}, pe_sliced.shape={pe_sliced.shape}\")\n            raise e\n        return result\n\n# --- Graph Attention Layer & MultiHeadGraphAttention ---\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n        super(GraphAttentionLayer, self).__init__()\n        self.in_features=in_features; self.out_features=out_features; self.dropout_val=dropout; self.alpha=alpha\n        self.W=nn.Parameter(torch.empty(size=(in_features, out_features))); nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a=nn.Parameter(torch.empty(size=(2*out_features, 1))); nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu=nn.LeakyReLU(self.alpha); self.dropout_layer=nn.Dropout(self.dropout_val)\n    def forward(self, h, adj):\n        Wh=torch.matmul(h, self.W); a_input=self._prepare_attention_input(Wh)\n        e=self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n        zero_vec = -9e15*torch.ones_like(e); attention_masked = torch.where(adj.unsqueeze(0) > 0, e, zero_vec)\n        attention_softmax=F.softmax(attention_masked, dim=-1); attention_dropout=self.dropout_layer(attention_softmax)\n        h_prime=torch.matmul(attention_dropout, Wh); return F.elu(h_prime)\n    def _prepare_attention_input(self, Wh):\n        B,N,_=Wh.size(); Wh_i=Wh.unsqueeze(2).expand(B,N,N,-1); Wh_j=Wh.unsqueeze(1).expand(B,N,N,-1)\n        return torch.cat([Wh_i, Wh_j], dim=-1)\n\nclass MultiHeadGraphAttention(nn.Module):\n    def __init__(self, in_features, out_features_per_head, n_heads, dropout=0.2, alpha=0.2, concat=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.n_heads=n_heads; self.concat=concat\n        self.attentions=nn.ModuleList([GraphAttentionLayer(in_features, out_features_per_head, dropout, alpha) for _ in range(n_heads)])\n        self.out_dim = out_features_per_head * n_heads if concat else out_features_per_head\n    def forward(self, x, adj):\n        head_outputs=[att(x, adj) for att in self.attentions]\n        if self.concat: return torch.cat(head_outputs, dim=-1)\n        else: return torch.mean(torch.stack(head_outputs, dim=-1), dim=-1)\n\n# --- GAT+Transformer Model ---\nclass MTSPredictorGATTransformer(nn.Module):\n    def __init__(self, input_size, output_size, d_model_gat=64, gat_heads=4,\n                 d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n                 dim_feedforward_transformer=256, dropout_rate=0.2):\n        super(MTSPredictorGATTransformer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.gat_input_proj = nn.Linear(input_size, d_model_gat)\n        self.gat_attention = MultiHeadGraphAttention(\n            in_features=d_model_gat,\n            out_features_per_head=d_model_gat // gat_heads,\n            n_heads=gat_heads, dropout=dropout_rate, concat=True\n        )\n        gat_output_dim = self.gat_attention.out_dim\n        self.pos_encoder = PositionalEncoding(gat_output_dim, max_len=LOOKBACK + 10) # max_len should be larger than LOOKBACK\n        transformer_encoder_layer = nn.TransformerEncoderLayer(\n            d_model=gat_output_dim, nhead=transformer_heads, dim_feedforward=dim_feedforward_transformer,\n            dropout=dropout_rate, batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_transformer_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(gat_output_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(32, output_size)\n        )\n    def forward(self, x, adj):\n        x_proj = F.elu(self.gat_input_proj(x))\n        x_gat_drop = F.dropout(x_proj, self.dropout_rate, training=self.training)\n        x_gat_attended = self.gat_attention(x_gat_drop, adj)\n\n        if x_gat_attended.dim() == 4 and x_gat_attended.shape[0] == 1: # Handling potential batching issue\n             x_gat_attended = x_gat_attended.squeeze(0)\n\n        x_pos_encoded = self.pos_encoder(x_gat_attended)\n        x_transformed = self.transformer_encoder(x_pos_encoded)\n        x_last_step = x_transformed[:, -1, :]\n        output = self.fc(x_last_step)\n        return output\n\n# --- Training Function ---\ndef train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device, model_save_name):\n    best_val_loss = float('inf'); epochs_no_improve = 0\n    train_losses_history, val_losses_history = [], []\n    for epoch in range(num_epochs):\n        model.train(); epoch_train_loss = 0\n        for batch_X, batch_adj_single, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n            batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n            outputs = model(batch_X, batch_adj_single[0]);\n            loss = criterion(outputs, batch_y)\n            optimizer.zero_grad(); loss.backward(); optimizer.step(); epoch_train_loss += loss.item()\n        epoch_train_loss /= len(train_loader); train_losses_history.append(epoch_train_loss)\n        \n        model.eval(); epoch_val_loss = 0\n        with torch.no_grad():\n            for batch_X, batch_adj_single, batch_y in val_loader:\n                batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n                outputs = model(batch_X, batch_adj_single[0])\n                loss = criterion(outputs, batch_y); epoch_val_loss += loss.item()\n        epoch_val_loss /= len(val_loader); val_losses_history.append(epoch_val_loss)\n        print(f\"Epoch {epoch+1}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f}\")\n\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss; epochs_no_improve = 0; torch.save(model.state_dict(), model_save_name)\n        else: epochs_no_improve += 1\n        if epochs_no_improve >= patience: print(f\"Early stopping @ epoch {epoch+1}\"); break\n    \n    if os.path.exists(model_save_name): model.load_state_dict(torch.load(model_save_name))\n    return model, train_losses_history, val_losses_history\n\n# --- Evaluation Function ---\ndef evaluate_model_gat_based(model, test_loader, scaler_y, device, model_name=\"GAT-based Model\"):\n    model.eval(); all_preds_s, all_targets_s = [], []\n    with torch.no_grad():\n        for batch_X, batch_adj_single, batch_y_s in test_loader:\n            batch_X, batch_adj_single = batch_X.to(device), batch_adj_single.to(device)\n            outputs_s = model(batch_X, batch_adj_single[0])\n            all_preds_s.append(outputs_s.cpu().numpy()); all_targets_s.append(batch_y_s.numpy())\n    preds_s_np = np.vstack(all_preds_s); tgts_s_np = np.vstack(all_targets_s)\n    \n    if scaler_y: preds_o = scaler_y.inverse_transform(preds_s_np); tgts_o = scaler_y.inverse_transform(tgts_s_np)\n    else: preds_o = preds_s_np; tgts_o = tgts_s_np\n    \n    mse=mean_squared_error(tgts_o,preds_o); rmse=np.sqrt(mse); r2=r2_score(tgts_o,preds_o); mae=mean_absolute_error(tgts_o,preds_o)\n    print(f\"\\nEvaluation metrics ({model_name}):\\n MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n    return preds_o, tgts_o, mse, rmse, r2, mae\n\n# --- Plotting Functions ---\ndef plot_training_losses(train_losses, val_losses, model_name, save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'training_losses_{model_name}.png')\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n    plt.xlabel('Epochs'); plt.ylabel('Loss (MSE)'); plt.title(f'Training & Validation Losses ({model_name})')\n    plt.legend(); plt.grid(True); plt.savefig(save_path); plt.show()\n\ndef plot_predictions_vs_true(targets_original, predictions_original, model_name, target_name=\"TAVG\", save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'predictions_vs_true_{model_name}.png')\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(targets_original, predictions_original, alpha=0.3, label='Sample Predictions')\n    min_val = min(targets_original.min(), predictions_original.min())\n    max_val = max(targets_original.max(), predictions_original.max())\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='y=x (Perfect Prediction)')\n\n    plt.xlabel(f'True Values ({target_name})'); plt.ylabel(f'Predicted Values ({target_name})')\n    plt.title(f'Predictions vs True Values for {target_name} ({model_name})')\n    plt.legend(); plt.grid(True)\n    plt.tight_layout(); plt.savefig(save_path); plt.show()\n\n# --- Main Function ---\ndef main_mts_forecaster():\n    print(f\"--- Running {MODEL_NAME} Model ---\")\n    print(f\"Using device: {DEVICE}\")\n    output_dir = f\"results_{MODEL_NAME.lower()}\"\n    viz_dir = os.path.join(output_dir, \"visualizations\")\n    if not os.path.exists(viz_dir): os.makedirs(viz_dir)\n\n    # --- Load and Preprocess Washington DC Weather Data ---\n    # !!! USER ACTION: Make sure this path matches your downloaded file name !!!\n    data_path = '/kaggle/input/washington-dc-historical-weather-20158202407/dc_weather.csv' # Or the actual name of your CSV\n    if not os.path.exists(data_path):\n        print(f\"ERROR: Dataset not found at '{data_path}'\")\n        print(\"Please download it from Kaggle and place it in the same directory, then update the 'data_path' variable.\")\n        return\n\n    print(\"Loading and preprocessing DC weather data...\")\n    df = pd.read_csv(data_path)\n    # print(\"Columns in DataFrame:\", df.columns) # You can keep this for debugging if needed\n\n    # --- CORRECTED SECTION ---\n    actual_date_column_name = 'datetime' # Corrected column name\n\n    # Convert actual_date_column_name to datetime and set as index\n    df[actual_date_column_name] = pd.to_datetime(df[actual_date_column_name])\n    df.set_index(actual_date_column_name, inplace=True)\n    \n    # Select relevant columns for features and target, using new names\n    # TAVG (average temp) is our target, likely 'temp' in your file\n    # Other features: 'tempmax', 'tempmin', 'precip' (Precipitation), 'snow' (Snowfall), \n    # 'snowdepth' (Snow Depth), 'windspeed' (Average Wind Speed)\n    \n    # Updated relevant columns based on your DataFrame's column names\n    relevant_cols = ['temp', 'tempmax', 'tempmin', 'precip', 'snow', 'snowdepth', 'windspeed']\n    \n    # Ensure all selected columns are numeric, coercing errors will turn non-convertible to NaN\n    for col in relevant_cols:\n        if col in df.columns: # Check if column exists before trying to convert\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n        else:\n            print(f\"Warning: Column '{col}' not found in DataFrame. It will be ignored.\")\n\n    # Keep only the relevant columns that were found and are numeric\n    # Filter relevant_cols to only include those present in df.columns to avoid KeyErrors\n    existing_relevant_cols = [col for col in relevant_cols if col in df.columns]\n    df_processed = df[existing_relevant_cols].copy()\n    \n    # Handle missing values - forward fill is a common strategy for time series\n    df_processed.ffill(inplace=True)\n    # In case any leading NaNs remain after ffill, bfill them\n    df_processed.bfill(inplace=True) \n    \n    # Check if any NaNs are left after filling\n    if df_processed.isnull().sum().any():\n        print(\"Warning: NaNs still present after ffill and bfill. Consider other imputation strategies.\")\n        print(df_processed.isnull().sum())\n        df_processed.fillna(0, inplace=True) # Fallback to fill with 0, review if this is appropriate\n\n    print(f\"Data processed. Shape: {df_processed.shape}\")\n    print(df_processed.head())\n\n    # Define feature and target columns using the corrected names\n    feature_cols = existing_relevant_cols \n    target_cols = ['temp']       # We are forecasting 'temp' (average temperature)\n    target_name_for_plot = \"Average Temperature (temp)\"\n\n    # Check if the target column exists\n    if target_cols[0] not in df_processed.columns:\n        print(f\"ERROR: Target column '{target_cols[0]}' not found in processed data. Available columns: {df_processed.columns}\")\n        return\n    # --- END OF CORRECTED SECTION ---\n    \n    data_df = df_processed\n    \n    X_all, y_all = load_mts_data(data_df, feature_cols, target_cols, LOOKBACK)\n    \n    if X_all.shape[0] == 0: print(\"No valid samples generated. Exiting.\"); return\n    \n    X_tv, X_test, y_tv, y_test = chronological_train_test_split(X_all, y_all, test_ratio=0.2)\n    \n    if X_tv.shape[0] == 0: print(f\"No training/validation samples for {MODEL_NAME}. Exiting.\"); return\n\n    # Create the single temporal adjacency matrix for the LOOKBACK window\n    adj_matrix_template = torch.zeros(LOOKBACK, LOOKBACK, dtype=torch.float32)\n    for i in range(LOOKBACK):\n        if i > 0: adj_matrix_template[i, i-1] = 1\n        if i < LOOKBACK - 1: adj_matrix_template[i, i+1] = 1\n        adj_matrix_template[i, i] = 1 # Self-loops\n    adj_matrix_template = adj_matrix_template.to(DEVICE)\n    \n    tv_dataset = TimeSeriesDataset(X_tv, y_tv, adj_matrix_template.cpu(), is_train=True)\n    scaler_X, scaler_y = tv_dataset.get_scalers()\n    \n    test_loader = None\n    if X_test.shape[0] > 0:\n        test_dataset = TimeSeriesDataset(X_test, y_test, adj_matrix_template.cpu(), scaler_X, scaler_y, is_train=False)\n        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) \n\n    train_s = int(0.8 * len(tv_dataset)); val_s = len(tv_dataset) - train_s\n    if val_s == 0 and train_s > 0:\n        print(\"Warning: Validation set size is 0. Using training set for validation.\")\n        train_sub, val_sub = tv_dataset, tv_dataset \n    elif train_s == 0 or val_s == 0 : \n        print(\"Not enough data for train/val split. Exiting.\"); return\n    else: \n        gen=torch.Generator().manual_seed(42); \n        train_sub, val_sub = torch.utils.data.random_split(tv_dataset, [train_s, val_s], generator=gen)\n\n    train_loader = DataLoader(train_sub, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_sub, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n   \n    model_input_size = X_tv.shape[-1]\n    model_output_size = y_tv.shape[-1]\n    \n    model = MTSPredictorGATTransformer(\n        input_size=model_input_size, \n        output_size=model_output_size,\n        d_model_gat=64, gat_heads=4,\n        d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n        dim_feedforward_transformer=256, dropout_rate=0.2\n    ).to(DEVICE)\n    \n    print(f\"\\n{MODEL_NAME} Model Architecture:\\n{model}\")\n    criterion = nn.MSELoss(); optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-05)\n   \n    model_save_path = os.path.join(output_dir, f\"best_mts_{MODEL_NAME.lower()}_model.pt\")\n    trained_model, train_L, val_L = train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, PATIENCE, DEVICE, model_save_path)\n    plot_training_losses(train_L, val_L, MODEL_NAME, save_dir=viz_dir)\n   \n    if test_loader and X_test.shape[0] > 0 :\n        preds_o, tgts_o, mse, rmse, r2, mae = evaluate_model_gat_based(trained_model, test_loader, scaler_y, DEVICE, MODEL_NAME)\n        plot_predictions_vs_true(tgts_o, preds_o, MODEL_NAME, target_name=target_name_for_plot, save_dir=viz_dir) # Using target_name_for_plot\n        np.savez_compressed(os.path.join(output_dir, f'{MODEL_NAME.lower()}_test_results.npz'), predictions=preds_o, targets=tgts_o)\n    else: print(f\"Test set for {MODEL_NAME} was empty or test_loader not created. Skipping final evaluation plot.\")\n\n    if scaler_X and scaler_y:\n        joblib.dump(scaler_X, os.path.join(output_dir, f\"scaler_X_{MODEL_NAME.lower()}.pkl\"))\n        joblib.dump(scaler_y, os.path.join(output_dir, f\"scaler_y_{MODEL_NAME.lower()}.pkl\"))\n    print(f\"{MODEL_NAME} model training complete. Results in {output_dir}\")\n\nif __name__ == \"__main__\":\n    main_mts_forecaster()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T11:33:45.987820Z","iopub.execute_input":"2025-07-01T11:33:45.988097Z","iopub.status.idle":"2025-07-01T11:36:44.838105Z","shell.execute_reply.started":"2025-07-01T11:33:45.988074Z","shell.execute_reply":"2025-07-01T11:36:44.837276Z"}},"outputs":[{"name":"stdout","text":"--- Running GAT_Transformer_DC_Weather_TAVG Model ---\nUsing device: cuda\nLoading and preprocessing DC weather data...\nData processed. Shape: (3319, 7)\n            temp  tempmax  tempmin  precip  snow  snowdepth  windspeed\ndatetime                                                              \n2015-08-01  28.2     33.1     22.8   0.000   0.0        0.0       15.9\n2015-08-02  27.3     32.0     22.8   0.000   0.0        0.0       22.3\n2015-08-03  27.9     33.2     21.8   0.000   0.0        0.0       24.8\n2015-08-04  29.3     35.3     24.9   1.231   0.0        0.0       22.6\n2015-08-05  28.6     33.6     24.0   0.000   0.0        0.0       21.6\n--- GAT_Transformer_DC_Weather_TAVG: Creating sequences from data ---\nLoaded MTS data: X shape: (2954, 365, 7), y shape: (2954, 1)\nX_train shape (GAT_Transformer_DC_Weather_TAVG): (2364, 365, 7)\nX_test shape (GAT_Transformer_DC_Weather_TAVG): (590, 365, 7)\n\nGAT_Transformer_DC_Weather_TAVG Model Architecture:\nMTSPredictorGATTransformer(\n  (gat_input_proj): Linear(in_features=7, out_features=64, bias=True)\n  (gat_attention): MultiHeadGraphAttention(\n    (attentions): ModuleList(\n      (0-3): 4 x GraphAttentionLayer(\n        (leakyrelu): LeakyReLU(negative_slope=0.2)\n        (dropout_layer): Dropout(p=0.2, inplace=False)\n      )\n    )\n  )\n  (pos_encoder): PositionalEncoding()\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (linear1): Linear(in_features=64, out_features=256, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (linear2): Linear(in_features=256, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.2, inplace=False)\n        (dropout2): Dropout(p=0.2, inplace=False)\n      )\n    )\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=64, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=64, out_features=32, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=32, out_features=1, bias=True)\n  )\n)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 15/15 [00:06<00:00,  2.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.5743, Val Loss=0.2621\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2717, Val Loss=0.1617\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2251, Val Loss=0.1353\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2025, Val Loss=0.1240\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1758, Val Loss=0.1098\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1674, Val Loss=0.1111\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1544, Val Loss=0.1131\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1465, Val Loss=0.1063\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1464, Val Loss=0.1080\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1443, Val Loss=0.1053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1450, Val Loss=0.1111\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1530, Val Loss=0.1127\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1415, Val Loss=0.1092\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1331, Val Loss=0.1084\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1346, Val Loss=0.1157\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1420, Val Loss=0.1087\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1349, Val Loss=0.1036\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1307, Val Loss=0.1048\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1377, Val Loss=0.1091\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1321, Val Loss=0.1036\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1286, Val Loss=0.1072\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1273, Val Loss=0.1051\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1262, Val Loss=0.1076\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1289, Val Loss=0.1067\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1283, Val Loss=0.1072\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1252, Val Loss=0.1057\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 15/15 [00:05<00:00,  2.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1320, Val Loss=0.1072\nEarly stopping @ epoch 27\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nEvaluation metrics (GAT_Transformer_DC_Weather_TAVG):\n MSE: 7.1092, RMSE: 2.6663, MAE: 2.0318, R²: 0.9021\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x800 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"GAT_Transformer_DC_Weather_TAVG model training complete. Results in results_gat_transformer_dc_weather_tavg\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport math\nimport joblib\nimport itertools\n\n# --- Configuration ---\nMODEL_NAME = \"GAT_Transformer_DC_Weather_TAVG\"\nLOOKBACK = 30  # Fixed lookback for all grid search runs\nNUM_EPOCHS = 100 # Max epochs for each run\nPATIENCE = 15    # Increased patience for early stopping\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Generic function to load tabular Multivariate Time Series data ---\ndef load_mts_data(data_df, feature_cols, target_cols, lookback_period):\n    print(f\"--- Creating sequences with lookback={lookback_period} ---\")\n    features = data_df[feature_cols].values\n    targets = data_df[target_cols].values\n    X_list, y_list = [], []\n    for i in range(lookback_period, len(data_df)):\n        X_list.append(features[i - lookback_period : i])\n        y_list.append(targets[i])\n    X, y = np.array(X_list), np.array(y_list)\n    print(f\"Loaded MTS data: X shape: {X.shape}, y shape: {y.shape}\")\n    return X, y\n\n# --- Chronological Train-Test Split ---\ndef chronological_train_test_split(X, y, test_ratio=0.2):\n    num_samples = X.shape[0]\n    num_test_samples = int(num_samples * test_ratio)\n    split_index = num_samples - num_test_samples\n    X_train, X_test = X[:split_index], X[split_index:]\n    y_train, y_test = y[:split_index], y[split_index:]\n    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n    return X_train, X_test, y_train, y_test\n\n# --- TimeSeriesDataset for GAT (Optimized Adjacency) ---\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, features, targets, single_adj_matrix, scaler_X=None, scaler_y=None, is_train=True):\n        original_shape = features.shape\n        if is_train and scaler_X is None:\n            self.scaler_X = StandardScaler()\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.fit_transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        elif not is_train and scaler_X is not None:\n            self.scaler_X = scaler_X\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        else: self.features, self.scaler_X = features, scaler_X\n        if is_train and scaler_y is None:\n            self.scaler_y = StandardScaler(); self.targets = self.scaler_y.fit_transform(targets)\n        elif not is_train and scaler_y is not None:\n            self.scaler_y = scaler_y; self.targets = self.scaler_y.transform(targets)\n        else: self.targets, self.scaler_y = targets, scaler_y\n        self.features = torch.tensor(self.features, dtype=torch.float32)\n        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n        if not isinstance(single_adj_matrix, torch.Tensor):\n            self.single_adj_matrix = torch.tensor(single_adj_matrix, dtype=torch.float32)\n        else:\n            self.single_adj_matrix = single_adj_matrix.to(dtype=torch.float32)\n    def __len__(self): return len(self.features)\n    def __getitem__(self, idx): return self.features[idx], self.single_adj_matrix, self.targets[idx]\n    def get_scalers(self): return self.scaler_X, self.scaler_y\n\n# --- Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term); pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0); self.register_buffer('pe', pe)\n    def forward(self, x):\n        pe_sliced = self.pe[:, :x.size(1), :]\n        try:\n            result = x + pe_sliced\n        except RuntimeError as e:\n            print(f\"ERROR in PositionalEncoding: x.shape={x.shape}, pe_sliced.shape={pe_sliced.shape}\")\n            raise e\n        return result\n\n# --- Graph Attention Layer & MultiHeadGraphAttention ---\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n        super(GraphAttentionLayer, self).__init__()\n        self.in_features=in_features; self.out_features=out_features; self.dropout_val=dropout; self.alpha=alpha\n        self.W=nn.Parameter(torch.empty(size=(in_features, out_features))); nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a=nn.Parameter(torch.empty(size=(2*out_features, 1))); nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu=nn.LeakyReLU(self.alpha); self.dropout_layer=nn.Dropout(self.dropout_val)\n    def forward(self, h, adj):\n        Wh=torch.matmul(h, self.W); a_input=self._prepare_attention_input(Wh)\n        e=self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n        zero_vec = -9e15*torch.ones_like(e); attention_masked = torch.where(adj.unsqueeze(0) > 0, e, zero_vec)\n        attention_softmax=F.softmax(attention_masked, dim=-1); attention_dropout=self.dropout_layer(attention_softmax)\n        h_prime=torch.matmul(attention_dropout, Wh); return F.elu(h_prime)\n    def _prepare_attention_input(self, Wh):\n        B,N,_=Wh.size(); Wh_i=Wh.unsqueeze(2).expand(B,N,N,-1); Wh_j=Wh.unsqueeze(1).expand(B,N,N,-1)\n        return torch.cat([Wh_i, Wh_j], dim=-1)\n\nclass MultiHeadGraphAttention(nn.Module):\n    def __init__(self, in_features, out_features_per_head, n_heads, dropout=0.2, alpha=0.2, concat=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.n_heads=n_heads; self.concat=concat\n        self.attentions=nn.ModuleList([GraphAttentionLayer(in_features, out_features_per_head, dropout, alpha) for _ in range(n_heads)])\n        self.out_dim = out_features_per_head * n_heads if concat else out_features_per_head\n    def forward(self, x, adj):\n        head_outputs=[att(x, adj) for att in self.attentions]\n        if self.concat: return torch.cat(head_outputs, dim=-1)\n        else: return torch.mean(torch.stack(head_outputs, dim=-1), dim=-1)\n\n# --- GAT+Transformer Model ---\nclass MTSPredictorGATTransformer(nn.Module):\n    def __init__(self, input_size, output_size, d_model_gat=64, gat_heads=4,\n                 d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n                 dim_feedforward_transformer=256, dropout_rate=0.2):\n        super(MTSPredictorGATTransformer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.gat_input_proj = nn.Linear(input_size, d_model_gat)\n        self.gat_attention = MultiHeadGraphAttention(\n            in_features=d_model_gat, out_features_per_head=d_model_gat // gat_heads,\n            n_heads=gat_heads, dropout=dropout_rate, concat=True)\n        gat_output_dim = self.gat_attention.out_dim\n        self.pos_encoder = PositionalEncoding(gat_output_dim, max_len=LOOKBACK + 100)\n        transformer_encoder_layer = nn.TransformerEncoderLayer(\n            d_model=gat_output_dim, nhead=transformer_heads, dim_feedforward=dim_feedforward_transformer,\n            dropout=dropout_rate, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_transformer_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(gat_output_dim, 64), nn.ReLU(), nn.Dropout(dropout_rate),\n            nn.Linear(64, 32), nn.ReLU(), nn.Dropout(dropout_rate), nn.Linear(32, output_size))\n    def forward(self, x, adj):\n        x_proj = F.elu(self.gat_input_proj(x)); x_gat_drop = F.dropout(x_proj, self.dropout_rate, training=self.training)\n        x_gat_attended = self.gat_attention(x_gat_drop, adj)\n        if x_gat_attended.dim() == 4 and x_gat_attended.shape[0] == 1:\n             x_gat_attended = x_gat_attended.squeeze(0)\n        x_pos_encoded = self.pos_encoder(x_gat_attended); x_transformed = self.transformer_encoder(x_pos_encoded)\n        x_last_step = x_transformed[:, -1, :]; output = self.fc(x_last_step)\n        return output\n\n# --- Training Function ---\ndef train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device, model_save_name):\n    best_val_loss = float('inf'); epochs_no_improve = 0\n    train_losses_history, val_losses_history = [], []\n    for epoch in range(num_epochs):\n        model.train(); epoch_train_loss = 0\n        for batch_X, batch_adj_single, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n            batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n            outputs = model(batch_X, batch_adj_single[0]); loss = criterion(outputs, batch_y)\n            optimizer.zero_grad(); loss.backward(); optimizer.step(); epoch_train_loss += loss.item()\n        epoch_train_loss /= len(train_loader); train_losses_history.append(epoch_train_loss)\n        \n        model.eval(); epoch_val_loss = 0\n        with torch.no_grad():\n            for batch_X, batch_adj_single, batch_y in val_loader:\n                batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n                outputs = model(batch_X, batch_adj_single[0]); loss = criterion(outputs, batch_y)\n                epoch_val_loss += loss.item()\n        epoch_val_loss /= len(val_loader); val_losses_history.append(epoch_val_loss)\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch {epoch+1}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f}, LR={current_lr:.6f}\")\n        if scheduler: scheduler.step(epoch_val_loss)\n        \n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss; epochs_no_improve = 0\n            if model_save_name: torch.save(model.state_dict(), model_save_name)\n        else: epochs_no_improve += 1\n        if epochs_no_improve >= patience: print(f\"Early stopping @ epoch {epoch+1}\"); break\n    \n    if model_save_name and os.path.exists(model_save_name): model.load_state_dict(torch.load(model_save_name))\n    return model, train_losses_history, val_losses_history, best_val_loss\n\n# --- Evaluation Function ---\ndef evaluate_model_gat_based(model, test_loader, scaler_y, device, model_name=\"GAT-based Model\"):\n    model.eval(); all_preds_s, all_targets_s = [], []\n    with torch.no_grad():\n        for batch_X, batch_adj_single, batch_y_s in test_loader:\n            batch_X, batch_adj_single = batch_X.to(device), batch_adj_single.to(device)\n            outputs_s = model(batch_X, batch_adj_single[0])\n            all_preds_s.append(outputs_s.cpu().numpy()); all_targets_s.append(batch_y_s.cpu().numpy())\n    preds_s_np = np.vstack(all_preds_s); tgts_s_np = np.vstack(all_targets_s)\n    if scaler_y: preds_o = scaler_y.inverse_transform(preds_s_np); tgts_o = scaler_y.inverse_transform(tgts_s_np)\n    else: preds_o, tgts_o = preds_s_np, tgts_s_np\n    mse=mean_squared_error(tgts_o,preds_o); rmse=np.sqrt(mse); r2=r2_score(tgts_o,preds_o); mae=mean_absolute_error(tgts_o,preds_o)\n    print(f\"\\nEvaluation metrics ({model_name}):\\n MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n    return preds_o, tgts_o, mse, rmse, r2, mae\n\n# --- Plotting Functions ---\ndef plot_training_losses(train_losses, val_losses, model_name, save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'training_losses_{model_name}.png')\n    plt.figure(figsize=(10, 6)); plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n    plt.xlabel('Epochs'); plt.ylabel('Loss (MSE)'); plt.title(f'Training & Validation Losses ({model_name})')\n    plt.legend(); plt.grid(True); plt.savefig(save_path); plt.close()\n\ndef plot_predictions_vs_true(targets_original, predictions_original, model_name, target_name=\"Target\", save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'predictions_vs_true_{model_name}.png')\n    plt.figure(figsize=(12, 8)); plt.scatter(targets_original, predictions_original, alpha=0.3, label='Sample Predictions')\n    min_val = min(targets_original.ravel().min(), predictions_original.ravel().min())\n    max_val = max(targets_original.ravel().max(), predictions_original.ravel().max())\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='y=x (Perfect Prediction)')\n    plt.xlabel(f'True Values ({target_name})'); plt.ylabel(f'Predicted Values ({target_name})')\n    plt.title(f'Predictions vs True Values for {target_name} ({model_name})')\n    plt.legend(); plt.grid(True); plt.tight_layout(); plt.savefig(save_path); plt.close()\n\n# --- New Function to run a single training session for grid search ---\ndef run_training_session(params, train_dataset, val_dataset, adj_matrix_template, model_input_size, model_output_size, output_dir):\n    print(f\"\\n--- Running with params: {params} ---\")\n    lr, dropout_rate, weight_decay, batch_size = params['LEARNING_RATE'], params['DROPOUT_RATE'], params['WEIGHT_DECAY'], params['BATCH_SIZE']\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    \n    model = MTSPredictorGATTransformer(\n        input_size=model_input_size, output_size=model_output_size, dropout_rate=dropout_rate).to(DEVICE)\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, verbose=False)\n    \n    param_str = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n    model_save_name = os.path.join(output_dir, f\"best_model_{param_str}.pt\")\n    \n    _, _, _, best_val_loss = train_model_gat_based(\n        model, train_loader, val_loader, criterion, optimizer, scheduler,\n        NUM_EPOCHS, PATIENCE, DEVICE, model_save_name\n    )\n    return best_val_loss\n\n# --- Main Function ---\n# --- Main Function ---\ndef main_mts_forecaster():\n    print(f\"--- Running {MODEL_NAME} Grid Search ---\")\n    print(f\"Using device: {DEVICE}\")\n    output_dir = f\"results_{MODEL_NAME.lower()}_grid_search\"\n    viz_dir = os.path.join(output_dir, \"visualizations\")\n    if not os.path.exists(viz_dir): os.makedirs(viz_dir)\n\n    # --- 1. DEFINE HYPERPARAMETER GRID ---\n    param_grid = {\n        'LEARNING_RATE': [0.001, 0.0005],\n        'DROPOUT_RATE': [0.2, 0.4],\n        'WEIGHT_DECAY': [1e-5, 1e-4],\n        'BATCH_SIZE': [64, 128]\n    }\n    print(\"--- Hyperparameter Grid ---\"); print(param_grid)\n\n    # --- 2. LOAD AND PREPROCESS DATA ONCE ---\n    data_path = '/kaggle/input/washington-dc-historical-weather-20158202407/dc_weather.csv'\n    if not os.path.exists(data_path): print(f\"ERROR: Dataset not found at '{data_path}'\"); return\n    \n    print(\"Loading and preprocessing DC weather data...\")\n    df = pd.read_csv(data_path)\n    if 'datetime' not in df.columns: print(\"ERROR: 'datetime' column not found.\"); return\n    df['datetime'] = pd.to_datetime(df['datetime']); df.set_index('datetime', inplace=True)\n    \n    feature_cols = ['temp', 'tempmax', 'tempmin', 'precip', 'snow', 'snowdepth', 'windspeed']\n    target_cols = ['temp'] # Predicting average temperature\n    target_name_for_plot = \"Average Temperature (temp)\"\n    \n    existing_cols = [col for col in feature_cols if col in df.columns]\n    df_processed = df[existing_cols].copy()\n    for col in df_processed.columns: df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n    df_processed.ffill(inplace=True); df_processed.bfill(inplace=True)\n    if df_processed.isnull().sum().any(): df_processed.fillna(0, inplace=True)\n\n    X_all, y_all = load_mts_data(df_processed, existing_cols, target_cols, LOOKBACK)\n    if X_all.shape[0] == 0: print(\"No valid samples generated. Exiting.\"); return\n    \n    X_tv_and_val, X_test, y_tv_and_val, y_test = chronological_train_test_split(X_all, y_all, test_ratio=0.2)\n    if X_tv_and_val.shape[0] == 0: print(\"No training/validation samples. Exiting.\"); return\n\n    # --- 3. CREATE DATASETS ONCE ---\n    adj_matrix_template = torch.zeros(LOOKBACK, LOOKBACK, dtype=torch.float32)\n    for i in range(LOOKBACK):\n        if i > 0: adj_matrix_template[i, i-1] = 1\n        if i < LOOKBACK - 1: adj_matrix_template[i, i+1] = 1\n        adj_matrix_template[i, i] = 1\n    adj_matrix_template = adj_matrix_template.to(DEVICE)\n\n    full_train_val_dataset = TimeSeriesDataset(X_tv_and_val, y_tv_and_val, adj_matrix_template.cpu(), is_train=True)\n    scaler_X, scaler_y = full_train_val_dataset.get_scalers()\n\n    train_s = int(0.8 * len(full_train_val_dataset)); val_s = len(full_train_val_dataset) - train_s\n    gen = torch.Generator().manual_seed(42)\n    train_dataset, val_dataset = torch.utils.data.random_split(full_train_val_dataset, [train_s, val_s], generator=gen)\n\n    # --- 4. RUN GRID SEARCH LOOP ---\n    best_params = {}; best_val_loss = float('inf')\n    param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n\n    for params in param_combinations:\n        val_loss = run_training_session(\n            params=params, train_dataset=train_dataset, val_dataset=val_dataset,\n            adj_matrix_template=adj_matrix_template, model_input_size=X_all.shape[-1],\n            model_output_size=y_all.shape[-1], output_dir=output_dir\n        )\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss; best_params = params\n            print(f\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n            print(f\"!!! New best validation loss: {best_val_loss:.4f} with params: {best_params}\")\n            print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n\n    print(f\"\\n--- Grid Search Complete ---\")\n    print(f\"Best validation loss found: {best_val_loss:.4f}\")\n    print(f\"Best hyperparameters: {best_params}\")\n\n    # --- 5. FINAL EVALUATION WITH BEST HYPERPARAMETERS ---\n    print(\"\\n--- Training final model with best hyperparameters on all training data ---\")\n    \n    final_model = MTSPredictorGATTransformer(\n        input_size=X_all.shape[-1], output_size=y_all.shape[-1],\n        dropout_rate=best_params.get('DROPOUT_RATE', 0.2)\n    ).to(DEVICE)\n    \n    final_optimizer = optim.Adam(\n        final_model.parameters(), \n        lr=best_params.get('LEARNING_RATE', 0.001), \n        weight_decay=best_params.get('WEIGHT_DECAY', 1e-5)\n    )\n    # --- FIX WAS HERE ---\n    final_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(final_optimizer, 'min', patience=7, verbose=True)\n    # --- END OF FIX ---\n    \n    final_criterion = nn.MSELoss()\n    \n    final_train_loader = DataLoader(full_train_val_dataset, batch_size=best_params.get('BATCH_SIZE', 128), shuffle=True)\n    final_val_loader = DataLoader(val_dataset, batch_size=best_params.get('BATCH_SIZE', 128), shuffle=False)\n    \n    final_model_save_path = os.path.join(output_dir, \"final_best_model.pt\")\n    \n    print(\"Starting final training run...\")\n    trained_model, train_L, val_L, _ = train_model_gat_based(\n        final_model, final_train_loader, final_val_loader, final_criterion, final_optimizer, final_scheduler,\n        NUM_EPOCHS, PATIENCE, DEVICE, final_model_save_path\n    )\n    plot_training_losses(train_L, val_L, f\"{MODEL_NAME}_final\", save_dir=viz_dir)\n   \n    if X_test.shape[0] > 0:\n        print(\"\\n--- Evaluating final model on test set ---\")\n        test_dataset = TimeSeriesDataset(X_test, y_test, adj_matrix_template.cpu(), scaler_X, scaler_y, is_train=False)\n        test_loader = DataLoader(test_dataset, batch_size=best_params.get('BATCH_SIZE', 128), shuffle=False)\n        preds_o, tgts_o, _, _, _, _ = evaluate_model_gat_based(trained_model, test_loader, scaler_y, DEVICE, f\"{MODEL_NAME}_final\")\n        plot_predictions_vs_true(tgts_o, preds_o, f\"{MODEL_NAME}_final\", target_name=target_name_for_plot, save_dir=viz_dir)\n    else: print(\"Test set was empty. Skipping final evaluation.\")\n\n    print(f\"Grid search and final evaluation complete. Results in {output_dir}\")\nif __name__ == \"__main__\":\n    main_mts_forecaster()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T15:24:20.515714Z","iopub.execute_input":"2025-06-29T15:24:20.516087Z","iopub.status.idle":"2025-06-29T16:01:51.866025Z","shell.execute_reply.started":"2025-06-29T15:24:20.516044Z","shell.execute_reply":"2025-06-29T16:01:51.864938Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"--- Running GAT_Transformer_DC_Weather_TAVG Grid Search ---\nUsing device: cpu\n--- Hyperparameter Grid ---\n{'LEARNING_RATE': [0.001, 0.0005], 'DROPOUT_RATE': [0.2, 0.4], 'WEIGHT_DECAY': [1e-05, 0.0001], 'BATCH_SIZE': [64, 128]}\nLoading and preprocessing DC weather data...\n--- Creating sequences with lookback=30 ---\nLoaded MTS data: X shape: (3289, 30, 7), y shape: (3289, 1)\nX_train shape: (2632, 30, 7), X_test shape: (657, 30, 7)\n\n--- Running with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 64} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.3716, Val Loss=0.1580, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2077, Val Loss=0.1227, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.1778, Val Loss=0.1145, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.1676, Val Loss=0.1064, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1671, Val Loss=0.1014, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1630, Val Loss=0.1075, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1600, Val Loss=0.1057, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1566, Val Loss=0.1070, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1538, Val Loss=0.1197, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1498, Val Loss=0.1039, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1425, Val Loss=0.0989, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1430, Val Loss=0.1002, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1389, Val Loss=0.1028, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1419, Val Loss=0.0940, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1439, Val Loss=0.1008, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1437, Val Loss=0.1008, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1367, Val Loss=0.0942, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1350, Val Loss=0.0932, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1408, Val Loss=0.1057, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1342, Val Loss=0.0949, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1315, Val Loss=0.0919, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 12.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1244, Val Loss=0.0942, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1294, Val Loss=0.0913, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1294, Val Loss=0.1029, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1355, Val Loss=0.0863, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1240, Val Loss=0.1102, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1391, Val Loss=0.0953, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1249, Val Loss=0.0897, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1241, Val Loss=0.0963, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1293, Val Loss=0.1019, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1187, Val Loss=0.0962, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1176, Val Loss=0.1067, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1259, Val Loss=0.1051, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1108, Val Loss=0.0926, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1151, Val Loss=0.0928, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1156, Val Loss=0.0934, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1153, Val Loss=0.0925, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1128, Val Loss=0.0934, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1096, Val Loss=0.0928, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1102, Val Loss=0.0931, LR=0.000100\nEarly stopping @ epoch 40\n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n!!! New best validation loss: 0.0863 with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 64}\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\n--- Running with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 128} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.3725, Val Loss=0.1773, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2353, Val Loss=0.1286, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2026, Val Loss=0.1233, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.1880, Val Loss=0.1140, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1689, Val Loss=0.1117, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1642, Val Loss=0.1058, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1645, Val Loss=0.1048, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1619, Val Loss=0.0996, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1549, Val Loss=0.0979, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1610, Val Loss=0.0984, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1564, Val Loss=0.1010, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1460, Val Loss=0.0965, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1492, Val Loss=0.0972, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1470, Val Loss=0.0974, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1416, Val Loss=0.1067, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1419, Val Loss=0.0949, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1407, Val Loss=0.0939, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1423, Val Loss=0.1080, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1425, Val Loss=0.0891, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1344, Val Loss=0.1160, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1435, Val Loss=0.0893, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1369, Val Loss=0.1099, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1367, Val Loss=0.0956, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1387, Val Loss=0.0907, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1347, Val Loss=0.0891, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1383, Val Loss=0.1048, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1314, Val Loss=0.0886, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1364, Val Loss=0.0953, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1318, Val Loss=0.0900, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1316, Val Loss=0.1016, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1280, Val Loss=0.0872, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1287, Val Loss=0.0983, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1235, Val Loss=0.0888, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1280, Val Loss=0.1000, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1236, Val Loss=0.0912, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1273, Val Loss=0.1060, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1210, Val Loss=0.0862, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1231, Val Loss=0.0977, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1208, Val Loss=0.0860, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1177, Val Loss=0.0844, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1214, Val Loss=0.0946, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1212, Val Loss=0.0890, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1174, Val Loss=0.0882, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1184, Val Loss=0.0865, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1207, Val Loss=0.0946, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1171, Val Loss=0.0906, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1185, Val Loss=0.0903, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1173, Val Loss=0.1055, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1137, Val Loss=0.0878, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1133, Val Loss=0.0880, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1117, Val Loss=0.0861, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1129, Val Loss=0.0870, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1121, Val Loss=0.0881, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1078, Val Loss=0.0860, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1080, Val Loss=0.0864, LR=0.000100\nEarly stopping @ epoch 55\n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n!!! New best validation loss: 0.0844 with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 128}\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\n--- Running with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 64} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.4331, Val Loss=0.1954, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2276, Val Loss=0.1217, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.1941, Val Loss=0.1180, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.1881, Val Loss=0.1301, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1634, Val Loss=0.1036, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1593, Val Loss=0.1038, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1533, Val Loss=0.1056, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1626, Val Loss=0.1156, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1576, Val Loss=0.0966, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1540, Val Loss=0.1358, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1521, Val Loss=0.0962, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1466, Val Loss=0.0981, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1397, Val Loss=0.1104, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1417, Val Loss=0.0982, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1327, Val Loss=0.0957, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1417, Val Loss=0.0970, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1319, Val Loss=0.0945, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1338, Val Loss=0.0935, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1398, Val Loss=0.1134, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1364, Val Loss=0.1052, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 12.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1421, Val Loss=0.0957, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1338, Val Loss=0.1188, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1271, Val Loss=0.0937, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1385, Val Loss=0.1034, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1297, Val Loss=0.1013, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1273, Val Loss=0.0944, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1246, Val Loss=0.0890, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1235, Val Loss=0.0900, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1189, Val Loss=0.0910, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1220, Val Loss=0.0896, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1168, Val Loss=0.0906, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1246, Val Loss=0.0887, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1286, Val Loss=0.0915, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1205, Val Loss=0.0905, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1193, Val Loss=0.0897, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1146, Val Loss=0.0888, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1178, Val Loss=0.0915, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1184, Val Loss=0.0889, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1172, Val Loss=0.0887, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1172, Val Loss=0.0882, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1177, Val Loss=0.0894, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1137, Val Loss=0.0895, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1186, Val Loss=0.0894, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1158, Val Loss=0.0916, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1201, Val Loss=0.0898, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1178, Val Loss=0.0916, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1176, Val Loss=0.0905, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1161, Val Loss=0.0900, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1134, Val Loss=0.0897, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1233, Val Loss=0.0898, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1198, Val Loss=0.0897, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1167, Val Loss=0.0894, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1172, Val Loss=0.0890, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1166, Val Loss=0.0891, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1158, Val Loss=0.0890, LR=0.000010\nEarly stopping @ epoch 55\n\n--- Running with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 128} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.4097, Val Loss=0.1669, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2295, Val Loss=0.1245, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.1919, Val Loss=0.1357, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.1891, Val Loss=0.1089, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1712, Val Loss=0.1125, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1660, Val Loss=0.1017, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1670, Val Loss=0.1105, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1589, Val Loss=0.1054, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1617, Val Loss=0.1010, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1557, Val Loss=0.1038, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1535, Val Loss=0.1048, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1518, Val Loss=0.1016, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1472, Val Loss=0.0889, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1413, Val Loss=0.0895, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1442, Val Loss=0.0925, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1413, Val Loss=0.0954, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1458, Val Loss=0.0894, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1415, Val Loss=0.0909, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1439, Val Loss=0.0894, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1421, Val Loss=0.0893, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1347, Val Loss=0.1009, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1391, Val Loss=0.0900, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1280, Val Loss=0.0884, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1332, Val Loss=0.0907, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1275, Val Loss=0.0892, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1282, Val Loss=0.0884, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1280, Val Loss=0.0889, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1255, Val Loss=0.0873, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1301, Val Loss=0.0889, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1264, Val Loss=0.0901, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1296, Val Loss=0.0880, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1331, Val Loss=0.0872, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1289, Val Loss=0.0872, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1285, Val Loss=0.0873, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1279, Val Loss=0.0890, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1266, Val Loss=0.0883, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1290, Val Loss=0.0878, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1248, Val Loss=0.0872, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1259, Val Loss=0.0868, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1204, Val Loss=0.0871, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1226, Val Loss=0.0880, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1233, Val Loss=0.0872, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1251, Val Loss=0.0879, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1289, Val Loss=0.0870, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1277, Val Loss=0.0878, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1264, Val Loss=0.0880, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1238, Val Loss=0.0862, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1206, Val Loss=0.0887, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1295, Val Loss=0.0866, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1249, Val Loss=0.0883, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1294, Val Loss=0.0862, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1285, Val Loss=0.0864, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1219, Val Loss=0.0853, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1271, Val Loss=0.0859, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1242, Val Loss=0.0881, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1290, Val Loss=0.0866, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1205, Val Loss=0.0854, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1252, Val Loss=0.0870, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1210, Val Loss=0.0883, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1244, Val Loss=0.0869, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61: Train Loss=0.1177, Val Loss=0.0884, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62: Train Loss=0.1255, Val Loss=0.0876, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63: Train Loss=0.1195, Val Loss=0.0868, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64: Train Loss=0.1203, Val Loss=0.0863, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65: Train Loss=0.1206, Val Loss=0.0861, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66: Train Loss=0.1206, Val Loss=0.0860, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67: Train Loss=0.1188, Val Loss=0.0862, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68: Train Loss=0.1195, Val Loss=0.0863, LR=0.000010\nEarly stopping @ epoch 68\n\n--- Running with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 64} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.4763, Val Loss=0.2013, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.3066, Val Loss=0.1450, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2713, Val Loss=0.1372, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2527, Val Loss=0.1445, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.2251, Val Loss=0.1304, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.2256, Val Loss=0.1159, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 12.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.2210, Val Loss=0.1176, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.2141, Val Loss=0.1220, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.2160, Val Loss=0.1098, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.2178, Val Loss=0.1099, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.2029, Val Loss=0.1241, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.2069, Val Loss=0.1192, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.2059, Val Loss=0.1147, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1871, Val Loss=0.1076, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.2066, Val Loss=0.1240, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1871, Val Loss=0.1059, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1940, Val Loss=0.1048, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.2023, Val Loss=0.1021, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1874, Val Loss=0.1031, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1817, Val Loss=0.0997, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1838, Val Loss=0.1015, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1772, Val Loss=0.1012, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1896, Val Loss=0.1033, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1889, Val Loss=0.1042, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1859, Val Loss=0.1031, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1841, Val Loss=0.0989, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1849, Val Loss=0.1043, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1817, Val Loss=0.0989, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1784, Val Loss=0.0989, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1767, Val Loss=0.1014, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1764, Val Loss=0.0979, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1735, Val Loss=0.0983, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1844, Val Loss=0.1031, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1801, Val Loss=0.0983, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1834, Val Loss=0.1027, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1758, Val Loss=0.1016, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1798, Val Loss=0.0964, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1784, Val Loss=0.1293, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1790, Val Loss=0.1010, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1813, Val Loss=0.1113, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1685, Val Loss=0.0950, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1728, Val Loss=0.1003, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1800, Val Loss=0.0961, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1855, Val Loss=0.1009, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1753, Val Loss=0.0958, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1693, Val Loss=0.0963, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1763, Val Loss=0.1116, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1672, Val Loss=0.0928, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1765, Val Loss=0.1032, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1713, Val Loss=0.1001, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1705, Val Loss=0.1070, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1659, Val Loss=0.0940, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1711, Val Loss=0.0954, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1698, Val Loss=0.0975, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1738, Val Loss=0.0981, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1694, Val Loss=0.0972, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1598, Val Loss=0.0949, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1679, Val Loss=0.0924, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1679, Val Loss=0.0937, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1534, Val Loss=0.0919, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61: Train Loss=0.1571, Val Loss=0.0918, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62: Train Loss=0.1620, Val Loss=0.0936, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63: Train Loss=0.1653, Val Loss=0.0926, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64: Train Loss=0.1606, Val Loss=0.0922, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65: Train Loss=0.1536, Val Loss=0.0925, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66: Train Loss=0.1579, Val Loss=0.0928, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67: Train Loss=0.1662, Val Loss=0.0915, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68: Train Loss=0.1631, Val Loss=0.0926, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 69: Train Loss=0.1632, Val Loss=0.0934, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70: Train Loss=0.1579, Val Loss=0.0940, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 71: Train Loss=0.1490, Val Loss=0.0915, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72: Train Loss=0.1608, Val Loss=0.0931, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 73: Train Loss=0.1562, Val Loss=0.0928, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74: Train Loss=0.1608, Val Loss=0.0922, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 75: Train Loss=0.1603, Val Loss=0.0920, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 76: Train Loss=0.1517, Val Loss=0.0944, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 77: Train Loss=0.1625, Val Loss=0.0915, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 78: Train Loss=0.1587, Val Loss=0.0926, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 79: Train Loss=0.1576, Val Loss=0.0930, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 80: Train Loss=0.1567, Val Loss=0.0922, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 81/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 81: Train Loss=0.1605, Val Loss=0.0920, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 82/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 82: Train Loss=0.1513, Val Loss=0.0918, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 83/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 83: Train Loss=0.1616, Val Loss=0.0919, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 84: Train Loss=0.1559, Val Loss=0.0916, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 85: Train Loss=0.1538, Val Loss=0.0916, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 86: Train Loss=0.1578, Val Loss=0.0917, LR=0.000010\nEarly stopping @ epoch 86\n\n--- Running with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 128} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.6462, Val Loss=0.2854, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.3496, Val Loss=0.1735, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2939, Val Loss=0.1382, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2535, Val Loss=0.1276, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.2449, Val Loss=0.1232, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.2415, Val Loss=0.1123, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.2347, Val Loss=0.1199, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.2254, Val Loss=0.1141, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.2062, Val Loss=0.1138, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.2159, Val Loss=0.1153, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.2127, Val Loss=0.1177, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.2058, Val Loss=0.1107, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.2123, Val Loss=0.1038, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1911, Val Loss=0.1051, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.2054, Val Loss=0.1030, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.2008, Val Loss=0.1111, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1888, Val Loss=0.0998, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1922, Val Loss=0.1074, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1944, Val Loss=0.1055, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1937, Val Loss=0.1039, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1881, Val Loss=0.1170, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1955, Val Loss=0.1000, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1879, Val Loss=0.0970, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1873, Val Loss=0.1101, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1855, Val Loss=0.1037, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1859, Val Loss=0.1006, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1838, Val Loss=0.1009, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  6.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1948, Val Loss=0.1072, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1846, Val Loss=0.0911, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1997, Val Loss=0.1032, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1736, Val Loss=0.0949, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1857, Val Loss=0.0976, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1862, Val Loss=0.1034, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1885, Val Loss=0.0920, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1785, Val Loss=0.1057, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1688, Val Loss=0.0942, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1848, Val Loss=0.1021, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1837, Val Loss=0.0999, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1731, Val Loss=0.0922, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1835, Val Loss=0.0968, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1807, Val Loss=0.0943, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1786, Val Loss=0.0930, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1728, Val Loss=0.0942, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1745, Val Loss=0.0927, LR=0.000100\nEarly stopping @ epoch 44\n\n--- Running with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 64} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.4968, Val Loss=0.1976, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2841, Val Loss=0.1771, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2681, Val Loss=0.1268, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2399, Val Loss=0.1165, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.2294, Val Loss=0.1182, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.2175, Val Loss=0.1268, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.2252, Val Loss=0.1180, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.2105, Val Loss=0.1118, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.2039, Val Loss=0.1172, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.2075, Val Loss=0.1178, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.2067, Val Loss=0.1167, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.2091, Val Loss=0.1101, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1904, Val Loss=0.1088, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1948, Val Loss=0.1066, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.2016, Val Loss=0.1129, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.2046, Val Loss=0.1071, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1975, Val Loss=0.1071, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.2041, Val Loss=0.1043, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1996, Val Loss=0.1018, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1923, Val Loss=0.1039, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1942, Val Loss=0.1092, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1895, Val Loss=0.1210, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1878, Val Loss=0.1023, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1861, Val Loss=0.1021, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1836, Val Loss=0.1025, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1781, Val Loss=0.1007, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1828, Val Loss=0.1007, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1862, Val Loss=0.1040, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1872, Val Loss=0.1049, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1765, Val Loss=0.1016, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1839, Val Loss=0.1021, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1837, Val Loss=0.1042, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1929, Val Loss=0.1039, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1721, Val Loss=0.0987, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1797, Val Loss=0.1085, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1820, Val Loss=0.1105, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1862, Val Loss=0.1020, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1775, Val Loss=0.1164, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1651, Val Loss=0.1058, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1738, Val Loss=0.0983, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1696, Val Loss=0.1097, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1773, Val Loss=0.1030, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1825, Val Loss=0.0981, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1734, Val Loss=0.0961, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1698, Val Loss=0.0962, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1735, Val Loss=0.0988, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1709, Val Loss=0.1157, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1653, Val Loss=0.1002, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1691, Val Loss=0.1091, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1727, Val Loss=0.0966, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1666, Val Loss=0.0975, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1667, Val Loss=0.0954, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1766, Val Loss=0.1039, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1671, Val Loss=0.0958, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1732, Val Loss=0.1100, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1685, Val Loss=0.0978, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1732, Val Loss=0.0956, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1598, Val Loss=0.1020, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1600, Val Loss=0.0986, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1676, Val Loss=0.1037, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61: Train Loss=0.1667, Val Loss=0.0959, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62: Train Loss=0.1696, Val Loss=0.0946, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63: Train Loss=0.1622, Val Loss=0.0951, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64: Train Loss=0.1542, Val Loss=0.0953, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65: Train Loss=0.1575, Val Loss=0.0941, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66: Train Loss=0.1621, Val Loss=0.0938, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67: Train Loss=0.1600, Val Loss=0.0951, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68: Train Loss=0.1565, Val Loss=0.0940, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 69: Train Loss=0.1576, Val Loss=0.0959, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70: Train Loss=0.1622, Val Loss=0.0929, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 71: Train Loss=0.1643, Val Loss=0.0952, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72: Train Loss=0.1554, Val Loss=0.0941, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 73: Train Loss=0.1524, Val Loss=0.0945, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74: Train Loss=0.1622, Val Loss=0.0933, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 75: Train Loss=0.1609, Val Loss=0.0933, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 76: Train Loss=0.1492, Val Loss=0.0931, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 77: Train Loss=0.1533, Val Loss=0.0942, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 78: Train Loss=0.1640, Val Loss=0.0940, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 79: Train Loss=0.1547, Val Loss=0.0946, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 80: Train Loss=0.1558, Val Loss=0.0942, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 81/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 81: Train Loss=0.1525, Val Loss=0.0935, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 82/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 82: Train Loss=0.1614, Val Loss=0.0932, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 83/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 83: Train Loss=0.1582, Val Loss=0.0937, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 84: Train Loss=0.1590, Val Loss=0.0936, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 85: Train Loss=0.1555, Val Loss=0.0931, LR=0.000010\nEarly stopping @ epoch 85\n\n--- Running with params: {'LEARNING_RATE': 0.001, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 128} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.5766, Val Loss=0.2605, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.3398, Val Loss=0.1685, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2827, Val Loss=0.1432, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2607, Val Loss=0.1242, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.2461, Val Loss=0.1220, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.2471, Val Loss=0.1236, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.2419, Val Loss=0.1163, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.2144, Val Loss=0.1209, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.2222, Val Loss=0.1136, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.2102, Val Loss=0.1165, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.2115, Val Loss=0.1147, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.2123, Val Loss=0.1152, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.2047, Val Loss=0.1058, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.2037, Val Loss=0.1049, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1918, Val Loss=0.1030, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1981, Val Loss=0.1084, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1995, Val Loss=0.1042, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.2052, Val Loss=0.1033, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1971, Val Loss=0.1002, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1990, Val Loss=0.1064, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1949, Val Loss=0.1016, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1952, Val Loss=0.1030, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1952, Val Loss=0.0984, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1929, Val Loss=0.1038, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1919, Val Loss=0.0967, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1929, Val Loss=0.0969, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1868, Val Loss=0.0998, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1794, Val Loss=0.1035, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1901, Val Loss=0.0945, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1845, Val Loss=0.1032, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1889, Val Loss=0.0959, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1843, Val Loss=0.0946, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1752, Val Loss=0.0976, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1805, Val Loss=0.1007, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1885, Val Loss=0.1100, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1913, Val Loss=0.0959, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1850, Val Loss=0.0996, LR=0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1946, Val Loss=0.0951, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1836, Val Loss=0.0944, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1681, Val Loss=0.0932, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1742, Val Loss=0.0951, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1751, Val Loss=0.0956, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1815, Val Loss=0.0940, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1723, Val Loss=0.0932, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1727, Val Loss=0.0925, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1683, Val Loss=0.0928, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1734, Val Loss=0.0959, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1746, Val Loss=0.0928, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1755, Val Loss=0.0933, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1716, Val Loss=0.0949, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1735, Val Loss=0.0935, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1745, Val Loss=0.0937, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1744, Val Loss=0.0944, LR=0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1698, Val Loss=0.0937, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1727, Val Loss=0.0933, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1715, Val Loss=0.0931, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1790, Val Loss=0.0934, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1711, Val Loss=0.0933, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1759, Val Loss=0.0931, LR=0.000010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1779, Val Loss=0.0933, LR=0.000010\nEarly stopping @ epoch 60\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 64} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.5129, Val Loss=0.1949, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2426, Val Loss=0.1401, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2028, Val Loss=0.1251, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.1905, Val Loss=0.1294, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1742, Val Loss=0.1132, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1778, Val Loss=0.1085, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1644, Val Loss=0.1111, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1589, Val Loss=0.1096, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1574, Val Loss=0.1080, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1551, Val Loss=0.1008, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1566, Val Loss=0.0973, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1490, Val Loss=0.1127, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1494, Val Loss=0.1105, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1448, Val Loss=0.1024, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1426, Val Loss=0.0970, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1438, Val Loss=0.0988, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1450, Val Loss=0.0956, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1423, Val Loss=0.1034, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1403, Val Loss=0.0992, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1288, Val Loss=0.0995, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1448, Val Loss=0.0932, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1369, Val Loss=0.1056, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1411, Val Loss=0.0926, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1319, Val Loss=0.0958, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1382, Val Loss=0.0968, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1448, Val Loss=0.1029, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1323, Val Loss=0.0977, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1326, Val Loss=0.0912, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1303, Val Loss=0.0966, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1300, Val Loss=0.0931, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1285, Val Loss=0.0944, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1222, Val Loss=0.0951, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1323, Val Loss=0.0909, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1321, Val Loss=0.0979, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1289, Val Loss=0.0936, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1307, Val Loss=0.0900, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1266, Val Loss=0.0931, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1215, Val Loss=0.0907, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1204, Val Loss=0.0890, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1199, Val Loss=0.1039, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1270, Val Loss=0.0930, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1208, Val Loss=0.0943, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1222, Val Loss=0.0903, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1248, Val Loss=0.0916, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1273, Val Loss=0.0900, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1281, Val Loss=0.0924, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1231, Val Loss=0.0945, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1174, Val Loss=0.0893, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1124, Val Loss=0.0893, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1132, Val Loss=0.0896, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1153, Val Loss=0.0901, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1114, Val Loss=0.0884, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1161, Val Loss=0.0907, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1155, Val Loss=0.0895, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1092, Val Loss=0.0903, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 12.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1140, Val Loss=0.0909, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1143, Val Loss=0.0898, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1151, Val Loss=0.0900, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1131, Val Loss=0.0898, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1113, Val Loss=0.0895, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61: Train Loss=0.1106, Val Loss=0.0895, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62: Train Loss=0.1115, Val Loss=0.0897, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63: Train Loss=0.1086, Val Loss=0.0897, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64: Train Loss=0.1089, Val Loss=0.0894, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65: Train Loss=0.1175, Val Loss=0.0892, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66: Train Loss=0.1132, Val Loss=0.0893, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67: Train Loss=0.1136, Val Loss=0.0895, LR=0.000005\nEarly stopping @ epoch 67\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 128} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.5424, Val Loss=0.3079, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2846, Val Loss=0.1813, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2306, Val Loss=0.1445, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  6.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.1969, Val Loss=0.1295, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1843, Val Loss=0.1254, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1737, Val Loss=0.1159, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1685, Val Loss=0.1094, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1725, Val Loss=0.1121, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1592, Val Loss=0.1112, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1620, Val Loss=0.1111, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1514, Val Loss=0.1110, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1523, Val Loss=0.1031, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1543, Val Loss=0.1016, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1559, Val Loss=0.1025, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1510, Val Loss=0.1036, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1475, Val Loss=0.0966, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1474, Val Loss=0.0960, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1486, Val Loss=0.1011, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1515, Val Loss=0.1076, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1472, Val Loss=0.0990, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1456, Val Loss=0.0943, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1427, Val Loss=0.0920, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1437, Val Loss=0.0947, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1407, Val Loss=0.0984, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1395, Val Loss=0.0907, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1425, Val Loss=0.0904, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1343, Val Loss=0.0944, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1355, Val Loss=0.0931, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1343, Val Loss=0.0907, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1367, Val Loss=0.0933, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1316, Val Loss=0.0978, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1416, Val Loss=0.0924, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1334, Val Loss=0.0935, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1300, Val Loss=0.0983, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1276, Val Loss=0.0899, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1325, Val Loss=0.0896, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1261, Val Loss=0.0906, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1312, Val Loss=0.0894, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1239, Val Loss=0.0907, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1302, Val Loss=0.0890, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1342, Val Loss=0.0904, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1301, Val Loss=0.0905, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1326, Val Loss=0.0897, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1269, Val Loss=0.0908, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1264, Val Loss=0.0890, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1257, Val Loss=0.0904, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1321, Val Loss=0.0894, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1319, Val Loss=0.0900, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1354, Val Loss=0.0900, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1295, Val Loss=0.0900, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1280, Val Loss=0.0898, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1263, Val Loss=0.0896, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1309, Val Loss=0.0896, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1318, Val Loss=0.0897, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1323, Val Loss=0.0895, LR=0.000005\nEarly stopping @ epoch 55\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 64} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.4456, Val Loss=0.1845, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2319, Val Loss=0.1293, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2048, Val Loss=0.1195, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.1882, Val Loss=0.1098, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1767, Val Loss=0.1066, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1674, Val Loss=0.1099, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1666, Val Loss=0.1019, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1589, Val Loss=0.1003, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1585, Val Loss=0.1055, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1511, Val Loss=0.0982, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1513, Val Loss=0.1100, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1543, Val Loss=0.0960, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1481, Val Loss=0.1027, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1483, Val Loss=0.0975, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1430, Val Loss=0.1024, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1448, Val Loss=0.0977, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1410, Val Loss=0.0966, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1395, Val Loss=0.0980, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1398, Val Loss=0.1023, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1411, Val Loss=0.0936, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1357, Val Loss=0.1006, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1335, Val Loss=0.0965, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1319, Val Loss=0.0964, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1389, Val Loss=0.0963, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1325, Val Loss=0.0923, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1410, Val Loss=0.0938, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1279, Val Loss=0.1046, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1371, Val Loss=0.1019, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1346, Val Loss=0.0935, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1268, Val Loss=0.1037, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1329, Val Loss=0.0948, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1306, Val Loss=0.0925, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1263, Val Loss=0.0919, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1290, Val Loss=0.0952, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1265, Val Loss=0.1015, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1240, Val Loss=0.1012, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1256, Val Loss=0.0943, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1201, Val Loss=0.0909, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1232, Val Loss=0.0921, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1190, Val Loss=0.0965, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1259, Val Loss=0.0916, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1175, Val Loss=0.0894, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1136, Val Loss=0.0972, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1168, Val Loss=0.0938, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1200, Val Loss=0.0913, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1211, Val Loss=0.0943, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1171, Val Loss=0.0906, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1204, Val Loss=0.0989, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1207, Val Loss=0.1031, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1186, Val Loss=0.0915, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1180, Val Loss=0.0938, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1125, Val Loss=0.0902, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1102, Val Loss=0.0910, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1157, Val Loss=0.0913, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1101, Val Loss=0.0912, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1139, Val Loss=0.0910, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1111, Val Loss=0.0911, LR=0.000050\nEarly stopping @ epoch 57\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 128} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.6003, Val Loss=0.2575, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.2645, Val Loss=0.1637, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2324, Val Loss=0.1371, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2045, Val Loss=0.1240, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1769, Val Loss=0.1135, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1749, Val Loss=0.1094, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1684, Val Loss=0.1054, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1663, Val Loss=0.1058, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1595, Val Loss=0.1044, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1534, Val Loss=0.1061, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1644, Val Loss=0.1067, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1608, Val Loss=0.1002, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1526, Val Loss=0.1022, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1498, Val Loss=0.0966, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1466, Val Loss=0.1036, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1492, Val Loss=0.0959, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1442, Val Loss=0.0986, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1484, Val Loss=0.0940, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1447, Val Loss=0.1089, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1434, Val Loss=0.0930, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1457, Val Loss=0.0913, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1415, Val Loss=0.0925, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1415, Val Loss=0.0936, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1372, Val Loss=0.0895, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1420, Val Loss=0.0903, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1369, Val Loss=0.0923, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1398, Val Loss=0.0936, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1369, Val Loss=0.0892, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1335, Val Loss=0.0906, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1384, Val Loss=0.0912, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1404, Val Loss=0.0891, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1286, Val Loss=0.0915, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1339, Val Loss=0.0853, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1331, Val Loss=0.0928, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1269, Val Loss=0.0844, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1333, Val Loss=0.0882, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1244, Val Loss=0.0851, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1315, Val Loss=0.1001, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1236, Val Loss=0.0837, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1307, Val Loss=0.0868, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1278, Val Loss=0.0902, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1318, Val Loss=0.0865, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1291, Val Loss=0.1002, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1240, Val Loss=0.0977, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1233, Val Loss=0.0846, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1249, Val Loss=0.0862, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1213, Val Loss=0.0916, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1222, Val Loss=0.0841, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1172, Val Loss=0.0839, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1194, Val Loss=0.0845, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1199, Val Loss=0.0837, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1203, Val Loss=0.0835, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1202, Val Loss=0.0836, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1158, Val Loss=0.0832, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1185, Val Loss=0.0829, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1200, Val Loss=0.0825, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1230, Val Loss=0.0829, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1173, Val Loss=0.0836, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1202, Val Loss=0.0824, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1215, Val Loss=0.0839, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61: Train Loss=0.1234, Val Loss=0.0831, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62: Train Loss=0.1123, Val Loss=0.0831, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63: Train Loss=0.1250, Val Loss=0.0831, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64: Train Loss=0.1205, Val Loss=0.0828, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65: Train Loss=0.1192, Val Loss=0.0833, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66: Train Loss=0.1196, Val Loss=0.0826, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67: Train Loss=0.1215, Val Loss=0.0825, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68: Train Loss=0.1168, Val Loss=0.0826, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 69: Train Loss=0.1207, Val Loss=0.0826, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70: Train Loss=0.1161, Val Loss=0.0826, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 71: Train Loss=0.1198, Val Loss=0.0825, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72: Train Loss=0.1179, Val Loss=0.0826, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 73: Train Loss=0.1145, Val Loss=0.0826, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74: Train Loss=0.1220, Val Loss=0.0826, LR=0.000005\nEarly stopping @ epoch 74\n\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n!!! New best validation loss: 0.0824 with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 128}\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 64} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.7050, Val Loss=0.2995, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.3832, Val Loss=0.1753, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.3073, Val Loss=0.1365, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2589, Val Loss=0.1334, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.2673, Val Loss=0.1190, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.2581, Val Loss=0.1140, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.2404, Val Loss=0.1167, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.2300, Val Loss=0.1254, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.2344, Val Loss=0.1123, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.2320, Val Loss=0.1108, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.2174, Val Loss=0.1077, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.2271, Val Loss=0.1083, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.2213, Val Loss=0.1120, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.2214, Val Loss=0.1178, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.2041, Val Loss=0.1134, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.2075, Val Loss=0.1120, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.2172, Val Loss=0.1181, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.2147, Val Loss=0.1134, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.2116, Val Loss=0.1066, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.2062, Val Loss=0.1055, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.2109, Val Loss=0.1298, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1983, Val Loss=0.1038, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.2085, Val Loss=0.1150, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.2077, Val Loss=0.1193, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1901, Val Loss=0.1018, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.2070, Val Loss=0.1048, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.2035, Val Loss=0.1012, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1971, Val Loss=0.1066, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.2090, Val Loss=0.1033, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1969, Val Loss=0.1148, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.2033, Val Loss=0.1121, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1942, Val Loss=0.1192, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1858, Val Loss=0.1008, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1899, Val Loss=0.1038, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1824, Val Loss=0.1008, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1948, Val Loss=0.1268, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1902, Val Loss=0.1005, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1874, Val Loss=0.1010, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1924, Val Loss=0.1085, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1872, Val Loss=0.1000, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1834, Val Loss=0.1161, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1835, Val Loss=0.1045, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1891, Val Loss=0.1124, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1884, Val Loss=0.1107, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1893, Val Loss=0.0988, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1846, Val Loss=0.1027, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1913, Val Loss=0.1016, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1814, Val Loss=0.1120, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1886, Val Loss=0.1022, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1900, Val Loss=0.1087, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1941, Val Loss=0.1098, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1899, Val Loss=0.1001, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1814, Val Loss=0.1003, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1817, Val Loss=0.1035, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1772, Val Loss=0.1035, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1828, Val Loss=0.1003, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1759, Val Loss=0.1021, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1798, Val Loss=0.0999, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1769, Val Loss=0.1000, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1699, Val Loss=0.1018, LR=0.000050\nEarly stopping @ epoch 60\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 128} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.6350, Val Loss=0.2706, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.3814, Val Loss=0.1994, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.3251, Val Loss=0.1676, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2907, Val Loss=0.1447, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.2729, Val Loss=0.1241, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.2498, Val Loss=0.1245, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.2482, Val Loss=0.1283, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.2270, Val Loss=0.1140, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.2290, Val Loss=0.1342, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.2324, Val Loss=0.1259, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.2249, Val Loss=0.1359, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.2276, Val Loss=0.1289, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.2361, Val Loss=0.1090, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.2061, Val Loss=0.1159, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.2158, Val Loss=0.1075, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.2078, Val Loss=0.1084, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.2050, Val Loss=0.1097, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.2111, Val Loss=0.1081, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.2092, Val Loss=0.1086, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.2170, Val Loss=0.1246, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.2025, Val Loss=0.1162, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.2023, Val Loss=0.1115, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1994, Val Loss=0.1091, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.2038, Val Loss=0.1084, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1991, Val Loss=0.1065, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1982, Val Loss=0.1061, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.2007, Val Loss=0.1065, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1943, Val Loss=0.1072, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1965, Val Loss=0.1057, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.2069, Val Loss=0.1069, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.2005, Val Loss=0.1069, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1994, Val Loss=0.1054, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.2014, Val Loss=0.1073, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1966, Val Loss=0.1075, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1972, Val Loss=0.1046, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1999, Val Loss=0.1068, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1975, Val Loss=0.1075, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.2006, Val Loss=0.1080, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.2017, Val Loss=0.1043, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1955, Val Loss=0.1054, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1989, Val Loss=0.1065, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1972, Val Loss=0.1059, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1934, Val Loss=0.1052, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1963, Val Loss=0.1041, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1966, Val Loss=0.1055, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1920, Val Loss=0.1045, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1921, Val Loss=0.1036, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1966, Val Loss=0.1050, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1949, Val Loss=0.1052, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1993, Val Loss=0.1072, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1985, Val Loss=0.1073, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1981, Val Loss=0.1043, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.2028, Val Loss=0.1041, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1909, Val Loss=0.1060, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1962, Val Loss=0.1043, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1955, Val Loss=0.1046, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1939, Val Loss=0.1044, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1995, Val Loss=0.1047, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1930, Val Loss=0.1046, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1934, Val Loss=0.1045, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61: Train Loss=0.1987, Val Loss=0.1043, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62: Train Loss=0.1953, Val Loss=0.1041, LR=0.000005\nEarly stopping @ epoch 62\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 64} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.6093, Val Loss=0.2679, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.3592, Val Loss=0.1809, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.3023, Val Loss=0.1446, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2750, Val Loss=0.1269, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.2628, Val Loss=0.1224, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.2543, Val Loss=0.1272, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.2419, Val Loss=0.1185, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.2364, Val Loss=0.1175, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.2472, Val Loss=0.1198, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.2308, Val Loss=0.1136, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.2167, Val Loss=0.1090, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.2234, Val Loss=0.1102, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.2175, Val Loss=0.1123, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.2061, Val Loss=0.1167, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.2106, Val Loss=0.1149, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.2027, Val Loss=0.1140, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.2055, Val Loss=0.1131, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.2138, Val Loss=0.1132, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.2124, Val Loss=0.1101, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.2038, Val Loss=0.1086, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1979, Val Loss=0.1117, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1905, Val Loss=0.1097, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1936, Val Loss=0.1092, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1995, Val Loss=0.1080, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 13.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1931, Val Loss=0.1078, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.2025, Val Loss=0.1070, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1927, Val Loss=0.1085, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1951, Val Loss=0.1083, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.2054, Val Loss=0.1071, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1958, Val Loss=0.1087, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1952, Val Loss=0.1067, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.2031, Val Loss=0.1069, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1916, Val Loss=0.1082, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1992, Val Loss=0.1092, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 18.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1877, Val Loss=0.1072, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1900, Val Loss=0.1081, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1915, Val Loss=0.1069, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1979, Val Loss=0.1068, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 15.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1936, Val Loss=0.1079, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1908, Val Loss=0.1074, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 14.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1992, Val Loss=0.1074, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1953, Val Loss=0.1073, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 16.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1858, Val Loss=0.1067, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 33/33 [00:01<00:00, 17.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1949, Val Loss=0.1069, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1939, Val Loss=0.1070, LR=0.000005\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 33/33 [00:02<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1971, Val Loss=0.1071, LR=0.000005\nEarly stopping @ epoch 46\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 128} ---\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.7715, Val Loss=0.4443, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.4401, Val Loss=0.2636, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.3712, Val Loss=0.1884, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.3107, Val Loss=0.1543, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.2809, Val Loss=0.1472, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.2653, Val Loss=0.1344, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.2639, Val Loss=0.1291, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.2360, Val Loss=0.1189, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.2570, Val Loss=0.1144, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.2328, Val Loss=0.1224, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.2283, Val Loss=0.1145, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.2315, Val Loss=0.1193, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.2341, Val Loss=0.1157, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.2257, Val Loss=0.1208, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.2242, Val Loss=0.1118, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.2044, Val Loss=0.1241, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.2151, Val Loss=0.1090, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.2054, Val Loss=0.1227, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.2089, Val Loss=0.1132, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.2146, Val Loss=0.1087, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.2063, Val Loss=0.1075, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1989, Val Loss=0.1072, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1998, Val Loss=0.1076, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.2078, Val Loss=0.1140, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.2072, Val Loss=0.1059, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.2007, Val Loss=0.1050, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.2021, Val Loss=0.1078, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.2077, Val Loss=0.1035, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.2081, Val Loss=0.1050, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.2034, Val Loss=0.1044, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1998, Val Loss=0.1020, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.2012, Val Loss=0.1069, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1977, Val Loss=0.1060, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1891, Val Loss=0.1027, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1981, Val Loss=0.0992, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1887, Val Loss=0.0986, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1961, Val Loss=0.0978, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  7.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1844, Val Loss=0.0979, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1937, Val Loss=0.0992, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1896, Val Loss=0.0950, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1869, Val Loss=0.0967, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1856, Val Loss=0.0996, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1879, Val Loss=0.1032, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1911, Val Loss=0.1025, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1862, Val Loss=0.0977, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1799, Val Loss=0.0979, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1868, Val Loss=0.0989, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 17/17 [00:01<00:00, 10.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1829, Val Loss=0.0951, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1854, Val Loss=0.0973, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1821, Val Loss=0.0972, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1749, Val Loss=0.0969, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  8.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1838, Val Loss=0.0961, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 17/17 [00:02<00:00,  8.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1819, Val Loss=0.0966, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1791, Val Loss=0.0960, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 17/17 [00:01<00:00,  9.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1812, Val Loss=0.0957, LR=0.000050\nEarly stopping @ epoch 55\n\n--- Grid Search Complete ---\nBest validation loss found: 0.0824\nBest hyperparameters: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.2, 'WEIGHT_DECAY': 0.0001, 'BATCH_SIZE': 128}\n\n--- Training final model with best hyperparameters on all training data ---\nStarting final training run...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.6180, Val Loss=0.3309, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.3019, Val Loss=0.1799, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.2388, Val Loss=0.1396, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.2128, Val Loss=0.1283, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.1875, Val Loss=0.1149, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.1722, Val Loss=0.1097, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.1722, Val Loss=0.1005, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.1709, Val Loss=0.0962, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.1605, Val Loss=0.0949, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.1675, Val Loss=0.1013, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.1610, Val Loss=0.0925, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.1552, Val Loss=0.0924, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.1556, Val Loss=0.0912, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.1504, Val Loss=0.0925, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.1483, Val Loss=0.0880, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.1452, Val Loss=0.0884, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.1463, Val Loss=0.0943, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.1455, Val Loss=0.0940, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.1492, Val Loss=0.1085, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.1448, Val Loss=0.0863, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.1428, Val Loss=0.0839, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.1419, Val Loss=0.0863, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.1450, Val Loss=0.0917, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.1440, Val Loss=0.0830, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.1367, Val Loss=0.0875, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.1392, Val Loss=0.0889, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.1399, Val Loss=0.0797, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.1296, Val Loss=0.0824, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.1360, Val Loss=0.0891, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.1329, Val Loss=0.0852, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.1329, Val Loss=0.0797, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.1294, Val Loss=0.0829, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.1346, Val Loss=0.0775, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.1308, Val Loss=0.0819, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.1331, Val Loss=0.0817, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.1271, Val Loss=0.0770, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.1275, Val Loss=0.0871, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.1247, Val Loss=0.0789, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.1303, Val Loss=0.0801, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss=0.1296, Val Loss=0.0768, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 41/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss=0.1254, Val Loss=0.0760, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 42/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss=0.1243, Val Loss=0.0759, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 43/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss=0.1248, Val Loss=0.0776, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 44/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss=0.1273, Val Loss=0.0758, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 45/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss=0.1249, Val Loss=0.0746, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 46/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss=0.1236, Val Loss=0.0731, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 47/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss=0.1255, Val Loss=0.0788, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 48/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss=0.1211, Val Loss=0.0749, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 49/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss=0.1253, Val Loss=0.0743, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 50/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss=0.1239, Val Loss=0.0870, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 51/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss=0.1250, Val Loss=0.0737, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 52/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss=0.1279, Val Loss=0.0795, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 53/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss=0.1189, Val Loss=0.0757, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 54/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss=0.1199, Val Loss=0.0752, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 55/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss=0.1187, Val Loss=0.0732, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 56/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss=0.1179, Val Loss=0.0726, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 57/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 57: Train Loss=0.1150, Val Loss=0.0725, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 58/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 58: Train Loss=0.1177, Val Loss=0.0723, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 59/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 59: Train Loss=0.1176, Val Loss=0.0718, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 60/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 60: Train Loss=0.1189, Val Loss=0.0727, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 61/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 61: Train Loss=0.1155, Val Loss=0.0716, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 62/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 62: Train Loss=0.1149, Val Loss=0.0723, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 63/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 63: Train Loss=0.1227, Val Loss=0.0712, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 64/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 64: Train Loss=0.1179, Val Loss=0.0723, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 65/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 65: Train Loss=0.1171, Val Loss=0.0711, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 66/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 66: Train Loss=0.1168, Val Loss=0.0712, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 67/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 67: Train Loss=0.1230, Val Loss=0.0713, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 68/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 68: Train Loss=0.1168, Val Loss=0.0718, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 69/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 69: Train Loss=0.1203, Val Loss=0.0715, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 70/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 70: Train Loss=0.1166, Val Loss=0.0711, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 71/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 71: Train Loss=0.1195, Val Loss=0.0716, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 72/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 72: Train Loss=0.1130, Val Loss=0.0709, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 73/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 73: Train Loss=0.1170, Val Loss=0.0724, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 74/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 74: Train Loss=0.1186, Val Loss=0.0712, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 75/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 75: Train Loss=0.1133, Val Loss=0.0705, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 76/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 76: Train Loss=0.1153, Val Loss=0.0718, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 77/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 77: Train Loss=0.1138, Val Loss=0.0710, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 78/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 78: Train Loss=0.1163, Val Loss=0.0714, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 79/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 79: Train Loss=0.1155, Val Loss=0.0713, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 80/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 80: Train Loss=0.1196, Val Loss=0.0710, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 81/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 81: Train Loss=0.1144, Val Loss=0.0706, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 82/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 82: Train Loss=0.1160, Val Loss=0.0706, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 83/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 83: Train Loss=0.1146, Val Loss=0.0701, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 84: Train Loss=0.1157, Val Loss=0.0709, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 85: Train Loss=0.1153, Val Loss=0.0701, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 86: Train Loss=0.1205, Val Loss=0.0718, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 87: Train Loss=0.1140, Val Loss=0.0698, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 88: Train Loss=0.1184, Val Loss=0.0704, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 89: Train Loss=0.1157, Val Loss=0.0701, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 90: Train Loss=0.1154, Val Loss=0.0699, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 91: Train Loss=0.1156, Val Loss=0.0703, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 92: Train Loss=0.1198, Val Loss=0.0697, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  7.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 93: Train Loss=0.1155, Val Loss=0.0707, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 94: Train Loss=0.1127, Val Loss=0.0701, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  8.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 95: Train Loss=0.1143, Val Loss=0.0703, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 96: Train Loss=0.1109, Val Loss=0.0707, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 97: Train Loss=0.1162, Val Loss=0.0698, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 98: Train Loss=0.1104, Val Loss=0.0697, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99/100 [Train]: 100%|██████████| 21/21 [00:02<00:00, 10.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 99: Train Loss=0.1128, Val Loss=0.0697, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100/100 [Train]: 100%|██████████| 21/21 [00:02<00:00,  9.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 100: Train Loss=0.1176, Val Loss=0.0698, LR=0.000050\n\n--- Evaluating final model on test set ---\n\nEvaluation metrics (GAT_Transformer_DC_Weather_TAVG_final):\n MSE: 7.3648, RMSE: 2.7138, MAE: 2.0582, R²: 0.9052\nGrid search and final evaluation complete. Results in results_gat_transformer_dc_weather_tavg_grid_search\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport math\nimport joblib\n\n# --- Configuration ---\nMODEL_NAME = \"GAT_Transformer_Ukraine_NDVI_Mean_LRScheduler\" # Updated Name\nLOOKBACK = 365\nBATCH_SIZE = 32\nLEARNING_RATE = 0.0005\nNUM_EPOCHS = 200\nPATIENCE = 15 # Patience for early stopping\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Generic function to load tabular Multivariate Time Series data ---\ndef load_mts_data(data_df, feature_cols, target_cols, lookback_period):\n    print(f\"--- {MODEL_NAME}: Creating sequences from data ---\")\n    features = data_df[feature_cols].values\n    targets = data_df[target_cols].values\n\n    X_list = []\n    y_list = []\n\n    # Create sequences\n    for i in range(lookback_period, len(data_df)):\n        X_list.append(features[i - lookback_period : i])\n        y_list.append(targets[i])\n\n    X = np.array(X_list)\n    y = np.array(y_list)\n\n    print(f\"Loaded MTS data: X shape: {X.shape}, y shape: {y.shape}\")\n    return X, y\n\n# --- Chronological Train-Test Split ---\ndef chronological_train_test_split(X, y, test_ratio=0.2):\n    num_samples = X.shape[0]\n    num_test_samples = int(num_samples * test_ratio)\n    split_index = num_samples - num_test_samples\n\n    X_train = X[:split_index]\n    y_train = y[:split_index]\n    X_test = X[split_index:]\n    y_test = y[split_index:]\n\n    print(f\"X_train shape ({MODEL_NAME}): {X_train.shape}\")\n    print(f\"X_test shape ({MODEL_NAME}): {X_test.shape}\")\n    \n    return X_train, X_test, y_train, y_test\n\n# --- TimeSeriesDataset for GAT (Optimized Adjacency) ---\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, features, targets, single_adj_matrix, scaler_X=None, scaler_y=None, is_train=True):\n        original_shape = features.shape\n        if is_train and scaler_X is None:\n            self.scaler_X = StandardScaler()\n            # Reshape features to (num_samples * lookback, num_features) for scaling\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.fit_transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        elif not is_train and scaler_X is not None:\n            self.scaler_X = scaler_X\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        else: # Handles cases where features are pre-scaled or no scaling is needed\n            self.features = features; self.scaler_X = scaler_X\n        \n        if is_train and scaler_y is None:\n            self.scaler_y = StandardScaler(); self.targets = self.scaler_y.fit_transform(targets)\n        elif not is_train and scaler_y is not None:\n            self.scaler_y = scaler_y; self.targets = self.scaler_y.transform(targets)\n        else: # Handles pre-scaled targets or no target scaling\n            self.targets = targets; self.scaler_y = scaler_y\n        \n        self.features = torch.tensor(self.features, dtype=torch.float32)\n        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n        \n        if not isinstance(single_adj_matrix, torch.Tensor):\n            self.single_adj_matrix = torch.tensor(single_adj_matrix, dtype=torch.float32)\n        else:\n            self.single_adj_matrix = single_adj_matrix.to(dtype=torch.float32)\n\n    def __len__(self): return len(self.features)\n    def __getitem__(self, idx): return self.features[idx], self.single_adj_matrix, self.targets[idx]\n    def get_scalers(self): return self.scaler_X, self.scaler_y\n\n# --- Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000): # Default max_len\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term); pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0); self.register_buffer('pe', pe)\n    def forward(self, x): # x: (batch, seq_len, d_model)\n        # self.pe shape is (1, max_len, d_model)\n        # x.size(1) is seq_len\n        pe_sliced = self.pe[:, :x.size(1), :] \n        try:\n            result = x + pe_sliced\n        except RuntimeError as e:\n            print(f\"ERROR in PositionalEncoding: x.shape={x.shape}, pe_sliced.shape={pe_sliced.shape}, self.pe original shape={self.pe.shape}\")\n            raise e\n        return result\n\n# --- Graph Attention Layer & MultiHeadGraphAttention ---\nclass GraphAttentionLayer(nn.Module):\n    # Using dropout_rate from MTSPredictorGATTransformer (passed as 'dropout')\n    def __init__(self, in_features, out_features, dropout=0.5, alpha=0.2): \n        super(GraphAttentionLayer, self).__init__()\n        self.in_features=in_features; self.out_features=out_features; self.dropout_val=dropout; self.alpha=alpha\n        self.W=nn.Parameter(torch.empty(size=(in_features, out_features))); nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a=nn.Parameter(torch.empty(size=(2*out_features, 1))); nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu=nn.LeakyReLU(self.alpha); self.dropout_layer=nn.Dropout(self.dropout_val)\n    def forward(self, h, adj):\n        Wh=torch.matmul(h, self.W); a_input=self._prepare_attention_input(Wh)\n        e=self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n        zero_vec = -9e15*torch.ones_like(e); attention_masked = torch.where(adj.unsqueeze(0) > 0, e, zero_vec)\n        attention_softmax=F.softmax(attention_masked, dim=-1); attention_dropout=self.dropout_layer(attention_softmax)\n        h_prime=torch.matmul(attention_dropout, Wh); return F.elu(h_prime)\n    def _prepare_attention_input(self, Wh):\n        B,N,_=Wh.size(); Wh_i=Wh.unsqueeze(2).expand(B,N,N,-1); Wh_j=Wh.unsqueeze(1).expand(B,N,N,-1)\n        return torch.cat([Wh_i, Wh_j], dim=-1)\n\nclass MultiHeadGraphAttention(nn.Module):\n    # Using dropout_rate from MTSPredictorGATTransformer (passed as 'dropout')\n    def __init__(self, in_features, out_features_per_head, n_heads, dropout=0.5, alpha=0.2, concat=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.n_heads=n_heads; self.concat=concat\n        # Pass the dropout to each GraphAttentionLayer\n        self.attentions=nn.ModuleList([GraphAttentionLayer(in_features, out_features_per_head, dropout=dropout, alpha=alpha) for _ in range(n_heads)])\n        self.out_dim = out_features_per_head * n_heads if concat else out_features_per_head\n    def forward(self, x, adj):\n        head_outputs=[att(x, adj) for att in self.attentions]\n        if self.concat: return torch.cat(head_outputs, dim=-1)\n        else: return torch.mean(torch.stack(head_outputs, dim=-1), dim=-1)\n\n# --- GAT+Transformer Model ---\nclass MTSPredictorGATTransformer(nn.Module):\n    # Default dropout_rate set to 0.5 based on your best performing experiment\n    def __init__(self, input_size, output_size, d_model_gat=64, gat_heads=4,\n                 d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n                 dim_feedforward_transformer=256, dropout_rate=0.5): \n        super(MTSPredictorGATTransformer, self).__init__()\n        self.dropout_rate = dropout_rate # This will be used by internal dropout layers\n        self.gat_input_proj = nn.Linear(input_size, d_model_gat)\n        # Pass the overall dropout_rate to MultiHeadGraphAttention\n        self.gat_attention = MultiHeadGraphAttention(\n            in_features=d_model_gat,\n            out_features_per_head=d_model_gat // gat_heads,\n            n_heads=gat_heads, dropout=self.dropout_rate, concat=True \n        )\n        gat_output_dim = self.gat_attention.out_dim\n        # Pass appropriate max_len for PositionalEncoding based on global LOOKBACK\n        self.pos_encoder = PositionalEncoding(gat_output_dim, max_len=LOOKBACK + 100) \n        \n        transformer_encoder_layer = nn.TransformerEncoderLayer(\n            d_model=gat_output_dim, nhead=transformer_heads, dim_feedforward=dim_feedforward_transformer,\n            dropout=self.dropout_rate, batch_first=True # Transformer's own dropout\n        )\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_transformer_layers)\n        self.fc = nn.Sequential(\n            nn.Linear(gat_output_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(self.dropout_rate), # Dropout in FC layers\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(self.dropout_rate), # Dropout in FC layers\n            nn.Linear(32, output_size)\n        )\n    def forward(self, x, adj):\n        x_proj = F.elu(self.gat_input_proj(x))\n        # Note: The original code had an F.dropout here: F.dropout(x_proj, self.dropout_rate, training=self.training)\n        # This might be redundant if GAT layers themselves apply dropout, or it could be an additional dropout point.\n        # For now, passing x_proj directly as GAT layers have internal dropout.\n        # If you want dropout *before* GAT, uncomment the line below and pass x_gat_drop to self.gat_attention\n        # x_gat_drop = F.dropout(x_proj, self.dropout_rate, training=self.training)\n        x_gat_attended = self.gat_attention(x_proj, adj) # x_proj or x_gat_drop\n\n        if x_gat_attended.dim() == 4 and x_gat_attended.shape[0] == 1:\n             x_gat_attended = x_gat_attended.squeeze(0)\n\n        x_pos_encoded = self.pos_encoder(x_gat_attended)\n        x_transformed = self.transformer_encoder(x_pos_encoded)\n        x_last_step = x_transformed[:, -1, :]\n        output = self.fc(x_last_step)\n        return output\n\n# --- MODIFIED Training Function (to include scheduler.step) ---\ndef train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device, model_save_name): # Added scheduler\n    best_val_loss = float('inf'); epochs_no_improve = 0\n    train_losses_history, val_losses_history = [], []\n    \n    for epoch in range(num_epochs):\n        model.train(); epoch_train_loss = 0\n        for batch_X, batch_adj_single, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n            batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n            outputs = model(batch_X, batch_adj_single[0]);\n            loss = criterion(outputs, batch_y)\n            optimizer.zero_grad(); loss.backward(); optimizer.step(); epoch_train_loss += loss.item()\n        epoch_train_loss /= len(train_loader); train_losses_history.append(epoch_train_loss)\n        \n        model.eval(); epoch_val_loss = 0\n        with torch.no_grad():\n            for batch_X, batch_adj_single, batch_y in val_loader:\n                batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n                outputs = model(batch_X, batch_adj_single[0])\n                loss = criterion(outputs, batch_y); epoch_val_loss += loss.item()\n        epoch_val_loss /= len(val_loader); val_losses_history.append(epoch_val_loss)\n        \n        print(f\"Epoch {epoch+1}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f}, LR={optimizer.param_groups[0]['lr']:.6f}\")\n\n        # Step the scheduler based on validation loss\n        scheduler.step(epoch_val_loss) # <-- SCHEDULER STEP\n\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss; epochs_no_improve = 0; torch.save(model.state_dict(), model_save_name)\n            print(f\"Validation loss improved to {best_val_loss:.4f}. Saving model.\")\n        else: \n            epochs_no_improve += 1\n            print(f\"Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n\n        if epochs_no_improve >= patience: \n            print(f\"Early stopping @ epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n            break\n    \n    if os.path.exists(model_save_name): \n        print(f\"Loading best model from {model_save_name}\")\n        model.load_state_dict(torch.load(model_save_name))\n    return model, train_losses_history, val_losses_history\n\n# --- Evaluation Function ---\ndef evaluate_model_gat_based(model, test_loader, scaler_y, device, model_name=\"GAT-based Model\"):\n    model.eval(); all_preds_s, all_targets_s = [], [] # _s suffix indicates \"scaled\" or \"as output by model\"\n    with torch.no_grad():\n        for batch_X, batch_adj_single, batch_y_model_output_scale in test_loader:\n            batch_X, batch_adj_single = batch_X.to(device), batch_adj_single.to(device)\n            # batch_y_model_output_scale is already on CPU from DataLoader\n            outputs_model_scale = model(batch_X, batch_adj_single[0])\n            all_preds_s.append(outputs_model_scale.cpu().numpy())\n            all_targets_s.append(batch_y_model_output_scale.cpu().numpy()) # Ensure batch_y_s is on CPU\n            \n    preds_model_scale_np = np.vstack(all_preds_s)\n    tgts_original_scale_np = np.vstack(all_targets_s) # These are already in original scale if target wasn't scaled\n    \n    # If target was scaled, inverse transform predictions and original targets (which were also scaled in dataset)\n    if scaler_y:\n        preds_o = scaler_y.inverse_transform(preds_model_scale_np)\n        tgts_o = scaler_y.inverse_transform(tgts_original_scale_np) # This line assumes tgts_original_scale_np were scaled\n                                                                        # If target was not scaled, then tgts_original_scale_np are already 'o'\n    else: # Target was not scaled\n        preds_o = preds_model_scale_np # Predictions are direct if target not scaled (model learns to predict in original scale)\n        tgts_o = tgts_original_scale_np # Targets are already in original scale\n\n    mse=mean_squared_error(tgts_o,preds_o); rmse=np.sqrt(mse); r2=r2_score(tgts_o,preds_o); mae=mean_absolute_error(tgts_o,preds_o)\n    print(f\"\\nEvaluation metrics ({model_name}):\\n MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n    return preds_o, tgts_o, mse, rmse, r2, mae\n\n# --- Plotting Functions ---\ndef plot_training_losses(train_losses, val_losses, model_name, save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'training_losses_{model_name}.png')\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n    plt.xlabel('Epochs'); plt.ylabel('Loss (MSE)'); plt.title(f'Training & Validation Losses ({model_name})')\n    plt.legend(); plt.grid(True); plt.savefig(save_path); plt.show()\n\ndef plot_predictions_vs_true(targets_original, predictions_original, model_name, target_name=\"Target\", save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'predictions_vs_true_{model_name}.png')\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(targets_original, predictions_original, alpha=0.3, label='Sample Predictions')\n    min_val = min(targets_original.ravel().min(), predictions_original.ravel().min()) # Use ravel() for safety\n    max_val = max(targets_original.ravel().max(), predictions_original.ravel().max()) # Use ravel() for safety\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='y=x (Perfect Prediction)')\n\n    plt.xlabel(f'True Values ({target_name})'); plt.ylabel(f'Predicted Values ({target_name})')\n    plt.title(f'Predictions vs True Values for {target_name} ({model_name})')\n    plt.legend(); plt.grid(True)\n    plt.tight_layout(); plt.savefig(save_path); plt.show()\n\n# --- Main Function ---\ndef main_mts_forecaster():\n    print(f\"--- Running {MODEL_NAME} Model ---\")\n    print(f\"Using device: {DEVICE}\")\n    output_dir = f\"results_{MODEL_NAME.lower().replace(' ', '_')}\"\n    viz_dir = os.path.join(output_dir, \"visualizations\")\n    if not os.path.exists(viz_dir): os.makedirs(viz_dir)\n\n    data_path = '/kaggle/input/ukraine-vegetation-data-jan-2018-jan-2023/NDVI_Data.csv'\n    if not os.path.exists(data_path):\n        print(f\"ERROR: Dataset not found at '{data_path}'\")\n        print(\"Please download it from Kaggle, place it in the script's directory, and update 'data_path' if needed.\")\n        return\n\n    print(\"Loading and preprocessing Ukraine vegetation data...\")\n    df = pd.read_csv(data_path)\n    \n    # Rename columns for easier use (replacing '/' with '_')\n    column_mapping = {\n        'C0/date': 'date',\n        'C0/min': 'min_ndvi',\n        'C0/max': 'max_ndvi',\n        'C0/mean': 'mean_ndvi',\n        'C0/stDev': 'stdev_ndvi',\n        'C0/sampleCount': 'sample_count',\n        'C0/noDataCount': 'nodata_count',\n        'C0/median': 'median_ndvi',\n        'C0/p10': 'p10_ndvi',\n        'C0/p90': 'p90_ndvi',\n        'C0/cloudCoveragePercent': 'cloud_coverage_percent'\n    }\n    df.rename(columns=column_mapping, inplace=True)\n    \n    # Convert 'date' column to datetime and set as index\n    if 'date' not in df.columns:\n        print(\"ERROR: 'date' column (renamed from C0/date) not found. Check column names and mapping.\")\n        return\n        \n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    \n    # Define feature and target columns using new (renamed) column names\n    feature_cols = [\n        'min_ndvi', 'max_ndvi', 'mean_ndvi', 'stdev_ndvi', \n        'sample_count', 'nodata_count', 'median_ndvi', \n        'p10_ndvi', 'p90_ndvi', 'cloud_coverage_percent'\n    ]\n    target_cols = ['mean_ndvi']\n    target_name_for_plot = \"Mean NDVI\"\n\n    # Ensure all selected columns are numeric and exist\n    existing_feature_cols = []\n    for col in feature_cols:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n            existing_feature_cols.append(col)\n        else:\n            print(f\"Warning: Feature column '{col}' not found in DataFrame. It will be ignored.\")\n    \n    if not existing_feature_cols:\n        print(\"ERROR: No valid feature columns found. Please check column names and data.\"); return\n    if target_cols[0] not in df.columns:\n        print(f\"ERROR: Target column '{target_cols[0]}' not found. Please check column names and data.\"); return\n\n    # Ensure df_processed contains all necessary columns and remove duplicates if target is also a feature\n    df_processed = df[existing_feature_cols + target_cols].copy() \n    df_processed = df_processed.loc[:,~df_processed.columns.duplicated()] \n\n    # Handle missing values\n    df_processed.ffill(inplace=True)\n    df_processed.bfill(inplace=True) \n    if df_processed.isnull().sum().any():\n        print(\"Warning: NaNs still present after ffill and bfill. Filling with 0.\")\n        print(df_processed.isnull().sum())\n        df_processed.fillna(0, inplace=True)\n\n    print(f\"Data processed. Shape: {df_processed.shape}\")\n    print(df_processed.head())\n    \n    # Pass df_processed and the determined feature/target columns to load_mts_data\n    X_all, y_all = load_mts_data(df_processed, existing_feature_cols, target_cols, LOOKBACK)\n    \n    if X_all.shape[0] == 0: print(\"No valid samples generated. Exiting.\"); return\n    \n    X_tv, X_test, y_tv, y_test = chronological_train_test_split(X_all, y_all, test_ratio=0.2)\n    \n    if X_tv.shape[0] == 0: print(f\"No training/validation samples for {MODEL_NAME}. Exiting.\"); return\n\n    adj_matrix_template = torch.zeros(LOOKBACK, LOOKBACK, dtype=torch.float32)\n    for i in range(LOOKBACK):\n        if i > 0: adj_matrix_template[i, i-1] = 1\n        if i < LOOKBACK - 1: adj_matrix_template[i, i+1] = 1\n        adj_matrix_template[i, i] = 1\n    adj_matrix_template = adj_matrix_template.to(DEVICE)\n    \n    # TimeSeriesDataset will perform internal scaling\n    tv_dataset = TimeSeriesDataset(X_tv, y_tv, adj_matrix_template.cpu(), is_train=True)\n    scaler_X, scaler_y = tv_dataset.get_scalers()\n    \n    test_loader = None\n    if X_test.shape[0] > 0:\n        # Pass fitted scalers to test dataset\n        test_dataset = TimeSeriesDataset(X_test, y_test, adj_matrix_template.cpu(), scaler_X=scaler_X, scaler_y=scaler_y, is_train=False)\n        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) \n\n    train_s = int(0.8 * len(tv_dataset)); val_s = len(tv_dataset) - train_s\n    if val_s == 0 and train_s > 0:\n        print(\"Warning: Validation set size is 0. Using training set for validation.\")\n        train_sub, val_sub = tv_dataset, tv_dataset \n    elif train_s == 0 or val_s == 0 : \n        print(\"Not enough data for train/val split. Exiting.\"); return\n    else: \n        gen=torch.Generator().manual_seed(42); \n        train_sub, val_sub = torch.utils.data.random_split(tv_dataset, [train_s, val_s], generator=gen)\n\n    train_loader = DataLoader(train_sub, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_sub, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n   \n    model_input_size = X_tv.shape[-1]\n    model_output_size = y_tv.shape[-1]\n    \n    model = MTSPredictorGATTransformer(\n        input_size=model_input_size, \n        output_size=model_output_size,\n        d_model_gat=64, gat_heads=4,\n        d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n        dim_feedforward_transformer=256, dropout_rate=0.4 # Dropout from best run\n    ).to(DEVICE)\n    \n    print(f\"\\n{MODEL_NAME} Model Architecture:\\n{model}\")\n    criterion = nn.MSELoss() # Defined here for clarity\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n    \n    # Initialize the Learning Rate Scheduler\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n       \n    model_save_path = os.path.join(output_dir, f\"best_mts_{MODEL_NAME.lower().replace(' ', '_')}_model.pt\")\n    # Pass scheduler to training function\n    trained_model, train_L, val_L = train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, scheduler, NUM_EPOCHS, PATIENCE, DEVICE, model_save_path)\n    plot_training_losses(train_L, val_L, MODEL_NAME, save_dir=viz_dir)\n       \n    if test_loader and X_test.shape[0] > 0 :\n        preds_o, tgts_o, mse, rmse, r2, mae = evaluate_model_gat_based(trained_model, test_loader, scaler_y, DEVICE, MODEL_NAME)\n        plot_predictions_vs_true(tgts_o, preds_o, MODEL_NAME, target_name=target_name_for_plot, save_dir=viz_dir)\n        np.savez_compressed(os.path.join(output_dir, f'{MODEL_NAME.lower().replace(\" \", \"_\")}_test_results.npz'), predictions=preds_o, targets=tgts_o)\n    else: print(f\"Test set for {MODEL_NAME} was empty or test_loader not created. Skipping final evaluation plot.\")\n\n    if scaler_X: # Only save if scaler_X was created (i.e., if tv_dataset was not empty)\n        joblib.dump(scaler_X, os.path.join(output_dir, f\"scaler_X_{MODEL_NAME.lower().replace(' ', '_')}.pkl\"))\n    if scaler_y: # Only save if scaler_y was created\n        joblib.dump(scaler_y, os.path.join(output_dir, f\"scaler_y_{MODEL_NAME.lower().replace(' ', '_')}.pkl\"))\n    print(f\"{MODEL_NAME} model training complete. Results in {output_dir}\")\n\nif __name__ == \"__main__\":\n    main_mts_forecaster()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T14:31:51.140831Z","iopub.execute_input":"2025-06-05T14:31:51.141234Z","iopub.status.idle":"2025-06-05T16:27:44.030105Z","shell.execute_reply.started":"2025-06-05T14:31:51.141203Z","shell.execute_reply":"2025-06-05T16:27:44.029038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport math\nimport joblib\nimport itertools\n\n# --- Configuration ---\nMODEL_NAME = \"GAT_Transformer_Ukraine_NDVI_GridSearch\"\n# Fixed lookback for the grid search to ensure data consistency\nLOOKBACK = 365\nNUM_EPOCHS = 100 # Max epochs for each grid search run\nPATIENCE = 15    # Patience for early stopping\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# --- Generic function to load tabular Multivariate Time Series data ---\ndef load_mts_data(data_df, feature_cols, target_cols, lookback_period):\n    print(f\"--- Creating sequences with lookback={lookback_period} ---\")\n    features = data_df[feature_cols].values\n    targets = data_df[target_cols].values\n    X_list, y_list = [], []\n    for i in range(lookback_period, len(data_df)):\n        X_list.append(features[i - lookback_period : i])\n        y_list.append(targets[i])\n    X, y = np.array(X_list), np.array(y_list)\n    print(f\"Loaded MTS data: X shape: {X.shape}, y shape: {y.shape}\")\n    return X, y\n\n# --- Chronological Train-Test Split ---\ndef chronological_train_test_split(X, y, test_ratio=0.2):\n    num_samples = X.shape[0]\n    num_test_samples = int(num_samples * test_ratio)\n    split_index = num_samples - num_test_samples\n    X_train, X_test = X[:split_index], X[split_index:]\n    y_train, y_test = y[:split_index], y[split_index:]\n    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n    return X_train, X_test, y_train, y_test\n\n# --- TimeSeriesDataset for GAT (Optimized Adjacency) ---\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, features, targets, single_adj_matrix, scaler_X=None, scaler_y=None, is_train=True):\n        original_shape = features.shape\n        if is_train and scaler_X is None:\n            self.scaler_X = StandardScaler()\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.fit_transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        elif not is_train and scaler_X is not None:\n            self.scaler_X = scaler_X\n            flattened = features.reshape(-1, original_shape[-1]); scaled = self.scaler_X.transform(flattened)\n            self.features = scaled.reshape(original_shape)\n        else: self.features, self.scaler_X = features, scaler_X\n\n        if is_train and scaler_y is None:\n            self.scaler_y = StandardScaler(); self.targets = self.scaler_y.fit_transform(targets)\n        elif not is_train and scaler_y is not None:\n            self.scaler_y = scaler_y; self.targets = self.scaler_y.transform(targets)\n        else: self.targets, self.scaler_y = targets, scaler_y\n\n        self.features = torch.tensor(self.features, dtype=torch.float32)\n        self.targets = torch.tensor(self.targets, dtype=torch.float32)\n\n        if not isinstance(single_adj_matrix, torch.Tensor):\n            self.single_adj_matrix = torch.tensor(single_adj_matrix, dtype=torch.float32)\n        else:\n            self.single_adj_matrix = single_adj_matrix.to(dtype=torch.float32)\n\n    def __len__(self): return len(self.features)\n    def __getitem__(self, idx): return self.features[idx], self.single_adj_matrix, self.targets[idx]\n    def get_scalers(self): return self.scaler_X, self.scaler_y\n\n# --- Positional Encoding ---\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term); pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0); self.register_buffer('pe', pe)\n    def forward(self, x):\n        pe_sliced = self.pe[:, :x.size(1), :]\n        try:\n            result = x + pe_sliced\n        except RuntimeError as e:\n            print(f\"ERROR in PositionalEncoding: x.shape={x.shape}, pe_sliced.shape={pe_sliced.shape}\")\n            raise e\n        return result\n\n# --- Graph Attention Layer & MultiHeadGraphAttention ---\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=0.2, alpha=0.2):\n        super(GraphAttentionLayer, self).__init__()\n        self.dropout_val, self.alpha = dropout, alpha\n        self.W = nn.Parameter(torch.empty(size=(in_features, out_features))); nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1))); nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu = nn.LeakyReLU(self.alpha); self.dropout_layer = nn.Dropout(self.dropout_val)\n    def forward(self, h, adj):\n        Wh = torch.matmul(h, self.W); a_input = self._prepare_attention_input(Wh)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(-1))\n        zero_vec = -9e15*torch.ones_like(e); attention_masked = torch.where(adj.unsqueeze(0) > 0, e, zero_vec)\n        attention_softmax = F.softmax(attention_masked, dim=-1); attention_dropout = self.dropout_layer(attention_softmax)\n        h_prime = torch.matmul(attention_dropout, Wh); return F.elu(h_prime)\n    def _prepare_attention_input(self, Wh):\n        B, N, _ = Wh.size(); Wh_i = Wh.unsqueeze(2).expand(B, N, N, -1); Wh_j = Wh.unsqueeze(1).expand(B, N, N, -1)\n        return torch.cat([Wh_i, Wh_j], dim=-1)\n\nclass MultiHeadGraphAttention(nn.Module):\n    def __init__(self, in_features, out_features_per_head, n_heads, dropout=0.2, alpha=0.2, concat=True):\n        super(MultiHeadGraphAttention, self).__init__()\n        self.concat = concat\n        self.attentions = nn.ModuleList([GraphAttentionLayer(in_features, out_features_per_head, dropout=dropout, alpha=alpha) for _ in range(n_heads)])\n        self.out_dim = out_features_per_head * n_heads if concat else out_features_per_head\n    def forward(self, x, adj):\n        head_outputs = [att(x, adj) for att in self.attentions]\n        return torch.cat(head_outputs, dim=-1) if self.concat else torch.mean(torch.stack(head_outputs, dim=-1), dim=-1)\n\n# --- GAT+Transformer Model ---\nclass MTSPredictorGATTransformer(nn.Module):\n    def __init__(self, input_size, output_size, d_model_gat=64, gat_heads=4,\n                 d_model_transformer=64, transformer_heads=4, num_transformer_layers=2,\n                 dim_feedforward_transformer=256, dropout_rate=0.2):\n        super(MTSPredictorGATTransformer, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.gat_input_proj = nn.Linear(input_size, d_model_gat)\n        self.gat_attention = MultiHeadGraphAttention(in_features=d_model_gat, out_features_per_head=d_model_gat // gat_heads,\n                                                     n_heads=gat_heads, dropout=self.dropout_rate, concat=True)\n        gat_output_dim = self.gat_attention.out_dim\n        self.pos_encoder = PositionalEncoding(gat_output_dim, max_len=LOOKBACK + 100)\n        transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=gat_output_dim, nhead=transformer_heads,\n                                                               dim_feedforward=dim_feedforward_transformer,\n                                                               dropout=self.dropout_rate, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_transformer_layers)\n        self.fc = nn.Sequential(nn.Linear(gat_output_dim, 64), nn.ReLU(), nn.Dropout(self.dropout_rate),\n                                nn.Linear(64, 32), nn.ReLU(), nn.Dropout(self.dropout_rate),\n                                nn.Linear(32, output_size))\n    def forward(self, x, adj):\n        x_proj = F.elu(self.gat_input_proj(x))\n        x_gat_attended = self.gat_attention(x_proj, adj)\n        if x_gat_attended.dim() == 4 and x_gat_attended.shape[0] == 1:\n             x_gat_attended = x_gat_attended.squeeze(0)\n        x_pos_encoded = self.pos_encoder(x_gat_attended)\n        x_transformed = self.transformer_encoder(x_pos_encoded)\n        x_last_step = x_transformed[:, -1, :]\n        return self.fc(x_last_step)\n\n# --- Training Function ---\ndef train_model_gat_based(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device, model_save_name):\n    best_val_loss = float('inf'); epochs_no_improve = 0\n    train_losses_history, val_losses_history = [], []\n    for epoch in range(num_epochs):\n        model.train(); epoch_train_loss = 0\n        for batch_X, batch_adj_single, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n            batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n            outputs = model(batch_X, batch_adj_single[0])\n            loss = criterion(outputs, batch_y)\n            optimizer.zero_grad(); loss.backward(); optimizer.step(); epoch_train_loss += loss.item()\n        epoch_train_loss /= len(train_loader); train_losses_history.append(epoch_train_loss)\n\n        model.eval(); epoch_val_loss = 0\n        with torch.no_grad():\n            for batch_X, batch_adj_single, batch_y in val_loader:\n                batch_X, batch_adj_single, batch_y = batch_X.to(device), batch_adj_single.to(device), batch_y.to(device)\n                outputs = model(batch_X, batch_adj_single[0])\n                loss = criterion(outputs, batch_y); epoch_val_loss += loss.item()\n        epoch_val_loss /= len(val_loader); val_losses_history.append(epoch_val_loss)\n\n        current_lr = optimizer.param_groups[0]['lr']\n        print(f\"Epoch {epoch+1}: Train Loss={epoch_train_loss:.4f}, Val Loss={epoch_val_loss:.4f}, LR={current_lr:.6f}\")\n        if scheduler: scheduler.step(epoch_val_loss)\n\n        if epoch_val_loss < best_val_loss:\n            best_val_loss = epoch_val_loss; epochs_no_improve = 0\n            torch.save(model.state_dict(), model_save_name)\n        else:\n            epochs_no_improve += 1\n        if epochs_no_improve >= patience:\n            print(f\"Early stopping @ epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n            break\n    if os.path.exists(model_save_name):\n        model.load_state_dict(torch.load(model_save_name))\n    return model, train_losses_history, val_losses_history, best_val_loss\n\n# --- Evaluation Function ---\ndef evaluate_model_gat_based(model, test_loader, scaler_y, device, model_name=\"GAT-based Model\"):\n    model.eval(); all_preds_s, all_targets_s = [], []\n    with torch.no_grad():\n        for batch_X, batch_adj_single, batch_y_s in test_loader:\n            batch_X, batch_adj_single = batch_X.to(device), batch_adj_single.to(device)\n            outputs_s = model(batch_X, batch_adj_single[0])\n            all_preds_s.append(outputs_s.cpu().numpy()); all_targets_s.append(batch_y_s.cpu().numpy())\n    preds_s_np = np.vstack(all_preds_s); tgts_s_np = np.vstack(all_targets_s)\n\n    if scaler_y:\n        preds_o = scaler_y.inverse_transform(preds_s_np); tgts_o = scaler_y.inverse_transform(tgts_s_np)\n    else: preds_o, tgts_o = preds_s_np, tgts_s_np\n\n    mse = mean_squared_error(tgts_o, preds_o); rmse = np.sqrt(mse); r2 = r2_score(tgts_o, preds_o); mae = mean_absolute_error(tgts_o, preds_o)\n    print(f\"\\nEvaluation metrics ({model_name}):\\n MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n    return preds_o, tgts_o, mse, rmse, r2, mae\n\n# --- Plotting Functions ---\ndef plot_training_losses(train_losses, val_losses, model_name, save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'training_losses_{model_name}.png')\n    plt.figure(figsize=(10, 6)); plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Train Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss')\n    plt.xlabel('Epochs'); plt.ylabel('Loss (MSE)'); plt.title(f'Training & Validation Losses ({model_name})')\n    plt.legend(); plt.grid(True); plt.savefig(save_path); plt.close() # Close plot to save memory\n\ndef plot_predictions_vs_true(targets_original, predictions_original, model_name, target_name=\"Target\", save_dir='visualizations'):\n    if not os.path.exists(save_dir): os.makedirs(save_dir)\n    save_path = os.path.join(save_dir, f'predictions_vs_true_{model_name}.png')\n    plt.figure(figsize=(12, 8)); plt.scatter(targets_original, predictions_original, alpha=0.3, label='Sample Predictions')\n    min_val = min(targets_original.ravel().min(), predictions_original.ravel().min())\n    max_val = max(targets_original.ravel().max(), predictions_original.ravel().max())\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='y=x (Perfect Prediction)')\n    plt.xlabel(f'True Values ({target_name})'); plt.ylabel(f'Predicted Values ({target_name})')\n    plt.title(f'Predictions vs True Values for {target_name} ({model_name})')\n    plt.legend(); plt.grid(True); plt.tight_layout(); plt.savefig(save_path); plt.close() # Close plot\n\n# --- New Function to run a single training session for grid search ---\ndef run_training_session(params, train_dataset, val_dataset, adj_matrix_template, model_input_size, model_output_size, output_dir):\n    print(f\"\\n--- Running with params: {params} ---\")\n\n    lr = params['LEARNING_RATE']\n    dropout_rate = params['DROPOUT_RATE']\n    weight_decay = params['WEIGHT_DECAY']\n    batch_size = params['BATCH_SIZE']\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\n    model = MTSPredictorGATTransformer(\n        input_size=model_input_size,\n        output_size=model_output_size,\n        dropout_rate=dropout_rate\n    ).to(DEVICE)\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=7, verbose=False)\n\n    param_str = \"_\".join([f\"{k}_{v}\" for k, v in params.items()])\n    model_save_name = os.path.join(output_dir, f\"best_model_{param_str}.pt\")\n\n    _, _, _, best_val_loss = train_model_gat_based(\n        model, train_loader, val_loader, criterion, optimizer, scheduler,\n        NUM_EPOCHS, PATIENCE, DEVICE, model_save_name\n    )\n    return best_val_loss\n\n# --- Main Function ---\ndef main_mts_forecaster():\n    print(f\"--- Running {MODEL_NAME} ---\")\n    print(f\"Using device: {DEVICE}\")\n    output_dir = f\"results_{MODEL_NAME.lower()}_grid_search\"\n    viz_dir = os.path.join(output_dir, \"visualizations\")\n    if not os.path.exists(viz_dir): os.makedirs(viz_dir)\n\n    # --- 1. DEFINE HYPERPARAMETER GRID ---\n    param_grid = {\n        'LEARNING_RATE': [0.0005, 0.0001],\n        'DROPOUT_RATE': [0.4, 0.5],\n        'WEIGHT_DECAY': [1e-5, 1e-4],\n        'BATCH_SIZE': [32, 64]\n    }\n    print(\"--- Hyperparameter Grid ---\")\n    print(param_grid)\n\n    # --- 2. LOAD AND PREPROCESS DATA ONCE ---\n    data_path = '/kaggle/input/ukraine-vegetation-data-jan-2018-jan-2023/NDVI_Data.csv'\n    if not os.path.exists(data_path):\n        print(f\"ERROR: Dataset not found at '{data_path}'\"); return\n\n    print(\"Loading and preprocessing Ukraine vegetation data...\")\n    df = pd.read_csv(data_path)\n    column_mapping = {\n        'C0/date': 'date', 'C0/min': 'min_ndvi', 'C0/max': 'max_ndvi',\n        'C0/mean': 'mean_ndvi', 'C0/stDev': 'stdev_ndvi',\n        'C0/sampleCount': 'sample_count', 'C0/noDataCount': 'nodata_count',\n        'C0/median': 'median_ndvi', 'C0/p10': 'p10_ndvi', 'C0/p90': 'p90_ndvi',\n        'C0/cloudCoveragePercent': 'cloud_coverage_percent'\n    }\n    df.rename(columns=column_mapping, inplace=True)\n    if 'date' not in df.columns: print(\"ERROR: 'date' column not found.\"); return\n    df['date'] = pd.to_datetime(df['date']); df.set_index('date', inplace=True)\n    \n    feature_cols = [\n        'min_ndvi', 'max_ndvi', 'mean_ndvi', 'stdev_ndvi', \n        'sample_count', 'nodata_count', 'median_ndvi', \n        'p10_ndvi', 'p90_ndvi', 'cloud_coverage_percent'\n    ]\n    target_cols = ['mean_ndvi']\n    target_name_for_plot = \"Mean NDVI\"\n\n    existing_feature_cols = [col for col in feature_cols if col in df.columns]\n    df_processed = df[existing_feature_cols].copy()\n    for col in df_processed.columns:\n        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n    df_processed.ffill(inplace=True); df_processed.bfill(inplace=True)\n    if df_processed.isnull().sum().any(): df_processed.fillna(0, inplace=True)\n\n    print(f\"Data processed. Shape: {df_processed.shape}\")\n    \n    X_all, y_all = load_mts_data(df_processed, existing_feature_cols, target_cols, LOOKBACK)\n    if X_all.shape[0] == 0: print(\"No valid samples generated. Exiting.\"); return\n    \n    X_tv_and_val, X_test, y_tv_and_val, y_test = chronological_train_test_split(X_all, y_all, test_ratio=0.2)\n    if X_tv_and_val.shape[0] == 0: print(f\"No training/validation samples. Exiting.\"); return\n\n    # --- 3. CREATE DATASETS ONCE ---\n    adj_matrix_template = torch.zeros(LOOKBACK, LOOKBACK, dtype=torch.float32)\n    for i in range(LOOKBACK):\n        if i > 0: adj_matrix_template[i, i-1] = 1\n        if i < LOOKBACK - 1: adj_matrix_template[i, i+1] = 1\n        adj_matrix_template[i, i] = 1\n    adj_matrix_template = adj_matrix_template.to(DEVICE)\n\n    full_train_val_dataset = TimeSeriesDataset(X_tv_and_val, y_tv_and_val, adj_matrix_template.cpu(), is_train=True)\n    scaler_X, scaler_y = full_train_val_dataset.get_scalers()\n\n    train_s = int(0.8 * len(full_train_val_dataset))\n    val_s = len(full_train_val_dataset) - train_s\n    gen = torch.Generator().manual_seed(42)\n    train_dataset, val_dataset = torch.utils.data.random_split(full_train_val_dataset, [train_s, val_s], generator=gen)\n\n    # --- 4. RUN GRID SEARCH LOOP ---\n    best_params = {}\n    best_val_loss = float('inf')\n    \n    param_combinations = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n\n    for params in param_combinations:\n        val_loss = run_training_session(\n            params=params, train_dataset=train_dataset, val_dataset=val_dataset,\n            adj_matrix_template=adj_matrix_template, model_input_size=X_all.shape[-1],\n            model_output_size=y_all.shape[-1], output_dir=output_dir\n        )\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_params = params\n            print(f\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n            print(f\"!!! New best validation loss: {best_val_loss:.4f} with params: {best_params}\")\n            print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n\n    print(f\"\\n--- Grid Search Complete ---\")\n    print(f\"Best validation loss found: {best_val_loss:.4f}\")\n    print(f\"Best hyperparameters: {best_params}\")\n\n    # --- 5. FINAL EVALUATION WITH BEST HYPERPARAMETERS ---\n    print(\"\\n--- Training final model with best hyperparameters on all training data ---\")\n    \n    final_model = MTSPredictorGATTransformer(\n        input_size=X_all.shape[-1], output_size=y_all.shape[-1],\n        dropout_rate=best_params.get('DROPOUT_RATE', 0.5) # Use best or a default\n    ).to(DEVICE)\n    \n    final_optimizer = optim.Adam(\n        final_model.parameters(), \n        lr=best_params.get('LEARNING_RATE', 0.0005), \n        weight_decay=best_params.get('WEIGHT_DECAY', 1e-5)\n    )\n    final_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        final_optimizer, mode='min', factor=0.1, patience=7, verbose=True\n    )\n    final_criterion = nn.MSELoss()\n\n    final_train_loader = DataLoader(full_train_val_dataset, batch_size=best_params.get('BATCH_SIZE', 32), shuffle=True)\n    final_val_loader = DataLoader(val_dataset, batch_size=best_params.get('BATCH_SIZE', 32), shuffle=False)\n    \n    final_model_save_path = os.path.join(output_dir, \"final_best_model.pt\")\n    \n    print(\"Starting final training run...\")\n    trained_model, train_L, val_L, _ = train_model_gat_based(\n        final_model, final_train_loader, final_val_loader, final_criterion, final_optimizer, final_scheduler,\n        NUM_EPOCHS, PATIENCE, DEVICE, final_model_save_path\n    )\n    \n    plot_training_losses(train_L, val_L, f\"{MODEL_NAME}_final\", save_dir=viz_dir)\n   \n    if X_test.shape[0] > 0:\n        print(\"\\n--- Evaluating final model on test set ---\")\n        test_dataset = TimeSeriesDataset(X_test, y_test, adj_matrix_template.cpu(), scaler_X, scaler_y, is_train=False)\n        test_loader = DataLoader(test_dataset, batch_size=best_params.get('BATCH_SIZE', 32), shuffle=False)\n        \n        preds_o, tgts_o, _, _, _, _ = evaluate_model_gat_based(trained_model, test_loader, scaler_y, DEVICE, f\"{MODEL_NAME}_final\")\n        plot_predictions_vs_true(tgts_o, preds_o, f\"{MODEL_NAME}_final\", target_name=target_name_for_plot, save_dir=viz_dir)\n    else:\n        print(\"Test set was empty. Skipping final evaluation.\")\n\n    print(f\"Grid search and final evaluation complete. Results in {output_dir}\")\n\nif __name__ == \"__main__\":\n    main_mts_forecaster()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T19:49:24.173696Z","iopub.execute_input":"2025-06-13T19:49:24.173926Z","iopub.status.idle":"2025-06-13T19:51:29.572613Z","shell.execute_reply.started":"2025-06-13T19:49:24.173908Z","shell.execute_reply":"2025-06-13T19:51:29.571593Z"}},"outputs":[{"name":"stdout","text":"--- Running GAT_Transformer_Ukraine_NDVI_GridSearch ---\nUsing device: cuda\n--- Hyperparameter Grid ---\n{'LEARNING_RATE': [0.0005, 0.0001], 'DROPOUT_RATE': [0.4, 0.5], 'WEIGHT_DECAY': [1e-05, 0.0001], 'BATCH_SIZE': [32, 64]}\nLoading and preprocessing Ukraine vegetation data...\nData processed. Shape: (1828, 10)\n--- Creating sequences with lookback=365 ---\nLoaded MTS data: X shape: (1463, 365, 10), y shape: (1463, 1)\nX_train shape: (1171, 365, 10), X_test shape: (292, 365, 10)\n\n--- Running with params: {'LEARNING_RATE': 0.0005, 'DROPOUT_RATE': 0.4, 'WEIGHT_DECAY': 1e-05, 'BATCH_SIZE': 32} ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\nEpoch 1/100 [Train]: 100%|██████████| 30/30 [00:03<00:00,  9.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss=0.8071, Val Loss=0.5091, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss=0.5861, Val Loss=0.4987, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss=0.5076, Val Loss=0.4745, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss=0.5372, Val Loss=0.4336, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss=0.4799, Val Loss=0.4505, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss=0.4585, Val Loss=0.4187, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss=0.5063, Val Loss=0.4108, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss=0.4351, Val Loss=0.4340, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss=0.4347, Val Loss=0.4270, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss=0.4513, Val Loss=0.4239, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss=0.4317, Val Loss=0.4032, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss=0.4135, Val Loss=0.4026, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss=0.4577, Val Loss=0.4146, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss=0.4387, Val Loss=0.4092, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss=0.4224, Val Loss=0.4059, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss=0.4162, Val Loss=0.4339, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss=0.4377, Val Loss=0.4138, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss=0.4379, Val Loss=0.4229, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss=0.4150, Val Loss=0.4040, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss=0.4103, Val Loss=0.4022, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss=0.3978, Val Loss=0.3985, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss=0.3860, Val Loss=0.3912, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss=0.4101, Val Loss=0.4001, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss=0.4287, Val Loss=0.4008, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss=0.4052, Val Loss=0.3890, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss=0.3993, Val Loss=0.4086, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss=0.4119, Val Loss=0.3886, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss=0.4255, Val Loss=0.3926, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss=0.4116, Val Loss=0.3973, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss=0.4085, Val Loss=0.3946, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 31/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss=0.4201, Val Loss=0.3941, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 32/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss=0.4188, Val Loss=0.4585, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 33/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss=0.4278, Val Loss=0.3887, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 34/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss=0.3827, Val Loss=0.3934, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 35/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss=0.3786, Val Loss=0.4019, LR=0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 36/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss=0.4011, Val Loss=0.3915, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 37/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss=0.3806, Val Loss=0.3911, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 38/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 10.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss=0.4025, Val Loss=0.3930, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 39/100 [Train]: 100%|██████████| 30/30 [00:02<00:00, 11.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss=0.3800, Val Loss=0.3924, LR=0.000050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 40/100 [Train]:  30%|███       | 9/30 [00:00<00:02, 10.16it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4104553776.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0mmain_mts_forecaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/4104553776.py\u001b[0m in \u001b[0;36mmain_mts_forecaster\u001b[0;34m()\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_combinations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         val_loss = run_training_session(\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0madj_matrix_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madj_matrix_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/4104553776.py\u001b[0m in \u001b[0;36mrun_training_session\u001b[0;34m(params, train_dataset, val_dataset, adj_matrix_template, model_input_size, model_output_size, output_dir)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mmodel_save_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"best_model_{param_str}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     _, _, _, best_val_loss = train_model_gat_based(\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATIENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/4104553776.py\u001b[0m in \u001b[0;36mtrain_model_gat_based\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, patience, device, model_save_name)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_adj_single\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtrain_losses_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1}]}